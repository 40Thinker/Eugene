>[!nexus_user] **User** - 30.05.2025 at 7:04
> 1. любое 2. любое 3. любое 4. любое 5. любое
<!-- UID: b0e5eaf9-74be-471f-b6a9-250145560d45 -->




>[!nexus_agent] **Assistant** - 30.05.2025 at 7:04
> Отлично. Я начну глубокое исследование по расширенному списку из 50 вопросов, чтобы найти реальные кодовые реализации, библиотеки, архитектуры и исходники, в которых заложены элементы архитектурной идеи AGI как нейроядра + поля + флуктуационной логики.
> 
> Буду учитывать любые источники (open-source, проприетарные, старые или новые, из любых областей и на любых языках) — с акцентом на находки, из которых можно вырезать и переиспользовать архитектурные элементы: phase routing, memory fields, reasoning agents, LoRA на поведение и т.п.
> 
> Дам знать, как только соберу подробный обзор и рекомендации.
<!-- UID: 81df5193-de91-4910-96ea-fe2e4e32013d -->

---

>[!nexus_agent] **Assistant** - 30.05.2025 at 7:17
> ## Cognitive Architectures & AGI Frameworks
> 
> - **OpenCog (Classic)** – An open-source framework (started ~2008 by Ben Goertzel’s team) for integrating AI algorithms toward AGI. It uses a shared **Atomspace** knowledge graph (memory field) and modules for reasoning (probabilistic logic networks), natural language, etc., all via a cognitive loop. OpenCog was *“a laboratory for exploring how to integrate AI systems… a compilation of decades of work”*【61†L303-L311】. It includes attention allocation (importance values that fluctuate on atoms) and a centralized *CogServer* to coordinate “codelets” (processing units), resembling a global workspace. **Status:** Largely obsolete/“bit-rotting” now【61†L285-L294】, but many components (Atomspace, PLN) were spun off and could be reused.
> 
> - **OpenCog Hyperon** – The new generation of OpenCog (alpha released 2024). Introduces the **Distributed Atomspace (DAS)** – a distributed *metagraph* memory serving as a shared “field” of knowledge – and **MeTTa**, a custom programmable language for cognitive operations【12†L115-L123】. Hyperon enables neural-symbolic integration: different reasoning engines (e.g. probabilistic logic, evolutionary learning, even integration with OpenNARS) operate on the shared Atomspace【12†L79-L87】【12†L115-L123】. It provides a core *neurokernel* with procedures (as MeTTa subgraphs) transforming the Atomspace, echoing a *core loop of cognition*. **Status:** Open-source (C++/Rust), active development; suitable for building custom AGI experiments on a common knowledge “field”【12†L67-L75】【12†L79-L87】.
> 
> - **OpenNARS** – The **Non-Axiomatic Reasoning System** by Pei Wang, an AGI-oriented framework based on a unique logic for uncertain, incomplete knowledge. It runs a continual *inference cycle*: tasks (goals/questions) and beliefs are stored in a common memory and processed in *discrete steps*【15†L6-L14】. A built-in *control mechanism* dynamically adjusts priorities of tasks and resources【15†L15-L23】 – if new urgent tasks arrive (e.g. an “alarm”), it interrupts current reasoning to handle them【15†L8-L16】. This resembles a *global workspace with attention*: only the task with highest priority is processed at a time, but tasks can spawn new tasks (sub-goals, derived questions) that re-enter the queue (a form of fluctuation logic). **Status:** Open-source (C++/Java; also a Python port). Though niche, it’s a working system for goal-driven reasoning under changing contexts, and could be repurposed or extended (e.g. hooking an LLM as a knowledge source, as some experiments have done).
> 
> - **LIDA (Learning Intelligent Decision Agent)** – A comprehensive cognitive architecture by Stan Franklin based on **Global Workspace Theory**. LIDA agents have a repeating **cognitive cycle (~10 Hz)** as their “neuro-core”【9†L141-L149】. In each cycle, sensory inputs update a *situational model* in working memory; then **attention codelets** form coalitions and compete in a Global Workspace (the “consciousness” stage) – the winning coalition is broadcast as the conscious content【11†L203-L212】【11†L213-L221】. That broadcast triggers *learning and action selection*: various memory modules (episodic, procedural, etc.) encode or reinforce knowledge, while procedural memory proposes possible actions. Finally an action is selected and executed, completing the loop【11†L208-L217】. This architecture yields a “field” of dynamic memory (the workspace) and **fluctuating codelets** that can cause spontaneous goal shifts (not just reactive to prompts). **Status:** Academic. A Java-based LIDA framework was developed and used in robotics【11†L225-L234】【11†L235-L244】. It is open-source and modular, making it adaptable – e.g. one could swap out modules or adjust the codelet scheduling for new experiments.
> 
> - **MicroPsi 2** – A cognitive architecture by Joscha Bach based on Dietrich Dörner’s Psi-Theory of drives and urges. MicroPsi represents knowledge and mental state as a network of *nodes and * with spreading activation (a “semantic field”). It extends Psi-theory’s representations with taxonomies and symbolic labels, but retains neural-like activation dynamics【64†L13-L21】. The network’s activation spreading enables *associative retrieval, planning, and learning* in a unified “field”【64†L17-L25】. MicroPsi agents also have a **motivational system** (drives for hunger, curiosity, etc.) that create internal goal signals – a form of *tension-driven behavior* (agents seek to reduce drive tension via actions). The open-source MicroPsi 2 (Python) provides a toolkit implementing these concepts【20†L293-L302】【64†L25-L29】, including GUIs for visualizing node nets and debugging agent state. **Status:** Semi-maintained. It’s usable for building simulated agents with motivations and could be repurposed – for example, as a “cognitive sandbox” for testing how emergent goals arise from fluctuating activations and drive levels.
> 
> - **AERA (Autocatalytic Endogenous Reflective Architecture)** – An ambitious AGI architecture by K. Thórisson et al. emphasizing self-driven learning. AERA agents begin with only a small *seed code (initial knowledge)* and *self-improve* by interacting with the world【22†L49-L57】【22†L51-L59】. The architecture uses **value-driven dynamic priority scheduling** to manage a *parallel network of “lines of reasoning”* (multiple processes running concurrently)【22†L51-L59】. It constantly generates and tests **micro-models** (causal hypotheses) about the environment, and *autonomously modifies its own code* when beneficial (hence “autocatalytic”). This results in **recursive self-improvement**: after a certain point the agent’s learning is self-sustaining【22†L55-L63】. The design addresses *novelty*: AERA can handle scenarios not anticipated by its programmers by re-assessing goals and adapting on the fly【22†L71-L80】【22†L79-L87】. **Status:** AERA/Replicode was implemented in C++ and used in EU projects. Some code (Replicode VM) is available on GitHub and open-source forums. It’s complex but could be reused in parts (e.g. its scheduler or meta-learning mechanism) for building systems that **generate goals internally and modify themselves** – features core to the “fluctuation logic” concept.
> 
> - **Soar** – One of the oldest cognitive architectures (Newell, Laird et al.). Soar uses a **production-rule system** for all decision-making: knowledge is encoded as if-then rules, and the agent’s reasoning is a continuous cycle of *state → select operator → apply operator → new state*【66†L185-L194】【66†L193-L200】. Notably, Soar has a built-in mechanism for **impasse**: if no rule can decide what to do (knowledge is incomplete or conflicting), it automatically creates a *subgoal* to resolve it【66†L201-L209】【66†L207-L215】. This leads to *“universal subgoaling”*: the architecture will spontaneously spawn goal-subgoal hierarchies as needed, which is analogous to a system generating new thoughts/questions on its own when stuck. When a subgoal is solved, Soar learns a new rule (“chunk”) so that next time, the impasse is resolved more directly. Thus, Soar’s long-term memory grows with experience. **Elements:** The central working memory in Soar acts like a blackboard (field) that all rules see, and multiple rules can fire in parallel (a bit of fluctuation) to propose different operators【66†L201-L209】【66†L199-L207】. **Status:** Actively maintained (in C++ and Java as **JSoar**【42†L5-L13】). It’s robust for cognitive modeling and has been interfaced with environments (e.g. robotics, game AI). One could modify Soar’s rule base or use its subgoal mechanism to experiment with *“collapse and rebuild”* patterns (e.g. deliberately introducing impasses to trigger internal brainstorms).
> 
> - **ACT-R (Python ACT-R)** – A popular cognitive architecture by John Anderson, here available in a Python implementation (pyactr)【41†L253-L261】. ACT-R organizes cognition into distinct modules (e.g. declarative memory, procedural memory, visual module, etc.) with a central **production rule system** that coordinates them. It features a **symbolic level** (discrete facts and rules) and a **subsymbolic level** (numerical parameters like activation levels, utility values)【41†L255-L263】. The subsymbolic mechanisms make it *adaptive*: for example, each chunk of memory has an “activation” that increases with use and gradually decays – a simple **memory field** effect where relevant items tend to pop up (fluctuate) when needed. ACT-R’s core loop runs in cycles of matching rules to the current goal and context, firing one to produce an action or internal change, then repeating. Many cognitive processes (learning, forgetting, latency of response) emerge from the interplay of symbolic rules and these subsymbolic values. **Status:** ACT-R is widely used in cognitive psychology; the reference Lisp implementation is complex, but **pyactr** covers “all basic cases of ACT-R modeling”【41†L255-L263】 and makes it easier to extend or integrate (since it’s Python). It could be integrated with modern AI (e.g. using an LLM as a module) to give a system with a pre-built *observer loop* (the production system watching and updating a declarative memory).
> 
> - **CLARION (pyClarion)** – A hybrid cognitive architecture by Ron Sun, distinguishing **explicit vs. implicit** knowledge. CLARION is composed of multiple subsystems – notably the **Action-Centered** and **Non-Action-Centered** subsystems (roughly, procedural vs. declarative memory), plus a **Motivational subsystem** (drives and rewards) and a **Meta-cognitive subsystem** (which monitors and regulates the others)【67†L147-L155】. Each subsystem has a dual representation: a top level for explicit symbolic knowledge and a bottom level for implicit distributed knowledge【67†L129-L137】【67†L147-L155】. For example, the action subsystem might have explicit rules for decisions and a neural network representing habit learning. The motivational subsystem generates internal rewards/goals based on drives – analogous to a *tension field* (e.g. high “fatigue” drive might trigger a goal to rest). The meta-cognitive subsystem functions as an **observer** that can adjust parameters or switch strategies (metarules) based on how the cognitive process is going. **Status:** An experimental Python implementation, **pyClarion**, exists and aims to be *“easy to learn, read, use and extend”*【39†L259-L267】【39†L269-L277】. It provides a framework to assemble agents from these pieces. It’s suitable for modification – for instance, one could plug in a neural net at the implicit level or tweak the motivational dynamics to explore *tension-driven goal modulation*.
> 
> - **Sigma (PySigma)** – A cognitive architecture by P. Rosenbloom et al., intended as a Grand Unified Theory of cognition. Sigma uses **factor graphs** as its underlying representation: everything (beliefs, images, rules) is encoded as constraints in a large graphical model, which is solved iteratively. This provides a common “field” where symbolic and sub-symbolic processing meet. The architecture supports forms of memory, perception, and reasoning all in one framework. A Python re-implementation called **PySigma** wraps this idea on PyTorch tensors【36†L3-L11】. It leverages parallel computation to update the factor graph continuously, achieving a mix of neural-network style learning with symbolic-like inference. **Elements:** Sigma inherently has a core loop of propagating constraints (which could be seen as an activation spreading), and can represent **probabilistic beliefs that get dynamically updated** – a bit like “fluctuating phases” of belief as evidence comes and goes. It also has mechanisms for firing production rules in parallel (similar to Soar) but with probabilities. **Status:** The original Sigma (in Lisp) was a research project; PySigma is experimental (few stars on GitHub, but conceptually interesting)【37†L147-L155】. It is modifiable (since built on PyTorch). One might use it to integrate neural and logical components (e.g. an LLM could feed it propositions which it maintains probabilistic truth values for, across time – implementing a kind of **continuously reasoning core** rather than one-shot prompt-answer).
> 
> ## Autonomous Agents & Multi-Agent Systems
> 
> - **AutoGPT** – The original mass-popular *“autonomous GPT-4”* experiment (2023) that demonstrated an LLM agent looping over tasks. AutoGPT prompts GPT-4 to not just answer a question, but to **generate its own next goals and actions** towards a high-level objective. It chains reasoning, tool use (via code execution, web requests, etc.), and self-critique in a continuous loop until the goal is reached or failure. Key features include *internet access for information gathering and both short-term and long-term memory management*【72†L29-L36】 – e.g. it uses a vector database or file system to store interim results (knowledge base) and can summarize context if it becomes too large. AutoGPT thus implements a rudimentary **core loop**: *Think (plan) → Act (execute tool/code) → Observe (result) → Adjust*. This looping mimics an agent “observing its own thinking” by evaluating outcomes and iterating. **Status:** Open-source (Python). It’s somewhat brittle and requires careful prompting, but it inspired many variants. As a framework, it’s highly modifiable – many plugins exist to extend its toolset, and one can swap the language model or memory backend easily. It’s a good starting point for building an agent that **generates thoughts/events autonomously rather than just reacting** to a single user prompt.
> 
> - **BabyAGI** – Originally a simple task-driven agent proposed by Yohei Nakajima. It introduced the idea of an **automatic task list**: given an objective, BabyAGI would keep a list of pending tasks, pop the top one to execute (using an LLM), then dynamically insert new tasks that the LLM deems necessary based on the result【60†L269-L277】. This creates a self-directed workflow – the agent essentially *plans and re-plans* on the fly. The initial BabyAGI used GPT-4 to do things like: given a goal “Increase Twitter followers”, it would create tasks (“Research popular hashtags”, “Tweet about trending topic”, etc.), execute them (via API calls or just simulating), then evaluate and make new tasks. It treats memory not as fixed prompts but as an evolving list (akin to a scratchpad or field of intentions). Some implementations use vector databases to allow the agent to *remember facts across runs* (long-term memory) and **reinject relevant information when needed** (like a context field). **Status:** Open-source and widely forked. The original code is very short (a few hundred lines), making it easy to modify. It’s *“not meant for production… built to share ideas”*【60†L275-L283】, so developers often extend it – e.g. adding a better scheduler (prioritizing tasks = a form of phase routing) or integrating new types of tasks (API calls, etc.). BabyAGI embodies the *“collapse → rebuild”* idea in that it can summarize completed tasks (collapsing detail) and generate new higher-level tasks (rebuilding plan) as it goes.
> 
> - **MiniAGI** – A minimalistic autonomous agent framework inspired by AutoGPT/BabyAGI, focusing on simplicity. **MiniAGI** (by muellerberndt on GitHub) combines *“a robust prompt with a minimal set of tools, chain-of-thought, and memory”*【59†L1-L4】 to let GPT-3.5/4 solve user-given objectives. It typically includes: a looping prompt that asks the LLM to propose an action, a few built-in tools (e.g. web search, file I/O), and a memory of past actions/results. Despite its simplicity, MiniAGI demonstrates the **core loop pattern** clearly: the LLM is prompted to output a thought, an action, and an action input; the action executes (if it’s a tool), and the result is fed back in along with an updated prompt state, then the loop repeats. This continues until the LLM outputs that it’s done. Because it’s so small, it’s easy to tweak (e.g. adding a new tool or changing the prompt format to influence the agent’s “phase” of reasoning). **Status:** Open-source (Python). It’s essentially a single Python file example【59†L7-L15】. This makes it a great sandbox to experiment with “observer loops” – e.g. one could add a second LLM as a *critic* that watches the main loop’s outputs (a meta-routing addition), or adjust how memory is stored (vector vs. text). The minimal design ensures *few hidden variables*, so the *fluctuations in the agent’s behavior* come mostly from the LLM’s own reasoning.
> 
> - **Microsoft AutoGen** – An open-source framework from Microsoft Research for building **multi-agent conversational applications**【57†L15-L23】. AutoGen provides an API to create multiple agent personas (each backed by an LLM or tool) and define a conversation workflow between them. For example, one can spin up a “Developer Agent” and a “Critic Agent” and let them talk to each other to solve a coding task. AutoGen handles the message-passing, turn-taking, and can also integrate tools and humans in the loop【57†L3-L11】【57†L19-L27】. Essentially, it offers **event-driven agent loops** – you can set agents to react to messages or specific events (errors, timeouts), which is a flexible way to implement *meta-cognition* (an agent can observe another’s output and intervene if needed). Agents are fully configurable (you define their prompt, e.g. role instructions) and the system can be orchestrated via a simple *YAML/JSON config* (for defining agents and their conversations)【44†L347-L355】【44†L358-L366】. **Status:** Active (Python). It’s quite suitable for modification – you can plug in custom tools or swap in open models for the agents. AutoGen is useful for creating *phase-aware reasoning*, e.g., one agent could be in charge of generating ideas (brainstorm phase) and another in charge of filtering or refining them (critic phase), effectively implementing a “thinker–observer” pair.
> 
> - **CAMEL** – *Communicative Agents for “Mind” Exploration of LLM Society*, initially a research paper and now a community framework【56†L15-L23】【73†L327-L335】. CAMEL popularized the **AI-AI collaboration** approach: two (or more) ChatGPT instances role-play with each other to solve a task without human intervention. Typically, one agent is assigned as, say, *“User” (with a certain goal)* and another as *“AI assistant”*, and they converse back-and-forth generating solutions. This setup can be extended to many agents (e.g. an “agent society”). The CAMEL framework provides an environment to manage these multi-agent dialogs and track the state. It emphasizes *“evolvability and scalability”*: supporting large numbers of agents interacting and even learning from generated data【73†L343-L352】. Notably, CAMEL and its extensions (like the OWL framework【56†L9-L17】) aim for *persistent agent state* and **iterative improvement** – agents maintain memories and can refine their own prompts or strategies over time【73†L353-L361】【73†L345-L353】. For example, a pair of agents might tackle a coding task by iterative Q&A, effectively performing a self-contained *field of thought* where ideas are exchanged (the conversation = shared workspace) and no outside prompt is needed after initialization. **Status:** Open-source (Python) and community-driven. It’s highly configurable – you can define custom roles (e.g. Teacher & Student agents for a tutoring scenario) and let them loose. CAMEL showcases how **intention and “axis” can be explicitly given to agents** – each agent’s role prompt acts as an *intention axis* (e.g. one agent given the intention to maximize accuracy, another to maximize simplicity, and their debate will navigate those axes).
> 
> - **MetaGPT (ChatDev)** – An open-source multi-agent framework that treats AI development as an organizational problem【76†L79-L87】. In MetaGPT, multiple specialized agents (modeled after a software startup’s roles: Product Manager, Architect, Engineer, Tester, etc.) are created. They communicate via standard operating procedures to gradually refine a software product from a specification. This is effectively a *simulation of a team of AIs*, where each agent contributes from its perspective. MetaGPT orchestrates their cooperation so that they stay on task and exchange information when needed【76†L79-L87】【76†L90-L94】. For example, the Product Manager agent might break down a user request into tasks, the Architect agent produces a design, the Engineer writes code, and the Tester reviews it – all through conversation and artifact sharing. The framework highlights how *complex cognition can emerge from interacting sub-agents*, each focusing on one facet of the problem (similar to dividing an AGI’s mind into modules that talk). **Elements:** There is a persistent memory of the project (documents they collaboratively produce), essentially a shared field; there is fluctuation logic in how agents might request clarification or suggest changes spontaneously if something is unclear. **Status:** Released by team DeepWisdom, gained significant GitHub traction【76†L85-L93】. It’s usable (with GPT-4 API) for non-trivial projects. Modifiability: one can add new roles (e.g. a Security Auditor agent) or change the interaction protocols. MetaGPT underscores the idea of *modularizing an AGI* into specialized units with an **explicit meta-controller (the coordination script)** ensuring everyone stays aligned – a design that can be repurposed for other domains (not just coding, but any multi-faceted task).
> 
> - **Generative Agents (Stanford “Smallville” Simulation)** – This is the code and framework from the paper *“Generative Agents: Interactive Simulacra of Human Behavior”* (Park et al. 2023)【25†L254-L261】. It creates believable AI agents that **live** in a sandbox environment (a fictional town) and interact in real-time, powered by LLMs plus memory and planning modules. Each agent has: (1) a **Long-term memory store** of observations (encoded as text embeddings), (2) a **Reflection mechanism** that periodically distills memories into higher-level insights (e.g. “I often see Sam at the coffee shop in mornings, we must be friends”), and (3) a **Planning component** that decides the agent’s daily schedule or next actions based on its intentions and recent reflections【25†L254-L262】. These agents operate continuously, not just turn-by-turn – e.g., an agent might wake up, go about their routine, start a conversation with another agent, remember it, later reflect “I told a lie, I feel bad”, and decide to apologize the next day. This showcases *emergent behavior through internal simulation*, not direct prompting. **Architecture elements:** There is a core loop where agents retrieve relevant memories (contextual cues = field of memory) and then respond or act. The “field” concept is represented by the *sum of an agent’s experiences*, which the system queries by embedding similarity – effectively treating memory as a context-sensitive field rather than a static database. Fluctuation logic is seen when unrelated older memories resurface due to an associative cue, changing the agent’s behavior unexpectedly (like a thought “popping up”). **Status:** The project’s code is available【25†L254-L262】. It’s research-grade (requires an LLM like GPT-4 and a vector DB for memory). For reuse, one could simplify it to a REPL-based agent: the memory–reflection–planning loop can be run in any environment. It’s a prime example of building an agent that **generates its own “thoughts,” goals, and social interactions** over a long lifespan.
> 
> - **Voyager** – An *“open-ended embodied agent”* that uses GPT-4 to **autonomously explore and learn** in Minecraft【51†L17-L25】. Voyager (Wang et al. 2023) is noteworthy because it **doesn’t receive prompts for each task** – instead, it has an internal loop that (a) *explores the environment*, (b) *self-generates new high-level goals* (e.g. “build a shelter” might arise after it learns about nightfall), (c) *writes code (in Minecraft’s API) to act*, and (d) *stores new skills* in a skill library for future reuse【51†L25-L33】. It also evaluates outcomes: if an attempt fails, it can debug its approach or try alternative strategies. This resembles a curious child, learning iteratively. Key architecture aspects: Voyager has a **curriculum generator** (goal setter) – a component that looks at the world state and its skill library to propose the next interesting objective. This is guided by an intrinsic reward for novelty. There is also a feedback loop where if the code GPT-4 wrote results in an error or unexpected outcome, Voyager notes that and tries to fix it (an implicit meta-cognitive step of self-debugging). The *memory field* here is the skill library (and world state knowledge) which grows over time; *fluctuation logic* appears in the way Voyager might drop a goal and pick up a new thread if it finds something more enticing (since multiple goals can be in flight, it prioritizes adaptively). **Status:** Open-source (Python)【51†L1-L9】, though it needs access to the Minecraft environment and GPT-4 API. Adapting Voyager to other sandbox worlds or using smaller models is possible. It’s a strong template for *tension-driven exploration*: one could introduce a simulated “drive” (like curiosity or hunger in-game) and Voyager’s framework would support the agent roaming the world to satisfy those internal drives.
> 
> ## Reasoning, Memory, and Creative Systems
> 
> - **Copycat & Metacat** – **Copycat** (Hofstadter & Mitchell, 1980s) is a classic cognitive program for analogical reasoning in a microworld of letter strings. Its architecture is highly illustrative of an AGI-style “thinking field”: it consists of a **Slipnet** (a long-term semantic network of concepts and their relationships), a **Workspace** (short-term memory where the specific problem is represented, like a blackboard), and a **Coderack** of **codelets** (small stochastic processes that make incremental changes)【69†L7-L15】. When Copycat is given a problem (e.g. “abc -> abd; what is ijk -> ?”), it doesn’t use a deterministic algorithm; instead, *codelets pop off the coderack at random*, inspect or modify parts of the workspace (e.g. propose that “abc” has the structure alphabetical sequence), and update activations in the Slipnet (increasing activation for concepts like *“successor”* or *“predecessor”*)【69†L1-L9】【69†L7-L15】. The Slipnet’s activation, in turn, biases which codelets get put on the coderack next (e.g. if “successor relation” is highly active, codelets that try applying successor rules are more likely)【69†L9-L16】. This feedback loop continues until a coherent solution “snaps out”. This is a prime example of **fluctuation logic**: the solution emerges from an aperiodic, network-driven process, not a direct prompt response. **Metacat** (Marshall, 2002) is an extension that adds self-observation: it *monitors Copycat’s own progress*, recognizing when a mapping it tried was novel or when it found a clever trick, and it stores abstracted descriptions of its own reasoning as episodic memories【71†L1-L9】. This allows Metacat to say, for instance, “this problem was similar to an earlier one” or to explain its solution in a creative way. **Status:** Copycat has modern re-implementations (in JavaScript, Python, etc.)【50†L1-L4】. Though a toy domain, the system is usable and one can adjust parameters (e.g. temperature = how chaotic vs. focused the codelet selection is). It’s great for experimenting with a **self-tuning thought process**: one could incorporate new codelets or a different Slipnet (say, a concept network for another domain) to see if fluid analogies arise. The Metacat idea (self-watch and log interesting moments) is highly relevant to AGI “core loops” and could be ported as a module in other frameworks (e.g. an LLM agent that keeps a journal of why it chose each step, for later analysis).
> 
> - **MEXICA** – A **creative writing system** (R. Pérez y Pérez, 1999) that generates short stories (about Aztec warriors) using an **engagement–reflection cycle** rather than direct planning【55†L129-L137】. During *Engagement*, MEXICA improvisationally extends a story step by step, guided by a *“tension” metric* – it tracks emotional tension in the story (based on character relationships and events) and prefers actions that are narratively appropriate (e.g. if tension is low, maybe introduce a conflict)【53†L31-L39】【53†L35-L43】. This is done without a preset goal, simulating how a human writer might *just write and see where it goes*. Then in the *Reflection* phase, it evaluates the story’s coherence and tension arc as a whole, and may revise or restart if it’s not satisfied. The knowledge is encoded as *cases* (past story fragments with known tension patterns) and rules for modifying tension. Notably, MEXICA explicitly uses a **tension representation**: a numeric timeline that rises and falls with story events, which it tries to shape into a nice dramatic arc【53†L1-L9】【53†L29-L37】. This is **tension-driven generation** in practice – the system’s driving variable is not a user prompt but an internal metric (tension) it seeks to modulate. MEXICA’s architecture can be seen as an early form of a *“creative cognitive loop”* where an internal critic (reflection) guides the free generation (engagement). **Status:** It’s a research system; the original was in Delphi, but the concepts are documented and could be reimplemented. The importance for AGI field-architecture is showing that **internal dynamics (like an emotion model)** can replace prompt-reaction – the program can *start its own thought process* because, for example, a certain tension threshold was reached or a drive triggered. MEXICA is adaptable: one could take the engagement–reflection loop and apply it to, say, music generation (driving an AI composer with a “surprise” metric as the internal field).
> 
> - **Nvidia AgentIQ (Reasoning YAML)** – Not a single agent per se, but a component of NVIDIA’s AI workflow where an **LLM agent’s reasoning process is defined declaratively in YAML**【44†L347-L355】【44†L358-L366】. This is interesting as a *practical tool for phase-structured prompting*. For example, one can define in a config: first use a “Planning function” (perhaps a ReAct prompt) to produce a detailed plan, then use an “Execution function” to act step-by-step. The Reasoning Agent in AgentIQ basically sits on top of another base agent and adds a **phase of explicit planning**【44†L341-L349】【44†L358-L366】. The YAML might specify something like: `workflow: reasoning_agent` with an `augmented_fn: react_agent` (meaning it will take a basic React agent and prepend a chain-of-thought planning phase to its operation)【44†L358-L366】【44†L372-L381】. This meta-agent watches the base agent’s tool uses and can intervene or re-plan if needed. It also allows plugin of custom tools and has an **observer pattern** (their reactive utils) to, say, trigger a recap if the agent loops too many times【44†L313-L321】【44†L338-L346】. **Status:** Provided as part of NVIDIA’s AgentIQ (which may be proprietary, but the concept is documented). The idea can be reproduced with open tools – it shows how using a simple declarative format, one can create *frames for thinking phases (plan → judge → execute, etc.)*. This lowers the barrier to experiment with phase routing: e.g., you could quickly configure an agent that first does *brainstorming*, then *critical evaluation*, then final answer – all through YAML instructions rather than hard-coding prompts.
> 
> - **LLaMA-LoRA “Reasoning Chains”** – An approach to fine-tuning LLMs on the *process* of reasoning rather than just the outputs. Chen et al. (2023) propose **LLaMA-LoRA Neural Prompt Engineering**, where they fine-tuned a LLaMA model (using Low-Rank Adaptation) to *generate logical reasoning chains in Chinese text*【49†L5-L12】. Essentially, the model was trained to output not just an answer, but a step-by-step thought process (modeled after human-written or synthesized solutions). By applying LoRA, they injected this reasoning behavior without having to full-scale train the model, making it efficient. The result was a model that, when prompted, would produce an *organized chain-of-thought* on its own. This is relevant to AGI architectures in that it **embeds a core loop (think step by step) within the model itself** – the phases of reasoning are learned and internalized, rather than orchestrated externally by a wrapper. Similarly, others have used QLoRA (Quantized LoRA) to fine-tune smaller models on *datasets of thought processes or tool use*. For instance, projects have fine-tuned models on ReAct-like traces, so the model learns to call tools and then reflect, etc., as part of its generation. **Status:** LLaMA-LoRA reasoning models are research prototypes; some might be available on HuggingFace. Adapting them is feasible – one could take a foundation model and fine-tune with LoRA on transcripts of an AGI “field” (like dialogues between a thinker and an observer, or memory contents being accessed). This way, elements like *fluctuation logic* (e.g. digressions and returns in a conversation) could be learned as a style. LoRA and QLoRA make experimentation accessible because they are lightweight. In summary, this line of work shows how **learning** can supplement manual design of the core loop – the model itself can internalize a core thinking pattern (like “debate with yourself before answering”), which you can then plug into a larger architecture.
> 
> - **Hyperdimensional Computing (TorchHD)** – A library for **Vector Symbolic Architectures (VSA)**【58†L1-L9】, which can serve as a *memory substrate* in AGI systems. In a VSA, items are encoded as high-dimensional vectors, and sets or sequences of items are combined into single vectors (by addition, binding, etc.) – effectively distributing information across a “memory field” in a way that overlaps and superposes memories. TorchHD provides tools to encode data (like symbols, images) into hypervectors and perform operations like similarity search, bundling, etc. Unlike a typical database, retrieval is fuzzy by nature: if you probe with something similar to what was stored, you’ll get a match (even if not exact). This aligns with the idea of memory as a semantic field rather than exact address lookup. **Use in architectures:** One could use VSA to implement a *Global Workspace*: different subsystems could emit vectors representing content; these are added together into a blackboard vector; and an attention mechanism can detect which concept has the highest activation. For example, in a fluctuation scenario, multiple hypotheses are encoded and superposed – the system might naturally gravitate to the one with strongest overlap with current context (because its vector stands out). **Status:** TorchHD (Python) is open-source and easy to integrate with PyTorch models【58†L1-L9】. It’s suitable for modification – e.g., using HD vectors as an intermediary between an LLM and a symbolic reasoner (LLM’s outputs can be “memorized” in a vector that can be later decoded to retrieve approximate original thoughts). Projects aiming at *“memory not as DB but as distributed field”* can benefit from these techniques. 
> 
> - **NuPIC (Hierarchical Temporal Memory)** – An open-source implementation of **HTM**, a theory of the neocortex by Jeff Hawkins (Numenta) focusing on time-series learning. NuPIC uses networks of cortical neuron models to learn temporal patterns via unsupervised Hebbian-like mechanisms. The system continuously processes input streams and makes predictions, detecting anomalies when inputs don’t match predictions. In effect, it’s a *memory system that is stateful and always-active*, rather than query-based. HTM’s components include a **Spatial Pooler** (that forms sparse distributed representations – SDRs – for inputs, like a field of active bits encoding the essence of recent input) and a **Temporal Memory** (cells that learn sequences, so that given the current context pattern they predict the likely next pattern). This can be seen as a *fluctuating field of predictions* that changes with each time step. Notably, HTM doesn’t require explicit prompts – it’s more akin to a brain region that is always on, anticipating. **Relevance:** If we think of AGI architecture needing a memory that recalls and anticipates contextual shifts, HTM is a biologically inspired solution. It treats memory as a field of activation across many binary units (an SDR), where overlap between patterns signifies similarity (semantic axes can be inferred by which bits overlap). **Status:** NuPIC was open-sourced in 2013【79†L1-L9】【79†L7-L15】 (implemented in Python/C++). Though Numenta has moved focus, the code is available (the last “nupic” is legacy, but community forks exist). It’s quite adaptable – e.g., one could use HTM neurons as part of an *observer module* to detect when an agent’s behavior deviates from normal (an anomaly signal could trigger a reflective phase). Or HTM could serve as a *background learning system* that encodes the agent’s observations continuously, giving the AGI a kind of sensory memory that is always accumulating and forgetting (rather than a static knowledge base). In summary, HTM/NuPIC provides a working example of **memory as a dynamic, predictive field** with no need for explicit queries – a useful component to integrate for an AGI that should “think in streams” instead of discrete question-answer.
> 
> - **GBBopen (Blackboard Framework)** – An open-source Common Lisp framework for building **blackboard systems**【80†L5-L13】. Blackboard architecture is a classic model in AI where a common global memory (the *blackboard*) is incrementally updated by disparate knowledge sources (KS), and a control loop decides which KS to invoke next based on the state of the blackboard. GBBopen provides the infrastructure: you can define blackboard spaces, knowledge source modules, and scheduling strategies. In a modern AGI context, a blackboard can function as the **“field” where partial thoughts and results appear**, and knowledge source processes can be anything from simple rules to entire subsystems (vision, language, etc.). The control component acts like a *meta-manager*, potentially analogous to a global workspace’s spotlight of attention. The cooperative problem solving via blackboard is **phase-aware** in that the system can go through phases like hypothesis generation, hypothesis testing, solution consolidation, each implemented by different KS that get triggered as the blackboard fills with certain patterns. For example, in a complex puzzle-solving, one KS might watch for when a partial solution is formed and then trigger another KS to attempt to complete it (phase routing based on state). **Status:** Blackboard systems like GBBopen are mature but somewhat “forgotten” outside specific domains. GBBopen is robust and *“framework-like”*, meaning you can tailor it to many problems. It’s highly suitable for modification – you can integrate new modules (even call out to Python or use it just as the logic, while your main agent code is elsewhere). Using a blackboard approach could unify an LLM, a symbolic reasoner, and a reactive planner in one architecture: all post their suggestions to the board and a scheduler picks which to apply. This aligns perfectly with the AGI concept of a shared “mind-space” where *events (thoughts) occur and compete*, and an *observer loop* (the scheduler) modulates what happens next【80†L9-L17】.
> 
> - **Plug-and-Play Language Model (PPLM)** – A technique and tool from Uber AI (2019) for *controllable text generation* that can be seen as offering “axes” of generation explicitly. PPLM allows you to **steer a pretrained language model’s output** by plugging in tiny attribute models for desired directions (e.g. a topic classifier or a sentiment model)【81†L1-L9】【81†L11-L19】. As the language model generates text, PPLM adjusts the hidden activations at each step via gradient ascent to increase the likelihood of attributes you want (say, positive sentiment)【81†L29-L37】. This means the user can literally set an “intention axis” – e.g., generate a paragraph *about space (topic)* and *in an upbeat tone (sentiment)* – and the model will follow those without having them explicitly in the prompt. In an architectural sense, PPLM is like having an **overlay field** on top of the model’s normal next-word predictions: the attribute models define certain directions in activation space, and the generation process is modulated (pushed/pulled) along those axes. It’s reminiscent of having dials for aspects of “mindset” of the model (akin to how one might imagine an AGI’s mood or focus could be dialed). **Status:** Open-source code available【81†L1-L9】. It was demonstrated with GPT-2. For adaptation, one could use PPLM on newer models or even in multi-step reasoning – e.g., when an agent is doing chain-of-thought, one could apply an “attribute” that biases the chain-of-thought to be *more mathematically rigorous* or *more skeptical*, etc., effectively an on-the-fly meta-control. The PPLM approach highlights that *direct vector-level intervention* can implement aspects of a cognitive architecture (like biases or goals) **without retraining the model**. This is a different way to achieve a form of “field influence” on a model’s thinking: instead of changing the logic or prompts, you change the underlying vector calculus to nudge the outcome toward the desired semantic axis【81†L30-L34】. Such techniques could be combined with others above (for instance, a blackboard agent could use PPLM to ensure one of its knowledge sources always speaks in a certain style or perspective, serving as an axis-expressing interface).
> 
> Each of these projects, past or present, offers fragments of the overall AGI puzzle – from core cognitive loops and self-reflection mechanisms to memory fields, phase-based reasoning, and multi-agent interplay. By studying and **combining these implementations**, one could begin to assemble a new unified framework: e.g., using a blackboard (GBBopen or a custom one) as the global “field”, running an LLM-based planner with Reflexion as the core loop, plugging in a Copycat-like module for fluid analogies, a MicroPsi-like drive model for motivation, and using VSA/HTM for distributed memory – all orchestrated in a CAMEL/AutoGen style multi-agent society. While this is non-trivial, the **source code and ideas are available** from these projects to be remixed into a novel AGI architecture. Each contributes something tangible: a working codebase or prototype demonstrating that aspect X of “AGI-field” is possible in code, not just theory. 【61†L303-L311】【55†L129-L137】
<!-- UID: b6243b0d-a08a-4566-ba23-ad6c8d06460b -->