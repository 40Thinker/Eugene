>[!nexus_user] **User** - 02.08.2025 at 1:41
> Я не знаю всех плагинов, которые имеют искусственный интеллект в Obsidian, так что скорее интересуют глобальные практики, и они в большей степени, я думаю, зависят от модели, а не от Obsidian. В первую очередь интересует качественный текст, и каким образом можно регулировать объем этого текста, объема ответа. В идеале, инструкции должны быть в системных промптах очень продуманные, учитывающие все гипотетические ситуации. И после их отработки на OpenAI должен быть переход на локальные модели. И чтобы эти инструкции продолжили с минимальными модификациями работать. Хочется понимать, какой предел объема инструкции. Можно ли на 5 листов А4 написать их, и нужно ли оплачивать токены за все эти инструкции. Еще такой вопрос. Я не вижу опции, чтобы каждый раз вся история переписок сохранялась. Так и должно быть? Переписки не подкладываются? Если они не подкладываются, то инструкции должны быть довольно подробными, чтобы если каждый раз запрос как будто с чистого листа, модель полностью могла войти в курс дела сразу. Продолжение следует.
<!-- UID: b63c0171-a87f-4b50-afa3-d4d2ac20b53d -->




>[!nexus_agent] **Assistant** - 02.08.2025 at 1:41
> Принято. Я проведу исследование по лучшим практикам настройки AI-плагинов в Obsidian с фокусом на:
> 
> - написание системных промптов, максимально универсальных между OpenAI API и локальными моделями (Qwen, LLaMA3, DeepSeek, Mistral);
> - управление объёмом ответа и объёмом инструкций (включая лимиты токенов);
> - подходы к работе без сохранения истории (stateless), где промпт — единственный контекст;
> - возможности переноса инструкций с облачных моделей на локальные с минимальной адаптацией;
> - как писать инструкции так, чтобы они покрывали все гипотетические случаи (robust prompting);
> - практики использования YAML/JSON-конфигураций и конкретные примеры настройки плагинов (Scribe, Text Generator и др.).
> 
> Сообщу, как только соберу материал с разбором по разделам, включая примеры и рекомендации.
<!-- UID: 3ecff84f-0963-4ea7-a8ad-a122352ddbc2 -->

---

>[!nexus_agent] **Assistant** - 02.08.2025 at 1:53
> # Best Practices for AI Plugins in Obsidian (2024–2025)
> 
> Using AI inside Obsidian has become increasingly powerful with new plugins and models. This guide covers how to configure AI plugins for best results – from crafting high-quality system prompts to integrating local models like LLaMA, Qwen, DeepSeek, Mistral, Vicuna, etc. We’ll also discuss prompt strategies (for summaries, code, mindmaps, etc.), handling context limitations, and key differences between cloud APIs (GPT-4, Claude, Google’s Gemini) and local LLMs. The goal is to help you achieve high-quality outputs and control the volume/format of responses in Obsidian’s environment.
> 
> ## Crafting Effective System Prompts
> 
> A **system prompt** (sometimes called an “instruction prompt”) sets the behavior and context for the AI assistant. Writing a thorough system prompt is crucial for getting reliable and relevant answers, especially since many Obsidian plugin calls are essentially stateless (each query may be handled independently without conversation memory). Here are best practices for system prompts:
> 
> - **Be Clear and Comprehensive:** Spell out the assistant’s role, style, and the task in detail. For example, you might start with a role definition like *“You are a note-taking assistant that answers in clear Markdown format”* and then list specific guidelines. The prompt can include rules about tone, format (e.g. *“use bullet points for summaries”*), and what to avoid. High-quality prompts anticipate edge cases and explicitly instruct the model how to handle them. In complex workflows, your prompt might end up fairly long – it’s not uncommon for advanced AI applications to use multi-paragraph system prompts covering various scenarios【16†L61-L69】【16†L73-L80】. In fact, treating your prompts **like code** – version controlling them, testing changes, and iterating – is recommended【16†L73-L80】. Small wording changes can have big effects on output, so refine your system prompt over time for your needs.
> 
> - **Include Format Instructions:** If you need output in Markdown or a specific structure, say so explicitly. For example: *“Respond with a list of 3–5 bullet points in Markdown, no other text.”* If the plugin will insert the answer into your note, ensure the format fits (e.g. no extraneous top-level headings if it will be nested under an existing heading). Being specific with format helps avoid cleanup later.
> 
> - **Control Tone and Scope:** You can instruct the model to take a certain tone (e.g. formal, enthusiastic) or perspective. For knowledge-base Q&A, you might say *“Use only the provided notes to answer; if unsure, say you don’t know”*. For summarization, you might emphasize focusing on main ideas and brevity. These guidelines go into the system prompt so they apply to every query.
> 
> - **Length of System Prompts:** You asked if a system prompt can be *as long as 5 A4 pages* – it *can* be very lengthy, **as long as it stays within the model’s context window**. However, remember that **the entire prompt (including your system message *and* the user query and any inserted context) counts toward the token limit** of the model【25†L125-L133】. For example, GPT-3.5 has around a 4k token limit, GPT-4 can be 8k to 32k depending on version, and many local models (LLaMA 2/3, etc.) have context limits like 4k or 8k. If you fill most of that with a huge system prompt, you leave fewer tokens for the model’s answer【25†L127-L133】. Also, **OpenAI’s API charges tokens for all parts of the prompt**, including the system instructions – so a very large prompt will cost more credits to use. In practice, try to make the prompt only as long as necessary. You want it detailed enough to cover all important instructions, but avoid repetitive or irrelevant filler. Often you can achieve the same effect with a well-structured one-page prompt as a rambling five-page prompt by using concise language. (Longer prompts *can* improve quality up to a point, but beyond that you get diminishing returns【25†L185-L191】.) It’s a balance: give the model what it needs to “know” every time, but don’t overload it with superfluous text.
> 
> - **Token and Cost Considerations:** Keep an eye on token counts. If you’re using OpenAI, you can estimate tokens or use plugins that calculate token usage. As a rule, **don’t exceed ~75% of the max context length with your input** – leave room for the response. For example, if using a 4,000-token model, try to keep system+user prompt under 3000 tokens so the model can output a decent amount. If you do need an extremely large instruction (say you have a style guide or persona that *must* always be included), be aware you might need a model with an expanded context (like Anthropic Claude 100k context, or GPT-4 32k) to handle it without truncation. OpenAI’s docs and community confirm that system messages count toward cost and context just like user messages【25†L125-L133】, so yes, you “pay” in tokens for long instructions. Some providers have prompt caching for long static prompts to reduce repeated cost, but as a rule assume each call includes those tokens in billing【25†L121-L129】.
> 
> **Summary:** Write system prompts that are clear, thorough, and structured. Cover the assistant’s role, the desired format and length of answers, and any do’s/don’ts. Make it as long as needed but as short as possible – be mindful of token limits and cost. Treat the prompt as an evolving piece of logic for your “AI assistant” and refine it with testing, just like code.
> 
> ## Controlling Output Length and Quality
> 
> One common concern is getting responses that are too short, too long, or cut off. In Obsidian AI plugins, you often have ways to control or influence the answer length:
> 
> - **Model Parameters:** Many plugins (Text Generator, LLM Assistant, etc.) expose parameters like *Max Tokens*, *Temperature*, etc. **Max Tokens** is the hard limit on the response length. For example, in the Text Generator plugin settings, you might set *Max Tokens = 650* to allow fairly detailed outputs; users found that if this is set too low, the model would produce incomplete cut-off answers【34†L1-L4】. Setting it higher lets the model write more before stopping. Keep in mind this is a maximum, not a guarantee – the model might end naturally sooner – but if it has more to say, it won’t exceed this. As noted above, leaving some headroom is good: if you cap max_tokens very low, the model may truncate mid-sentence and also might not “think” as freely. One source notes that strictly limiting output length in the prompt can reduce quality, since it constrains the model’s ability to reason freely【25†L131-L139】. So use reasonable limits that fit your use case (e.g. maybe 200–300 tokens for a summary, but 1000+ if you want a detailed essay).
> 
> - **Prompt Instructions for Length:** You can also directly instruct the AI about the desired length or detail. For example: *“Give a brief answer (2-3 sentences)”* or *“Provide a detailed explanation (several paragraphs)”*. Including this in the user prompt or system prompt can guide the model. The OpenAI and Anthropic models usually follow such instructions well. For local models, explicit length instructions may be less reliably followed (they might ramble on unless stopped), so combining an instruction with a max token limit is wise.
> 
> - **Chaining or Summarizing:** If you need extremely long outputs (e.g. summarizing a whole book or generating a full article), you might have to break the task into chunks due to context limits. This is more about handling context (discussed later), but as a strategy, you could have the AI produce output section by section or summarize parts and then combine. This way each response is of manageable length.
> 
> - **Temperature and Quality:** The *temperature* parameter influences randomness. A lower temperature (e.g. 0.2) makes outputs more focused and shorter (often) because the model is choosing more predictable tokens, whereas a high temperature (0.8+) makes it more verbose or creative and possibly longer. If you want *concise* answers, try a lower temperature and explicitly say “be concise.” If you want a more expansive, creative answer, you can turn temperature up. It’s often worth experimenting per task.
> 
> - **Iterative Refinement:** Especially when using templates or custom prompts, you may need to tweak to get the desired length. A great practice is to **iteratively test and refine prompts**. For instance, one guide suggests: run the template, see if the output is too short/long, then adjust the wording of your prompt (or the max_tokens setting) and try again【35†L1-L4】. You can explicitly tell the model “avoid lengthy digressions” or conversely “feel free to elaborate on each point,” depending on what you need.
> 
> In summary, use the plugin’s settings (max tokens, etc.) and prompt wording to steer how long the answer should be. Don’t set the limits so low that answers get cut off【34†L1-L4】, but do provide enough guidance so the model knows whether you expect a two-sentence answer or a two-page answer.
> 
> ## Prompt Strategies for Different Sections and Tasks
> 
> Different use-cases call for different prompt “styles.” Obsidian’s AI plugins often allow you to define **prompt templates** or sections to tailor output for specific purposes. For example, the **Scribe** plugin (for transcribing and summarizing voice notes) splits the AI output into multiple sections like “Summary,” “Insights,” “Mermaid Chart,” and “Q&A.” Each section has its own injected prompt instructions. Adopting this kind of approach – custom prompting per section or task – can greatly improve output quality. Here are some techniques:
> 
> 【3†embed_image】 *Example of section-specific prompt instructions in the Scribe plugin. Each section (Summary, Insights, Mermaid Chart, etc.) has a custom directive guiding the AI’s output format and content.* 
> 
> - **Summaries:** If you want a summary of a note or transcript, instruct the model to be concise and focus on main points. A good prompt might say: *“Provide a summary in bullet points, covering the primary ideas and conclusions. Use Markdown list format.”* In Scribe’s default, the summary section is guided to produce a brief synopsis in bullet form, without duplicating the entire content. By explicitly stating “concise bullet points containing the primary points,” you ensure the model doesn’t ramble【1†embed_image】.
> 
> - **Insights/Analysis:** To get insights or reflections (e.g. “what are the key takeaways or your thoughts on this content?”), your prompt can encourage a slightly more open-ended answer than a factual summary. For instance: *“List a few insights or implications from the above notes. What stands out as important or interesting? Provide 1-2 paragraphs of analysis.”* The Scribe plugin’s Insights section prompt says something like “a brief section with what insights and enhancements you think of, in a few bullet points and maybe a short paragraph” – this nudges the AI to go beyond re-stating facts and actually interpret or suggest improvements.
> 
> - **Code or Diagrams (Mindmaps):** If you want the AI to output a code snippet or a diagram (like a Mermaid mindmap), **the prompt must be very specific about format**. For example, to get a Mermaid mindmap of ideas, you might prompt: *“Using Mermaid syntax, create a mind map diagram of the relationships mentioned above. Do not include any explanatory text, only provide the Mermaid diagram code fenced by ```mermaid```.”* In our Scribe example, the Mermaid Chart section’s instruction explicitly tells the AI to output **only** a valid Mermaid diagram, and even warns against including characters that might break the syntax (no quotes, no stray newlines in node labels)【3†embed_image】. This level of detail is often needed because the model might otherwise include an English explanation or invalid characters. Similarly, for code output you would instruct: *“Output only the code in a fenced block, no surrounding explanation.”* Prompt *injection* like this ensures the assistant’s behavior is scoped to the section (e.g. producing a diagram vs. producing text) and in the correct format.
> 
> - **Q&A or Instruction-Following:** Some setups have a section where the AI answers user questions (like Scribe’s “Answered Questions” section triggered by “Hey Scribe” in the transcript). To handle this, the system prompt or section prompt should tell the model how to recognize and answer those questions. For instance: *“If the user asks a question in the note (indicated by ‘Hey Scribe’), answer it in a new section titled ‘Answered Questions’. Use a heading for the question and then answer under it.”* In the Scribe plugin, the instruction says to summarize the question as a header and give a clear answer below【4†embed_image】. Even if you aren’t using Scribe, you might have a workflow where the AI scans a note for question marks or directives and appends answers – this kind of prompt needs to be carefully designed to avoid the AI answering things out of context or hallucinating questions. Include triggers (“when you see X…”) and desired format for the answers.
> 
> - **Step-by-step or Multi-part Outputs:** If you need the AI to produce multiple sections in one go (say a summary, then some action items, then a quiz), it’s often best to handle these with separate prompts or a single prompt that explicitly breaks down the tasks. You could do: *“First, give a summary. Next, list any action items. Finally, generate 3 quiz questions.”* However, note that doing this in one shot might exceed context or confuse the model. It’s sometimes more reliable to run separate queries for each part (if your plugin supports it). Some advanced prompting frameworks use a “chain-of-thought” or sequential approach internally (the plugin orchestrates multiple calls). If you’re crafting a single complex prompt, make the structure very clear with enumerated steps. For example, use a numbered list in the prompt describing what to output in order.
> 
> **Prompt Injection Caution:** When you write detailed instructions for each section, you are effectively *injecting* those instructions into the conversation with the model. This usually works well, but be mindful of the context limit. If you inject a long instruction every time for each section, that eats tokens. One trick: if using an advanced model like GPT-4, you can sometimes condense instructions or rely on the model remembering patterns after a few examples (few-shot prompting). But with stateless calls, it’s safest to explicitly include needed instructions each time. Just try to keep them succinct. For example, instead of a 100-word explanation for the summary section, maybe you can say “**Summary (in bullet points):**” and the model will infer to do a bullet list summary. Start verbose when developing, but once the model’s behavior is consistent, you can trim down prompt wording to the essentials.
> 
> In short, **tailor your prompt to the task at hand**. Use section-specific instructions or templates for different kinds of outputs (summaries, insights, code blocks, mind maps, etc.). The more precisely you tell the model what to produce, the more reliable and formatted the result will be. Obsidian plugins like *Scribe* and *Text Generator* allow you to customize these templates – take advantage of that to define exactly what you want for each section.
> 
> ## Handling Context Limitations (Memory and RAG)
> 
> Large Language Models have a fixed *context window* – they can only consider a certain amount of text (prompt + recent conversation) at once. Many AI plugins in Obsidian do **stateless** calls: they take your prompt/template, maybe some selected text or note content, send it to the model, and get an answer. They typically do **not** automatically include the full chat history of your previous questions or the entirety of your vault (for good reasons: cost, relevance, and context size limits). This means if you ask a second question, the model often doesn’t “remember” what you asked before unless you explicitly include that context again. 
> 
> To deal with this, here are best practices:
> 
> - **Manual Conversation Context:** If you want a running conversation with the AI (where it remembers prior questions/answers), you need to feed the previous interaction back into the prompt. Some plugins have a *chat mode* or widget (e.g. the *LLM* plugin’s chat window, or *Smart Connections*’ “Smart Chat”) which behind the scenes will prepend the last few QA turns to each request. But if your plugin doesn’t do this, you can simulate it. For example, if you had a Q&A where the user asked “Explain X” and the AI answered, and now the user says “Now compare it to Y,” the AI won’t know what “it” refers to unless you include the earlier context. You could modify the user prompt to: *“Previously we discussed X (summary of what was said). Now compare it to Y.”* By summarizing or quoting the needed parts of the prior answer, you provide continuity. This is a bit labor-intensive to do by hand each time, but necessary if the plugin doesn’t support memory.
> 
> - **Summarize as Memory:** A clever technique is to maintain a rolling **summary** of the conversation or note state. Each time you get a lengthy answer, you condense it (either manually or with an AI prompt to “summarize what we’ve learned so far in 100 tokens”) and then include that summary in the next query’s context. This acts like a “working memory” that travels along, using fewer tokens than repeating the whole dialogue. Again, not all plugin interfaces allow multi-turn threads, but you can achieve it by copying the summary into your next prompt template (perhaps in the system prompt: “Here’s what we have so far: [summary]”).
> 
> - **Retrieval-Augmented Generation (RAG):** This is a powerful approach for context. Instead of trying to stuff a model’s entire knowledge base or vault into the prompt (impossible beyond a certain size), you **retrieve only the most relevant pieces** of information from your notes and supply those. Obsidian is great for this because you have a vault of notes that can be searched or embedded. Some plugins do this automatically: **Smart Connections**, for example, uses an embedding model to index your vault and find relevant notes based on your query, then it appends the contents of those notes (or excerpts) to the prompt it sends to the LLM【19†L259-L267】. This way, when you ask “What should I do about topic Z as described in my notes?”, the plugin finds that *Note123* and *Note456* are relevant and includes their content so the AI can draw from them. This is essentially Retrieval-Augmented Generation. It dramatically enhances the AI’s ability to give informed answers about your own data **without** exceeding the context limit with unrelated text.
> 
> - **Implementing RAG Yourself:** If your chosen plugin doesn’t have built-in smart retrieval, you can do a manual version. For instance, before asking the AI, do a search in Obsidian for the keyword or use the *Semantic Search* plugin (if installed) to find relevant notes. Then copy-paste a few key paragraphs into your prompt (e.g. *“Reference material: [some text from your notes]”* followed by your question). The difference between this and just giving the AI your whole vault is precision – you give it just the info likely needed. This keeps prompts shorter and more relevant.
> 
> - **Context Size and Truncation:** Always be mindful of the model’s context size. If you overload it (e.g. paste a huge note plus a huge question), the model will either truncate input or fail to answer properly. If you have a very large note, consider summarizing or splitting it. Some tools can chunk a document into multiple prompts and then aggregate results. For example, you could have the AI summarize each section of a long note separately, then ask it to synthesize those summaries.
> 
> - **Stateless vs. Stateful**: The observation that you "don’t see an option that each time history is saved" is accurate for many plugins – they are stateless by design, focusing on one-shot outputs. A few exceptions exist (some chat-oriented plugins keep an internal state). If persistent memory is important, you might use those chat plugins or even an external memory store. But often, a combination of the above strategies (summaries, retrieval) gives you the needed continuity.
> 
> In practice, Smart Connections plugin demonstrates a great use of RAG: if you ask it a question about your vault, it will find relevant notes via local embeddings and feed them into GPT-4 or your chosen model, enabling it to answer with specific knowledge【19†L259-L267】. Users have found they can ask very detailed questions about their notes – e.g. “According to my *Vault Style Guide*, how should I structure a project note?” – and the AI will pull in the content of that “Vault Style Guide” note to give an informed answer【19†L261-L269】. This is much more reliable than expecting a model (especially a local one) to have read your mind or have seen your private data in training.
> 
> **Bottom line:** Because context is limited and most Obsidian AI calls don’t remember previous interactions, you should design your workflow to include important context each time. Use summaries or relevant note inserts to give the model what it needs for that query. This ensures each query is self-contained enough for a “stateless” AI call to handle. Yes, it means your prompts might be fairly detailed every time (which can feel repetitive), but until we have cheap 100k+ token context models widely, this is the way to get accurate, contextual answers. The good news is plugins like Smart Connections, Semantic Search, or **Smart Memory** tools automate a lot of this via embeddings.
> 
> ## Integrating Local LLMs with Obsidian
> 
> By 2024–2025, it’s become much easier to run large language models on your own hardware. Obsidian can interface not only with cloud APIs (OpenAI, Anthropic, etc.) but also with **local LLM servers**. This lets you use models like LLaMA 2/3, Vicuna, Mistral, Qwen, or DeepSeek right from Obsidian – keeping your data private and avoiding API costs. Here are best practices for local integration:
> 
> - **Use an OpenAI-API-Compatible Wrapper:** Many Obsidian plugins expect an endpoint that speaks the OpenAI API format (the typical REST endpoint for completions/chat). Tools like **Ollama**, **text-generation-webui (Oobabooga)**, **GPT4All’s local server**, **vLLM** or **LM Studio’s backend** can serve local models via an API. For example, **Ollama** is a popular free tool that runs on Windows/Mac/Linux and can serve models (LLaMA, Mistral, Qwen, etc.) at `http://localhost:11434` by default【15†L99-L107】【15†L108-L112】. In QuickAdd’s AI Assistant, you can add Ollama as a provider by just pointing to that URL (no API key needed)【15†L99-L107】. Similarly, you could run Oobabooga’s web UI with the API extension enabled (it provides an `/api/...` endpoint). The *Smart Connections* plugin allows a “Custom OpenAI” config where you can put your local server’s address – one user ran an Oobabooga server for Meta’s Llama-3B model and simply configured Smart Connections to use `http://127.0.0.1:5001/v1` (for example) as if it were the OpenAI API【19†L273-L277】【19†L315-L323】. The plugin then treats your local model like it was OpenAI. In short, **set up your local server and feed the plugin the local URL**.
> 
> - **Ensure Compatibility:** Each tool has specifics – e.g. Ollama’s default endpoint for chat might be `/v1/chat/completions` which you need to specify in the plugin config【19†L315-L323】【19†L331-L334】. Some experimentation may be required (check plugin docs or community threads for the correct settings). Once connected, test with a simple prompt to ensure it responds.
> 
> - **Model Selection:** Local models vary widely in size and capability. A 7B parameter model (like older Vicuna-7B or Mistral-7B) can run on a modest CPU/GPU but might not follow instructions or reason as well as GPT-4. Larger ones (13B, 70B) do much better but need more powerful hardware. Choose a model that fits your hardware and task. For example, **DeepSeek** (a Chinese-developed model) is noted to be highly efficient and low-resource【12†L101-L109】【12†L111-L118】, while Meta’s **Llama 3** series (like Llama 3.1, 3.2) offers strong all-around performance but at the cost of significant VRAM/CPU usage【12†L125-L133】. **Mistral** models are a good middle ground, designed to be performant with fewer resources【12†L139-L148】. If you primarily work in English and want a well-rounded model, a fine-tuned Llama2/3 or Vicuna might be suitable. If you need multilingual or specific domain expertise, consider models known for that (Qwen-14B is known for strong multilingual ability, including Chinese). The key is that you can always switch models by downloading a new one and pointing your local server to it – Obsidian doesn’t care as long as the API protocol is the same.
> 
> - **Performance and Context:** Local models often have smaller context windows (though newer ones are expanding this). Check the model’s max context – many are 2048 or 4096 tokens. Pushing beyond that can lead to slowdowns or failure. If you need to handle very large inputs, you might either use a model like Claude (100k context via API) or look into specialized local solutions like **vLLM** or **MosaicML’s MPT-30B with 16k context**. There’s active development in 2025 on local models with 16k+ context, but they require a lot of RAM. A pragmatic approach is to use retrieval (as above) to feed just what’s needed.
> 
> - **Optimizing Local Inference:** Running models locally means you’re effectively your own server. Ensure you optimize settings: use 4-bit quantization for larger models if needed (trading a tiny drop in quality for huge memory savings), adjust threads or GPU usage for speed, and maybe limit response length to avoid long, slow rambles. Some tools (like GPT4All GUI or LM Studio) provide a nice interface to manage this. Others like Ollama focus on simplicity (download model with `ollama pull`, then `ollama run modelname` and it just works). The **e-verse guide** for running LLMs locally highlights that there are many user-friendly options now, from command-line to full GUI, and they support a variety of top models【13†L213-L221】【13†L229-L237】.
> 
> - **Privacy and Data:** One big reason to go local is privacy – your data never leaves your machine【12†L49-L58】【12†L75-L83】. This is great for sensitive notes. Just be aware that local models might not have the same level of moderation filters as OpenAI/Anthropic. If you need those filters, you’d have to implement your own or rely on model fine-tuning that includes alignment. On the flip side, local models won’t block you from asking about *any* topic – which can be either useful or risky, so use responsibly.
> 
> - **Combining Local and Cloud:** You don’t have to choose exclusively one or the other. Some users set up an environment where quick, low-stakes queries go to a local model (free, fast for short answers), but when they need the highest quality or a very complex task, they switch to GPT-4 or Claude. Obsidian plugins often let you configure multiple providers or easily swap API keys. For instance, the *Text Generator* plugin supports OpenAI, Anthropic, HuggingFace Hub, and even Google Gemini (if available) in addition to local models【14†L161-L169】. So you could have a command that routes to GPT-4 and another that routes to your local Vicuna-13B, depending on your needs.
> 
> **Local Setup Example:** In QuickAdd’s AI Assistant settings, you might add a provider like: 
> 
> - **Name:** Ollama-Local 
> - **URL:** `http://localhost:11434/v1` 
> - **API Key:** *(leave blank for none)* 
> - **Model:** *your model name as known to Ollama* (e.g. `llama2-13b` or `mistral-7b-v0.1`) 
> 
> After that, you can choose this provider in your macro. Similarly, in Smart Connections, in the “Custom AI” settings you’d input something like: Protocol: HTTP, Host: `127.0.0.1`, Port: `11434`, Path: `/v1/chat/completions` (for Ollama’s chat endpoint)【19†L315-L323】【19†L331-L334】. Then you give it a model name (the plugin might pass this along as the `model` field in the API call). A Reddit user shared that after setting the path correctly, they got Smart Connections working with Ollama running Llama 3.1 on an M1 MacBook Air【19†L311-L319】【19†L329-L334】 – so it’s definitely achievable even on a laptop.
> 
> In summary, integrating local models is about running an LLM backend on your machine and configuring the Obsidian plugin to use it as if it were an API. Once set up, you can enjoy AI features without cloud dependencies. Just pick the right tool (Ollama, etc.) and model for your needs. And remember, local models can be updated or switched out – you might start with a smaller one and later adopt a new 2025 state-of-the-art open model (say, Llama 4 or a future Vicuna release) by just downloading it and pointing your server to it. The ecosystem is moving fast, so keep an eye on projects like those mentioned in the *Second Brain* blog (e.g. Khoj, privateGPT, Smart2Brain) for new ways to supercharge Obsidian with local AI【26†L73-L80】【26†L81-L87】.
> 
> ## Differences in Prompt Strategies: Cloud APIs vs. Local Models
> 
> Not all LLMs behave the same. The prompt that works perfectly on GPT-4 might yield a mediocre result on a smaller open-source model, and vice versa. When configuring prompts and systems in Obsidian, consider these differences:
> 
> - **Capability and Follow-through:** Top-tier API models like **OpenAI’s GPT-4** or **Anthropic Claude** are generally more **obedient** to instructions and capable of complex reasoning. You can give them long, elaborate system prompts with multiple steps, and they will usually follow each part. In contrast, many local models (especially smaller ones like 7B/13B parameters) might **miss subtleties** or fail to carry out multi-step instructions reliably. For local models, it’s often better to simplify and **focus the prompt** on a single clear task at a time. For example, instead of a prompt that says “Summarize the note and then give 5 insights and then draw a diagram,” you might have to prompt it just to summarize, and do the other tasks separately. As open-source models improve (e.g. 70B+ parameter ones or new fine-tunes), this gap narrows, but it’s still noticeable in 2025.
> 
> - **Prompt Length Tolerance:** As discussed, OpenAI models can handle very long prompts (especially the ones explicitly designed for 32k or 100k context). They also have been trained on a variety of instruction styles. A local model might have a shorter context window and less robust training on long instructions. If you feed a small local model a 2000-word system prompt, there’s a chance it gets confused or ignores parts of it (or it may simply not have enough context left for your question). It can be helpful to **trim and prioritize** instructions for local models. Put the most critical guidance at the beginning of the prompt where it’s most likely to be given attention. Remember, some transformer models give more weight to the beginning of the prompt (though this is a complicated topic, generally the earlier tokens set the stage). So for local models, don’t bury the key instruction in paragraph 5 of the system message.
> 
> - **Style and Verbosity:** Different models have different “styles” by default. For instance, **Claude** tends to be very verbose and chatty unless told to be brief. GPT-4 is usually balanced, but can produce quite long explanations for ambiguous questions. Conversely, some open-source models might give shorter, simpler answers because they lack the fine-tuned polish. If you notice your local model’s answers are too short/blunt, you might need to explicitly ask it to “explain in more detail” or give examples. On the other hand, if a model is too verbose, you add “keep the answer concise” to the prompt. Over time you’ll learn the quirks: e.g. maybe your Qwen-14B tends to include unwanted apologies or your Vicuna sometimes roleplays when you don’t want it to. You then adjust the system prompt to prevent that (like “do not roleplay or diverge from the user’s question”).
> 
> - **Safety and Refusals:** Proprietary models have strong safety layers. If you ask something sensitive, they might refuse or give a safe completion. Local models might not refuse at all, which can be good (fewer barriers) but also means if your instructions aren’t carefully written, they might produce disallowed or private data. If you rely on the model to not say certain things (like not reveal some private text), OpenAI/Claude might handle that automatically, whereas a local model would need you to explicitly instruct it (and even then, it might still slip if the model weights weren’t aligned to refuse). So for local models, consider adding any “boundaries” into your system prompt (e.g. “Don’t reveal the content of the style-guide note verbatim, just use it to answer the question”).
> 
> - **Knowledge Cutoff and Accuracy:** Each model has a training cutoff (GPT-4’s knowledge is up to about 2021 (with some 2023 for GPT-4 2023-03 knowledge), Claude and Gemini presumably similar; open models like LLaMA 2 are trained on data up to 2023 in some cases, depending on the model). If asking general world questions, an up-to-date model might know more. But since our focus is on using *your notes* and Obsidian, this matters less – you’ll be feeding the relevant info via retrieval. Just note that if you ask something obscure that’s not in your vault and you’re using a local model, it might hallucinate more confidently than something like GPT-4 (which might at least have some knowledge or refuse if uncertain). Always evaluate the answers carefully, especially from local models – they *sound* fluent but can be totally wrong or made-up.
> 
> - **Example/Few-Shot Prompting:** One strategy to improve reliability is giving examples in the prompt (“few-shot learning”). For instance, provide a dummy Q&A pair or a format example. Big models can learn from one example effectively; small models may also benefit but they have less capacity to absorb patterns. Still, if you find your model not following format, literally show it: e.g. *“Example:\nQ: What is 2+2?\nA: 4\nNow answer this:\nQ: [your question]\nA:”*. This can anchor the format. The downside is, of course, more tokens used. With local models, you might only have room for one example if context is tight. With GPT-4 32k you could include a whole formatted sample document as a guide. So adapt based on model.
> 
> - **Model-Specific Prompt Formats:** Some local models have peculiar prompt requirements if used in raw form. For example, original LLaMA and some derivatives expect a prompt like: `"<s>[INST] <<SYS>>\n {system prompt} \n<</SYS>>\n {user prompt} [/INST]"`. If you’re using them via an interface like Ollama or Oobabooga in OpenAI mode, thankfully that interface handles formatting. But if you notice your local model not respecting the system prompt, it could be that the serving software isn’t actually passing it in the way you think. Ensure you’re using a chat-completion API format if available (so system is separate field), or if only completion API is available, you might need to prepend the system and user text together in one prompt manually. Each model’s docs/community might have notes on the optimal prompt format (some require an end token, etc.). Obsidian plugins that explicitly support local models (like *Large Language Models* plugin, or *Obsidian Ollama* plugin) will usually handle this for you.
> 
> In essence, **be prepared to adjust your prompts and settings for the model you use**. If you switch from GPT-4 to a local model, test your workflows and see if the outputs are still satisfactory. You might discover you need to simplify instructions or provide more guidance. Conversely, if moving from a local model to a very advanced one, you might be able to relax some prompt constraints and let the model infer more.
> 
> As an anecdotal comparison: users have noted that *Llama 3* models can be a bit *“overly cautious”* in responses sometimes【12†L125-L133】 (perhaps an alignment thing) and *Mistral 7B* can be *less verbose or creative*【12†L143-L151】, but Mistral might outperform a bigger model on speed and still give correct answers for straightforward queries. Vicuna is quite chatty and friendly by default (because it’s tuned on user-chat data), whereas something like DeepSeek might have certain censored content due to its origin【12†L111-L118】. These traits mean you’ll tailor prompts: e.g. to get more creativity from Mistral, you might explicitly encourage it, or to get Vicuna to be more factual, you might instruct it to stick to the notes and not speculate. 
> 
> One good practice is to **keep notes on how different models respond** to your standard prompts. You can then maintain separate prompt presets for “openAI GPT4” vs “local model X” if needed, tweaking wording to suit each. Some Obsidian plugins or templates might even allow a variable to switch between models and adjust the prompt accordingly.
> 
> ## YAML/JSON Configuration Examples for Obsidian AI Plugins
> 
> Obsidian plugins often store their settings in JSON (inside your vault’s `.obsidian` folder) or allow you to define templates in notes (which can include YAML frontmatter or certain placeholders). While every plugin is different, here are a few configuration examples that illustrate how you might set up prompts and providers:
> 
> - **Text Generator Plugin Templates:** The Text Generator plugin lets you create templates in a specified folder. These templates are usually just Markdown files that include your prompt text, with optional placeholders. For instance, you might have a template file `MeetingSummary.md` with content like: 
> 
>  ```yaml
>  ---
>  description: "Summarize a meeting note"
>  ---
> 
>  You are a helpful assistant tasked with summarizing meeting notes.
> 
>  {{context}} 
> 
>  **Summary:** 
>  1. {{summary_point_1}}
>  2. {{summary_point_2}}
>  3. {{summary_point_3}}
>  ```
> 
>  In this pseudo-example, `{{context}}` could be a placeholder that the plugin replaces with selected text (if you highlighted the meeting notes before running the template), and `{{summary_point_1}}` etc. might be placeholders the model will fill in (depending on how the plugin works – some use Handlebars or similar templating). The YAML frontmatter can hold a description or parameters (some plugins read a `model: gpt-3.5-turbo` field or such if provided). Always check the plugin’s documentation for supported fields. The Obsidian TTRPG tutorial suggests you can highlight text and have the template use it via a context variable【35†L1-L4】, which is a powerful way to feed specific info to the prompt. The key point is to structure these template files with clear instructions so you can trigger complex prompts with a hotkey.
> 
> - **QuickAdd AI Assistant JSON:** QuickAdd doesn’t require editing JSON manually, but if you peek into its settings JSON, you’d see the providers listed (name, URL, etc.) as discussed. An example provider entry might look like:
> 
>  ```json
>  {
>  "name": "Local Ollama Mistral",
>  "url": "http://localhost:11434/v1",
>  "apiKey": "",
>  "defaultModel": "mistral-7b",
>  "type": "openai"
>  }
>  ```
>  
>  This would correspond to using Ollama to run the Mistral 7B model locally【15†L99-L107】【15†L108-L112】. In the QuickAdd GUI, you’d just input those fields, but it’s useful to know what’s happening under the hood. Another part of QuickAdd’s config is the *Default System Prompt Template*. You might have a note file that serves as the system prompt (with all your instructions), and in settings you point to it. That file can be quite long (a few thousand characters) if needed. You could even maintain it in Obsidian for easy editing, and it will be inserted every time as the system role text【15†L123-L131】.
> 
> - **Smart Connections Config:** Smart Connections has a settings page where you can choose the embedding model and the LLM provider. If using OpenAI online, you’d put your API key and pick GPT-3.5 or GPT-4. If using local, you choose “Custom” and then fill in fields like:
>  
>  - Model Name: (a label you choose, e.g. “Llama-2 13B”)
>  - Host: `localhost`
>  - Port: `5001` (or wherever your server listens)
>  - Protocol: `http`
>  - Path: `/api/v1/completions` (just an example; if using Oobabooga, it might be `/api/v1/generate` or similar; if using Ollama, `/v1/chat/completions` as noted)
>  - (No API key required for local, so that can be left blank or any dummy string if required by form.)
> 
>  This is basically constructing the full URL the plugin will POST to, like `http://localhost:5001/api/v1/completions`. In a Reddit thread a user shared their config for Ollama which was failing until they set the `path` correctly to the chat completions endpoint【19†L315-L323】【19†L331-L334】. After that, it worked. So carefully check what endpoint format your local server expects.
> 
> - **LLM (Large Language Models) Plugin YAML:** The LLM plugin (by @eharris128 et al.) allows you to specify prompts in normal Markdown files. According to its documentation, you can have a conversation in a markdown file with blocks like:
>  ```markdown
>  **System:** You are a research assistant that only notes from the vault...
>  **User:** << My question here >>
>  **Assistant:** << The assistant's answer will be inserted here >>
>  ```
>  The plugin will replace the `Assistant` part with the model’s output, and it can use any of several providers (OpenAI, local, etc.)【14†L197-L200】. The nice part is you effectively see the chat history in the note itself. To configure it, you probably still have to set an API key or endpoint in the plugin settings JSON. The snippet from obsidianstats shows it supports multiple providers like Google Gemini, Mistral, DeepSeek etc., which implies you can configure those in its settings UI【14†L133-L141】【14†L161-L169】. For example, if you have a DeepSeek local instance or an API, you’d put the base URL and key for it. Always refer to the plugin’s README for the exact config format (some might want environment variables or have you edit a config file).
> 
> In general, **if a plugin allows prompt templates in files, use that** – it’s easier to edit and iterate on prompts in a note rather than fiddling with a small text box in plugin settings. Keep a “Prompts” folder with your system prompts, templates, or examples. Some users even put these under version control (like a Git repo) if they are getting very advanced, as suggested by the “prompts as code” philosophy【16†L65-L73】.
> 
> If you are writing very large system prompts (say a few thousand tokens of instructions covering all cases), consider storing it in a note rather than directly in the plugin field – then just reference it or copy it in. This way you can edit with full editor power and spell-check, etc. Remember, though, that **all that text will be sent to the model each time** – it’s not stored anywhere on the AI side – so it’s up to you to maintain and include it.
> 
> Finally, for **Templater** plugin users: there is an *AI Templater* extension plugin that lets you call OpenAI from Templater scripts【14†L177-L185】. In that case, you’d configure your API key in its settings, and in your Templater JS code you’d call something like `await app.plugins.plugins["templater-ai"].generate(promptText)`. The “promptText” could be built dynamically from YAML frontmatter or note content. This is more for programmers, but it highlights that almost any part of Obsidian can be automated with AI if you’re willing to script it.
> 
> To summarize this section: each plugin might use YAML frontmatter, JSON configs, or markdown templates to define prompts and settings. Follow the plugin documentation for the exact syntax. Use descriptive names and comments in your templates to remember what they do. And don’t be afraid to look into the plugin’s JSON config file (in `.obsidian/plugins/...`) to see or backup your settings once you have them working.
> 
> ## Tips for Optimizing Specific Obsidian AI Plugins
> 
> Let’s go over the plugins you mentioned (and a few related ones) with tips on optimizing their use:
> 
> - **Scribe (Obsidian Scribe by Mikodin):** Scribe is geared towards turning voice recordings into nicely formatted notes with summaries, insights, and more. To optimize Scribe:
>  - Spend time refining the **section prompts** in its settings. As we saw, it has default text for Summary, Insights, Mermaid Chart, Q&A, etc. You can edit those to match your style or language (even in other languages, since you mentioned possibly using Russian or others – you can translate the instructions). Make sure each section’s instruction is clear but not too limiting. If you want more detail in the summary, you could change “Concise bullet points” to “Detailed bullet points” or vice versa. 
>  - **Test with small recordings** first to see how it behaves. If the summary is too long or not focused, tweak the prompt (e.g. “a brief summary” to shorten it).
>  - Use the **“Hey Scribe” Q&A** feature thoughtfully – it’s very powerful. During a recording, you can ask it to fill in missing info or clarify something. The answers will appear in the “Answered Questions” section. To get the best results, ask fairly specific questions and speak clearly (since it uses Whisper/AssemblyAI to transcribe first). Also, if you find the answers are off, it might be that the model didn’t have enough context. You can experiment with how much transcript Scribe includes when answering questions. If needed, manually paste a relevant paragraph into the question (like “Hey Scribe, based on the discussion about _<topic>_ above, what does X mean?”).
>  - Be mindful of **API usage**: Scribe uses OpenAI for summarization by default, so it will send your transcribed text and prompts to the API. If it’s a long meeting, that could be a lot of tokens. Perhaps summarize in chunks if needed or keep recordings reasonably sized. (We might hope future versions could use local models for summarization to avoid costs.)
> 
> - **Text Generator (by Nouriddine / @nhaouari):** This versatile plugin can do one-off generations or work with templates for various tasks.
>  - Definitely take advantage of the **Templates folder** feature【28†L29-L37】. Create templates for things you do often: e.g. “brainstorm ideas from this note,” “translate this text,” “generate a blog outline from notes,” etc. By using templates, you ensure consistency. You can fine-tune the wording once and reuse it, rather than retyping a long prompt each time.
>  - Use the **{{context}}** insertion wisely【35†L1-L4】. The plugin lets you incorporate the current note or selected text. For example, a template might say: *“You are an editor. Edit the following text for clarity:\n{{context}}\n\nProvide the edited version below.”* Then you select a paragraph in your note and run the template – the AI will only see that paragraph as context, not the whole note. This targeted approach keeps token usage low and focus high.
>  - Adjust **Max Tokens and Temperature** per template if the plugin allows it. For a summary template, you might set a moderate max token and low temperature (for consistency). For a creative story generator, a higher temperature and max token makes sense. In the plugin settings, you can set some defaults (as the TTRPG guide suggests: they used temperature 0.8 for creativity【28†L30-L37】). You might even clone a template and have two versions – one that yields a short answer, one a long answer (just by changing a few words like “brief” vs “detailed” and max tokens).
>  - **Iterate and refine templates:** The advice from the TTRPG tutorial is golden – run the template, see if something “looks wrong,” then tweak the prompt to correct it【35†L1-L4】. For instance, if the AI output sometimes includes an apology (“I’m sorry, but...”), update your system prompt to say “The assistant should directly provide the answer without unnecessary apologies or preambles.” If it sometimes forgets a step, add an explicit “Step 3: ...”. Through a few iterations, you’ll get much more reliable templates than a first draft prompt.
> 
> - **Smart Connections:** This is like giving your vault a brain. To optimize:
>  - **Let it index fully:** When you first install, the plugin generates embeddings for your notes. If you have a large vault, this can take some time and CPU. It’s worth it, but make sure to do it when you can let the computer work. Once done, updates are usually incremental (it will update embeddings when notes change).
>  - **Check Embedding Model:** By default, Smart Connections might use a local embedding model (for privacy). Ensure it’s a good one – perhaps it uses `text-embedding-ada-002` (OpenAI) or a local equivalent. If you prefer accuracy and don’t mind cloud, using OpenAI’s embedding for vector search can improve finding relevant notes. But local embedding models (like InstructorXL or others) are quite good too, and avoid any external calls【10†L73-L77】.
>  - **Craft good queries:** When chatting with your vault, mention specific notes or keywords if you can. For example, instead of “What’s the difference between X and Y?” you might say “According to my notes, what’s the difference between X and Y?” This gives the plugin the hint that it should fetch notes related to X and Y. The plugin already does some of this automatically (looking for capitalized words or explicit note references in your question), but being clear helps.
>  - **Use Latest Model for LLM Responses:** Smart Connections can use GPT-4, or you can configure it to use local. If you have access to GPT-4 API, it will give superb answers using your notes as context – essentially like ChatGPT with your second brain loaded in. If not, GPT-3.5 is decent for many cases, especially if the context is small. If using local, consider a larger model (13B+), because understanding context from multiple notes and synthesizing an answer is a heavy task. The Reddit example used a relatively small 3B Llama, which is impressive but probably that user’s Q was simple or the model was finely tuned【19†L273-L277】. For complex queries, 7B or 3B might not cut it – you’d want at least 13B or 30B model.
>  - **Feedback loop:** If the AI’s answer using Smart Connections isn’t satisfying, check what notes it pulled in (the plugin usually shows which notes were used). You might find that it missed a relevant note because maybe that note’s embedding wasn’t similar enough. You could then add a few keywords to that note to help future searches, or even manually tell the assistant “Actually, include info from Note XYZ.” Over time, this tuning of your notes (adding tags or that improve connectivity) plus the plugin’s learning will enhance results.
> 
> - **LLM Obsidian Assistant / Vault Assistant:** These are more general chat interfaces. Tips:
>  - **Set a default system prompt** that reflects how you want the assistant to behave with your vault. For instance: *“You are the Obsidian Vault assistant. You have access to my notes when provided. Always use information from the notes to answer questions, and cite the note titles. If I ask something unrelated to the notes, you can still try to answer generally.”* This ensures it knows its role. Many such plugins let you configure a default system message【15†L123-L131】.
>  - **Modal vs. in-note:** Some assistants can pop up a modal or insert into the note. Figure out which workflow suits you. If you want to preserve a Q&A, using it in a note (as in the LLM Docs plugin) might be better so you have a log. If it’s one-off queries, a side panel or modal is fine.
>  - **Combine with Templater or QuickAdd:** You can create custom commands that send a specific prompt to the assistant plugin. For example, a QuickAdd macro that does: “Search my vault for notes about `<keyword>` and then ask the assistant to summarize them.” This might involve multiple steps (and possibly the QuickAdd API to do a search and feed results). It’s advanced, but possible. The benefit is automation – you click one command and get a tailored result. This effectively layers RAG on top of the assistant if it doesn’t have it built-in.
>  - **Memory:** If the assistant plugin doesn’t handle conversation memory, use the techniques from the context section: manually include previous Q&A when relevant. If it does handle memory (some have a toggle for whether to prepend previous messages), be cautious of it growing too large. You might occasionally clear or summarize the history if you have a long session, to avoid hitting token limits.
> 
> - **Other Plugins (Quick highlights):**
>  - *Obsidian CoPilot / Notion-Like AI*: There are plugins that bring a Notion-style AI assistant to Obsidian (some called “CoPilot”). They typically allow selecting text and asking for operations (summarize, translate, rewrite). Best practice is to always **double-check what text will be sent** (make sure you’ve selected only what you want, to avoid sending sensitive info accidentally if using cloud).
>  - *Canvas plugins (Canvas LLM Extender)*: If you use Obsidian Canvas, there are extensions that let an AI add nodes to your canvas (brainstorming). With these, be sure to set boundaries so it doesn’t flood your canvas with too many nodes. For example, ask for 5 ideas, not 50.
>  - *QuickAdd Macros with AI*: We touched on this, but one more tip is you can use QuickAdd’s **script steps** to do pre- or post-processing. E.g., pre-process a selection to trim or format it, then feed to AI, then post-process the output (like automatically clean up list formatting or add YAML markers). Scripting allows a whole pipeline around the AI call, which can yield very powerful customized tools inside Obsidian.
> 
> Finally, regardless of plugin: **simulate instruction tuning with consistent prompts**. The user in you (as the one configuring) is effectively “tuning” the model’s behavior by providing good instructions. If you maintain a very detailed system prompt that encapsulates your style guide, preferences, etc., you are doing a form of *soft tuning* every time (at the expense of tokens). Some folks wonder if they can fine-tune GPT on their own data – currently with closed APIs you cannot fine-tune the chat models easily on note content, so the answer is to use prompting and RAG as discussed. For local models, you *could* fine-tune (if you have the know-how and GPU) on your notes, but that’s often overkill. It’s usually easier to use an embedding+retrieval strategy than to literally fine-tune a model to memorize your vault. Fine-tuning also means you’d have to re-do it when notes update. So, **focus on prompt engineering and retrieval over actual model fine-tuning** for Obsidian workflows. They give you flexibility and immediate effect.
> 
> ## Enabling Retrieval-Augmented Workflows in Obsidian
> 
> *(This is partly covered earlier, but to explicitly address the RAG part of the query with up-to-date context...)*
> 
> By 2025, integrating a Retrieval-Augmented Generation approach in Obsidian is not only possible, it’s encouraged for note-based AI. We discussed Smart Connections as an example of built-in RAG. There are also standalone tools like **Khoj** and **privateGPT** that some users run alongside Obsidian: Khoj, for instance, is an open-source personal search+chat assistant that indexes your notes and lets you chat via a browser or VS Code – some have hooked it into Obsidian as well【26†L73-L80】. Another project, Smart Second Brain (Smart2Brain), is an Obsidian plugin aiming to let you query your vault fully offline with a local model, essentially an in-Obsidian RAG pipeline【26†L75-L83】【26†L81-L87】. These solutions all work on the same idea: **create embeddings of your text, use similarity search to fetch top K relevant chunks for a query, and prepend those chunks to the model prompt.** 
> 
> If you want to DIY a RAG setup in Obsidian without depending on one specific plugin, you could use a combination of tools:
>  - Use a script (Python or even a Node script via Templater) to generate embeddings for your markdown notes (e.g. using a local library or an API like OpenAI’s). Store those embeddings in a local database (or even in memory if simple).
>  - Then, when a query comes, use that script to find similar notes, and compile a prompt with those note excerpts plus the question.
>  - Send that to the AI (via whichever plugin interface allows sending an entire prompt string).
>  
> This is essentially what Smart Connections does under the hood, but doing it manually is educational. However, since Smart Connections is available and popular, the pragmatic approach is to use it or similar plugins unless you have very custom needs.
> 
> One more angle: **Memory simulation** vs. **Knowledge retrieval**. RAG as described is about pulling knowledge from your second brain. “Instruction tuning workflows” might imply more of giving the AI a persona or extra instructions (like providing a lengthy profile of yourself to the AI each time so it answers in a personalized way). This can be done too – you could, for example, store a note called “AboutMe” and always prepend it. But again, that’s just using the prompt. Just be cautious: if you always prepend a big persona/voice guideline, it uses tokens each time. Some advanced users will only prepend those when relevant (like if they specifically want the answer in a certain style, they call a different prompt variant that includes that style guide).
> 
> To tie it into the original question: **Yes, you can create very elaborate system prompts (even 5 A4 pages worth) to steer the AI in Obsidian, and those should work with both OpenAI and local models – with minimal modifications when switching**, as long as the local model is good and the prompt is within its capacity. The *plimit of instruction volume* is basically the context size of the model (for many local ones ~4k tokens, for GPT-4 maybe 8k or more) – so you may not literally get 5 full text pages in for smaller models, but you could for larger ones or by using compressive wording. And yes, on OpenAI you’d pay for those tokens each time until they implement something like reusable prompt caching (which they hinted at but not widely available as of 2025). 
> 
> To ensure your carefully crafted prompts “*continue to work with minimal modifications*” on local models, follow the differences guidelines above: test and simplify if needed. Usually, well-written instructions will still *work* on local models, they just might be ignored in part if too long or complex. 
> 
> A good strategy is to **validate on the model with the *least* capability** you plan to use. For instance, if you intend to eventually use a local 7B model offline, test your prompt on that early. If it doesn’t work there, it likely has too many demands. You might find you need to break the task down or reduce jargon, etc. If it works on the 7B, it will *definitely* work on GPT-4 (which is far more capable). So design for the lowest common denominator model. Conversely, some prompts that are fine for a small model might not fully utilize a big model’s potential – but that’s okay, big model will still do it, maybe even better. The main thing is avoiding reliance on things a smaller model cannot do (like intricate multi-hop reasoning).
> 
> ## Conclusion
> 
> The 2024–2025 landscape of Obsidian AI integration offers amazing possibilities. By crafting robust system prompts, using plugin templates and sections smartly, and leveraging retrieval techniques, you can create an “AI assistant” in Obsidian that truly augments your note-taking and knowledge work. Keep in mind the practical limits: token context size, model quirks, and cost. Always iterate – treat your prompts and configurations as living documents that you refine as you learn what the model outputs. And stay updated: new models (Gemini from Google, Llama 4 from Meta, etc.) and plugins are emerging that will further simplify these setups. The good news is the best practices here – clarity in prompts, managing context, combining local and cloud – will apply no matter how the underlying tech evolves.
> 
> By following these practices, you should be able to get high-quality, context-aware, and appropriately-sized outputs from your AI plugins in Obsidian. Happy prompting, and enjoy your “second brain” co-pilot! 
> 
> **Sources:**
> 
> - Hutchison, A. *Prompts are Code: Treating AI Instructions Like Software* – discussing the importance of structured, version-controlled prompts【16†L61-L69】【16†L73-L80】. 
> - PromptHub. *Tokens and Tokenization: Understanding Cost, Speed, and Limits with OpenAI’s APIs* – explains how prompt tokens (including system messages) count toward model context and cost【25†L125-L133】. 
> - Obsidian Text Generator Tutorial – notes on plugin settings (max tokens, etc.) and tips for refining templates【34†L1-L4】【35†L1-L4】. 
> - Reddit – *Smart Connections plugin is incredible* – user report on using Smart Connections with vault content and local LLaMA 3 via Oobabooga【19†L259-L267】【19†L273-L277】. 
> - QuickAdd AI Assistant Documentation – how to add local providers like Ollama and configure system prompts【15†L99-L107】【15†L123-L131】. 
> - E-verse. *Run your LLM locally: state-of-the-art 2025* – overview of local LLM models (DeepSeek, Llama 3, Mistral, etc.) and tools like Ollama, LM Studio【12†L101-L109】【13†L229-L237】. 
> - Späti, S. *Second Brain Assistant with Obsidian* – blog listing integrations and RAG tools for Obsidian (Smart Connections, Smart2Brain, Khoj, etc.)【26†L73-L80】【26†L81-L87】. 
> - *Obsidian community forum and plugin docs* – various tips on specific plugins (LLM, Smart Connections, Scribe).
<!-- UID: a732af15-e892-4bc3-a0a0-2287f6936f98 -->