>[!nexus_user] **User** - 02.08.2025 at 2:10
> How close can large local LLMs (Qwen 30B–225B, LLaMA3 70B, DeepSeek 67B, Mistral, Mixtral) get to GPT-4 performance with detailed system prompts? 
> Benchmark studies, prompting experiments, system prompt engineering for local models, comparisons to OpenAI/Anthropic APIs. 
> Looking for:
> - Instruction-following capabilities of large local LLMs
> - Effects of detailed multi-page system prompts on response quality
> - Tradeoffs between model size, latency, and prompt complexity
> - Best practices for maximizing output quality from local models
> - Tests using long-form reasoning, summarization, coding, scientific Q&A
> - Examples using vLLM, Exllama, TensorRT-LLM, LMDeploy, Deepspeed, or GPTQ
> - Comparisons between GPT-4 and Qwen-72B/110B/225B in zero-shot vs. prompted performance
> - Prompt engineering techniques for offline models
> - Limitations of local LLMs in handling complex prompts without memory
> - Scaling laws for prompting vs. fine-tuning
<!-- UID: ed2c918c-1180-48f6-aac2-4c560c7fb893 -->




>[!nexus_agent] **Assistant** - 02.08.2025 at 2:10
> Хорошо. Чтобы точно подобрать материалы, подскажи, пожалуйста:
> 
> 1. Тебе интересны **исследования с открытым кодом и логами**, которые можно воспроизвести на своей машине? Или больше интересны **академические статьи и бенчмарки**?
> 2. Какой тип сравнения приоритетнее: **reasoning / code / summarization** или **Q&A / tool use / scientific domain**?
> 3. Планируешь запуск на **домашнем ПК**, **локальном сервере с GPU**, или **кластерной архитектуре**?
> 
> Это поможет мне уточнить приоритеты при поиске и отборе материалов.
<!-- UID: c64823ff-b018-4a25-a226-d15de117c538 -->