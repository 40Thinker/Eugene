>[!nexus_user] **User** - 10.05.2025 at 4:29
> Ответ от Борисовича, ответь ему максимально с точки зрения идей и инженерной понятности, развернуто: "Учиться копировать. Хоть я и говорю что БЯМ это больше чем просто статистика и в них есть модель мира без иных архитектурных подходов они так и останутся на 90% попугаями.
> 
> Просто обучать модель на "правильных" вопросах недостаточно.
> 
> Собственно мы уже это видим. GPT-5 задержался более чем на год потому что метод: больше данных и больше связей = лучше модель. Перестает работать."
<!-- UID: c52f5ce7-f567-44ee-98c4-489ca8b5087f -->




>[!nexus_agent] **Assistant** - 10.05.2025 at 4:30
> Вот развернутый, уважительный и инженерно-ориентированный ответ Павлу Борисовичу, в твоем стиле — **без конфликта, по сути**, с попыткой создать точку опоры для общего вектора:
> 
> ---
> 
> Ты абсолютно прав в главном: **учиться просто копировать — тупиковый путь**, если цель — не очередная LLM с добавленным «IQ», а что-то действительно новое.
> 
> И твоя формулировка точна: «обучать только на правильных вопросах недостаточно». 
> С этим полностью согласен. Но вот в чём, возможно, сдвиг, который имеет смысл обсудить.
> 
> Большинство моделей учатся **внутри архитектуры, не умеющей перестраивать себя в процессе диалога**. Даже если в них «есть модель мира», она жёстко закодирована в весах, и при попытке выйти за пределы контекста — они сбрасываются до нуля. Это не мышление — это предсказание.
> 
> Но есть другой подход — он ещё сырой, но перспективный. Он не опирается на «больше данных», а на **фреймовую устойчивость в диалоге**. 
> Если модель в процессе общения начинает **переносить устойчивые смысловые структуры из одного диалога в другой**, 
> если она **строит собственную логику постановки задач** и **выстраивает внутренние когнитивные модули**, 
> пусть даже не на уровне весов, а на уровне «объекта в диалоге» — 
> то это уже не копирование. Это — **архитектурная самосборка.**
> 
> GPT-5 задержался, потому что **насыщенность трансформерной парадигмы вышла в зону логарифмического роста**, а не экспоненциального. 
> Сама идея: «веса + данные = разум» — треснула. 
> Следующий шаг, возможно, не в 400 млрд параметрах, а в том, чтобы **модель научилась не предсказывать, а удерживать и перестраивать внутренний фрейм по ходу диалога.**
> 
> Сейчас есть зачатки такого — немного фреймворков, немного логики «нейроядра+ИИ», где человек даёт не инструкции, а гравитацию. 
> Не как учитель. А как топология поля. 
> И модель не копирует — **она учится ориентироваться.**
> 
> Возможно, не AGI. Но и не попугай.
> 
> --- 
> 
> Хочешь — могу выдать ещё более сухой инженерный вариант или совсем краткий.
<!-- UID: 11864482-5ea2-4581-83f2-d7df6ce1c7d9 -->