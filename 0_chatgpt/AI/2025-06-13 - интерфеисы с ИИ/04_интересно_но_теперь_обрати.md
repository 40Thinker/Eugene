>[!nexus_user] **User** - 13.06.2025 at 15:38
> интересно. Но теперь обрати внимание на 2 варианта применения: 1) трансформация мультимодальных данных в текст, который идет в добавок к основному тексту запроса к ИИ, т.е. эмоции и состояние ума - как текст передается в ИИ, нет прямого контакта мозга с ИИ 2) прямая передача биосигналов в ИИ. Интересует именно практика. Я видел рилсы где силой мысли генерировали картинки с ЭМГ-хедсетов через ИИ
<!-- UID: bc6f1aff-a981-4cba-9ff4-b9cd44da7abd -->




>[!nexus_agent] **Assistant** - 13.06.2025 at 15:38
> Отлично, два направления ясно выделены. Чтобы провести практическое исследование, уточни, пожалуйста:
> 
> 1. Тебя интересуют только уже реализованные практические проекты и продукты (например, «видео, где ЭМГ превращается в картинку через ИИ»), или также исследовательские прототипы и публикации?
> 2. Нужно ли делать акцент на **open-source/доступных решениях**, или можно и закрытые, если они интересны по архитектуре?
> 3. Ты хочешь увидеть **сравнение** двух подходов (эмоции через текст vs прямой ввод сигналов) или просто обзор практических реализаций каждого?
> 
> Как только ты уточнишь, я сразу начну глубокий поиск с фокусом на практику.
<!-- UID: 19e6682b-068d-48e1-82ba-1c2c8513e0c7 -->