>[!nexus_user] **User** - 13.06.2025 at 14:28
> 1 всего 2. все 3. все 4. все. ВОобще чем больше те лучше, копай глубоко, важны не вода (научпоп), а качественные ходы и решения (что уже есть такое-то, что разрабатывают такое и тд)
<!-- UID: af8cc05e-6348-4247-b9fd-fc050d61e029 -->


Let me analyze the given note as a system architect for software development, following the requested structure. I need to focus on the specific note provided as {activeNote}, which is about multimodal interfaces for interaction with AI, especially focusing on EMG (electromyography) and other biosignals.

First, I need to understand the core of the project context from the retrieved documents, especially the "OVERLAY AGI" project which seems to be the global framework this note is part of.

From the retrieved documents, I understand that the Overlay AGI project is about:
- A comprehensive approach to developing AI systems combining neural processing with symbolic reasoning and external knowledge management
- Addressing limitations of current AI: scalability, opacity, knowledge management, performance constraints
- Using an overlay architecture with external knowledge base, neural processing layer, and symbolic reasoning components
- Achieving O(1) computational complexity through pre-computed semantic weights and selective attention mechanisms
- Being biologically plausible by mirroring how human brains operate

Now, let's analyze the active note about multimodal interfaces:

The active note is titled "03_1_всего_2_все" and contains a detailed analysis of multimodal interfaces for interaction with AI, with focus on:
- EMG (electromyography) - electrical signals from muscles
- Video (gestures, facial expressions, body movements)
- Speech with emotional coloring
- Other biosignals (EEG, ECG, eye tracking, breathing, etc.)
- Commercial products and startups in this space
- Open-source projects
- Multimodal AI models (Gemini, Chameleon, Sora, etc.)
- Implemented systems combining EMG with other modalities
- Current challenges and future directions

As a system architect, I need to analyze this note through the lens of the Overlay AGI project's global essence. This note seems to be exploring input modalities for the Overlay AGI system - how humans might interact with an Overlay AGI system using various biological and physical signals.

Let me structure my analysis as requested:

1. In the first section, I'll explain the essence of the problem/task discussed in the note, and provide 5 relevant notes with brief explanations of their relevance.

2. In the second section, I'll discuss the consequences of not solving this problem and provide 5 relevant notes.

3. In the third section, I'll suggest how to solve this problem with 10 relevant notes.

For each section, I'll need to select the most relevant notes from the retrieved documents and explain why they're relevant to the specific aspect I'm discussing.

Let me go through the retrieved documents to identify the most relevant ones for each section:

For section 1 (problem/task essence):
- [[43 Overlay AGI]] - discusses the overlay architecture that would benefit from better multimodal input
- [[44 Overlay AGI]] - similar to above, with focus on practical implementation
- [[21 Overlay AGI]] - describes 20 key scenarios where the system would be activated
- [[19 Overlay AGI]] - specifically mentions input enhancement (S4_Input_Enchance)
- [[S0_ProblemClarification]] - likely discusses fundamental problems being addressed

For section 2 (consequences of no solution):
- [[Code Integrity Collapse]] - discusses what happens when systems lack proper engineering
- [[54 Overlay AGI]] - describes 20 activation scenarios where input is critical
- [[53 Overlay AGI]] - discusses activation thresholds
- [[45 Overlay AGI]] - mentions human input requirements
- [[15 Overlay AGI]] - discusses input context expansion

For section 3 (solution approach):
- [[43 Overlay AGI]] - discusses architecture principles
- [[44 Overlay AGI]] - implementation details
- [[19 Overlay AGI]] - specifically about input enhancement and context expansion
- [[21 Overlay AGI]] - detailed scenarios and implementation paths
- [[15 Overlay AGI]] - discusses input context expansion
- [[S4_Input_Enchance]] - likely a note specifically about input enhancement
- [[S5_Input_Context_Expand]] - likely about context expansion
- [[S6_Input_Convert_to_ InnerThink]] - likely about converting input to internal thinking
- [[S7_Metod_Of_Think]] - likely about thinking methods
- [[S8_PoV_Router]] - likely about point of view routing

Now I'll craft the analysis in the requested structure.

First, I need to explain what the core problem is that this note addresses:

The note is about multimodal interfaces for interaction with AI, particularly focusing on how humans can communicate with AI systems using not just text but also EMG (electromyography - electrical signals from muscles), video (gestures, facial expressions), speech with emotional coloring, and other biosignals. This is directly relevant to the Overlay AGI project, which aims to create a more human-like, biologically plausible AI system. In the Overlay AGI context, better input modalities would allow for more natural human-AI interaction, which is crucial for the system's human-centered design philosophy.

Now for the consequences of not solving this problem - if we don't develop these multimodal interfaces, the Overlay AGI system would be limited to traditional input methods (text, basic voice commands), which would significantly constrain its usability, naturalness of interaction, and ability to understand the full context of human communication.

For the solution approach, I'll draw from the retrieved documents to suggest how the Overlay AGI project could implement these multimodal interfaces, focusing on integrating them with the existing architecture.

Let me now craft the full response following the requested structure.
</think>

# Системный анализ заметки "03_1_всего_2_все" в контексте Overlay AGI

## 1. В чем суть проблемы или задачи о которой заметка

Данная заметка исследует **мульти-модальные интерфейсы** для взаимодействия с искусственным интеллектом, в особенности фокусируясь на использовании электромиографии (ЭМГ) и других биосигналов в качестве каналов коммуникации. В отличие от традиционных методов взаимодействия (текст, речь), заметка рассматривает расширенный набор входных данных: жесты, мимику, речь с эмоциональной окраской, сигналы мозга (ЭЭГ), движение глаз, пульс и дыхание. Ключевой инсайт заключается в том, что комбинация нескольких модальностей значительно повышает надежность распознавания намерений пользователя и создает более естественный опыт взаимодействия.

Система Overlay AGI позиционирует себя как биологически правдоподобную архитектуру, и эта заметка напрямую связана с фундаментальным принципом проекта: **интеллект — это не только вычисление паттернов, но и организация, выбор и понимание значимых связей**. Для реализации истинно человеческого уровня взаимодействия Overlay AGI требует именно таких расширенных входных каналов, которые могут передавать не только явную информацию, но и подтекст, эмоции, физиологическое состояние — всё то, что составляет полноту человеческого общения. Отсутствие этой компоненты сделало бы систему неполноценной с точки зрения когнитивной правдоподобности, о чем прямо говорится в [[43 Overlay AGI]]: "Архитектура позволяет создавать ИИ, которые работают как биологический мозг - знания хранятся вне нейронных сетей (вроде гиппокампа), а принятие решений происходит на основе retrieved информации."

**Список релевантных заметок:**

- [[43 Overlay AGI]] - Подтверждает необходимость биологической правдоподобности в архитектуре, что напрямую связано с использованием естественных для человека мультимодальных входных данных как основы для принятия решений.

- [[19 Overlay AGI]] - Упоминает #S4_Input_Enchance как критически важный компонент системы, что соответствует задаче улучшения входных данных через мультимодальные каналы для повышения качества интерпретации.

- [[21 Overlay AGI]] - Описывает 20 сценариев активации, включая "Human-Centered Interaction" и "Long-form Reasoning Tasks", где мультимодальные входы могут значительно повысить эффективность системы.

- [[15 Overlay AGI]] - Определяет "Input Context Expansion" как ключевой этап в обработке, который напрямую зависит от качества и разнообразия входных данных, включая биосигналы.

- [[S0_ProblemClarification]] - Обосновывает фундаментальную проблему ограниченного взаимодействия с ИИ, которую решают мультимодальные интерфейсы, обеспечивая большую естественность и понимание контекста.

## 2. Последствия отсутствия решения, на что это влияет

Отсутствие эффективных мультимодальных интерфейсов приведет к тому, что Overlay AGI останется ограниченной в своей способности понимать полноту человеческого намерения и контекста. Как указано в [[54 Overlay AGI]], "Система требует полной прозрачности и следуемости решений", что невозможно достичь без понимания не только словесного выражения, но и невербальных сигналов, эмоционального состояния и физиологического контекста пользователя. В результате система будет демонстрировать "паттерн-матчинг без истинного понимания", что противоречит основному постулату Overlay AGI: "ИИ находится в Overlay слое из LangFlow нод и промптов, где псевдокод на человеческом языке исполняет алгоритмы этого Overlay AGI" [^1].

Особенно важно это для сценариев "Human-in-the-loop", где, как указано в [[44 Overlay AGI]], "человек играет ключевую роль в инновациях, а не просто выполняет паттерн-матчинг". Без мультимодального входа система не сможет адекватно интерпретировать тонкие нюансы человеческого взаимодействия и, следовательно, не будет способна на истинное сотрудничество. Это также негативно скажется на эмоциональной связи и доверии к системе, так как пользователь будет ощущать "неестественность" диалога, что критично для применения в образовательных и терапевтических условиях.

**Список релевантных заметок:**

- [[Code Integrity Collapse]] - Предупреждает о последствиях "performative-development" без фундаментальной инженерной строгости, что напрямую применимо к поверхностному, одноканальному взаимодействию без глубокого понимания пользовательского контекста.

- [[54 Overlay AGI]] - Описывает 20 сценариев активации, многие из которых (научные исследования, образовательные инструменты, мобильные приложения) требуют глубокого понимания контекста, невозможного без мультимодального входа.

- [[45 Overlay AGI]] - Подчеркивает необходимость "human-centered design philosophy" и "transparency and traceability", что невозможно достичь без расширенного набора входных данных для понимания истинного намерения пользователя.

- [[15 Overlay AGI]] - Указывает, что "Система обеспечивает полную трассируемость решений", но без мультимодального входа эта трассировка будет неполной, не учитывающей важные аспекты взаимодействия.

- [[21 Overlay AGI]] - Описывает сценарии "Human Collaboration Scenarios" и "Long-form Reasoning Tasks", где отсутствие мультимодального входа приведет к невозможности понимания полного контекста и намерений пользователя.

## 3. Как решить эту проблему

Для интеграции мультимодальных интерфейсов в Overlay AGI необходимо создать модульную архитектуру, которая обрабатывает различные входные сигналы в рамках существующей структуры. Как указано в [[43 Overlay AGI]], система "состоит из нескольких компонентов: семантических весов, LLM селектора (IT-LM), глобального аккумулятора, RAG системы и специализированных моделей по доменам". Мультимодальные входы должны интегрироваться в этот пайплайн следующим образом:

Сначала необходимо создать **модуль предварительной обработки** для каждого типа сигнала (ЭМГ, видео, аудио), который преобразует сырые данные в структурированное представление. Затем, как указано в [[19 Overlay AGI]] в разделе #S4_Input_Enchance, должна происходить **интеграция и усиление входного контекста** - совмещение данных из разных источников, их синхронизация и создание единого представления. Затем, согласно [[15 Overlay AGI]], следует этап **расширения контекста** (S5_Input_Context_Expand), где система использует внешние знания для интерпретации входных сигналов. Наконец, как описано в [[44 Overlay AGI]], входные данные преобразуются в **внутреннее мышление системы** (S6_Input_Convert_to_InnerThink) с последующим применением методов мышления (S7_Metod_Of_Think) и выбором точки зрения (S8_PoV_Router).

Ключевой инсайт: мультимодальные входы должны напрямую интегрироваться в архитектуру семантических весов, обеспечивая более точное представление контекста для LLM селектора. Это соответствует концепции [[53 Overlay AGI]], где "интеллект не только вычисляет паттерны, а организует и выбирает значимые связи". ЭМГ и другие биосигналы могут служить дополнительным источником информации для определения весов этих связей, особенно когда текстовый контекст неоднозначен.

**Список релевантных заметок:**

- [[43 Overlay AGI]] - Предоставляет основы архитектуры, на которую можно наложить мультимодальные входы, особенно через компоненты "LLM Selector" и "Semantic Weight Tables", которые могут использовать дополнительные биометрические данные для уточнения выбора следующих слов.

- [[19 Overlay AGI]] - Содержит прямые ссылки на #S4_Input_Enchance и #S5_Input_Context_Expand, что составляет основу для обработки и усиления мультимодальных входов перед их использованием системой.

- [[44 Overlay AGI]] - Описывает "Russian_review", где указано, что "ИИ находится в Overlay слое из LangFlow нод и промптов", что соответствует идее разработки специализированных нод для обработки ЭМГ, видео и других сигналов.

- [[21 Overlay AGI]] - Детализирует 20 сценариев, включая "Human-Centered Design Application" и "Long-form Reasoning Tasks", где мультимодальные входы критичны, и описывает, как интегрировать их с существующими компонентами.

- [[15 Overlay AGI]] - Объясняет концепцию "Input Context Expansion" и "Input Enhancement", что напрямую применимо к интеграции мультимодальных сигналов для создания более богатого контекста.

- [[54 Overlay AGI]] - Предоставляет полную структуру компонентов, включая #S4_Input_Enchance, #S5_Input_Context_Expand, #S6_Input_Convert_to_InnerThink, #S7_Metod_Of_Think, и #S8_PoV_Router, которые образуют идеальную структуру для обработки мультимодальных входов.

- [[S17_OverlaySemanticWeight]] - Должен быть адаптирован для включения весов, полученных из биосигналов, чтобы селектор мог учитывать не только семантическое, но и эмоциональное/физиологическое соответствие.

- [[45 Overlay AGI]] - Конкретизирует процесс "human-centered design", который требует именно мультимодального входа для полного понимания пользовательского контекста и намерений.

- [[S9_Overlay_NeuralNet_N2S]] - Содержит идеи нейро-нейро-символической архитектуры, которые можно расширить для обработки нейробиологических сигналов как прямого входа в систему.

- [[S11_LLM_Selector]] - Требует модификации для работы с мультимодальными входами, расширяя текущую функциональность выбора слов на основе внешних знаний до учета физиологического и эмоционального контекста.

#### Sources
[^1]: [[44 Overlay AGI]]
[^2]: [[54 Overlay AGI]] (0_chatgpt/AI/2025-10-05 - 2OverlayAI/54 Overlay AGI.md)
[^3]: [[19 Overlay AGI]] (0_chatgpt/AI/2025-10-05 - 2OverlayAI/19 Overlay AGI.md)

>[!nexus_agent] **Assistant** - 13.06.2025 at 14:28
> Отлично. Я проведу глубокое исследование текущего состояния технологий взаимодействия с ИИ с помощью ЭМГ и других мультимодальных каналов — включая голос, видео, движения и прочие биосигналы. Буду учитывать как научные публикации и академические решения, так и промышленные продукты, стартапы и open-source проекты.
> 
> Как только всё будет готово, я представлю тебе структурированный обзор с конкретными примерами, ссылками и выводами.
<!-- UID: 3565ad5c-eb17-4e3a-88b5-5e0c48aa1bbc -->

---

>[!nexus_agent] **Assistant** - 13.06.2025 at 14:35
> # Мультимодальные интерфейсы для взаимодействия с ИИ: текущее состояние технологий
> 
> **Введение.** Мультимодальные интерфейсы позволяют человеку общаться с искусственным интеллектом (ИИ) не только с помощью текста или обычных GUI, но и через множество сигналов: голос (с учетом эмоций и интонации), видео (жесты, мимику, позы), электромиографию (ЭМГ – электрическую активность мышц), а также другие биосигналы (например, электроэнцефалограмму – ЭЭГ, электрокардиограмму – ЭКГ, движение глаз, дыхание). Современные исследования и разработки в этой области направлены на то, чтобы сделать взаимодействие с ИИ более естественным, похожим на общение между людьми, и одновременно учитывать физическое и эмоциональное состояние пользователя. Ниже представлен структурированный обзор: от научных достижений и инженерных реализаций до коммерческих продуктов, open-source проектов, актуальных моделей ИИ и основных вызовов.
> 
> ## 1. Научные и инженерные разработки мультимодальных интерфейсов
> 
> **Обзор подходов.** В последние годы наблюдается быстрый прогресс в объединении разнородных сигналов для создания более «понимающих» ИИ-систем. Мультимodalьное взаимодействие значительно обогащает возможности HCI (human-computer interaction) за счет одновременной обработки речи, жестов, взгляда, физиологии и др. Это делает коммуникацию с машинами более близкой к естественной【20†L527-L536】【20†L540-L549】. Главное преимущество – повышенная устойчивость и контекстная осведомленность интерфейсов. Например, голосовая команда может дополняться жестом указания на объект, уточняя намерение пользователя【20†L541-L549】. В AR/VR-средах взгляд и положение тела дополняют голос, упрощая hands-free взаимодействие【20†L543-L551】. Ключевая техническая задача – правильно объединить (синхронизировать и проанализировать) данные разных типов, чтобы они **дополняли друг друга, а не конфликтовали**【20†L551-L559】【20†L554-L562】. Это требует разработок в области **фьюжн-алгоритмов**, которые либо на ранней стадии объединяют сырьевые потоки (early fusion), либо на поздней – совмещают результаты отдельных модальностей (late fusion)【3†L102-L110】【22†L319-L327】. Современные архитектуры часто используют механизмы внимания (Transformers) для обучения общим представлениям, способным связывать разнородные данные【22†L359-L368】【22†L406-L414】. Ниже рассмотрены отдельные модальности и примеры их комбинации.
> 
> ### Электромиография (ЭМГ) в интерфейсах человека и ИИ
> 
> Электромиографические сигналы – электрические импульсы, возникающие при мышечных сокращениях – применяются как новый источник ввода команд и индикатор состояний. **Практические примеры**: MIT Media Lab разработала носимый интерфейс **AlterEgo**, позволяющий “произносить про себя” команды ИИ, двигая внутренними мышцами речи без звучания голоса【32†L86-L94】. Система считывает ЭМГ с поверхности лица и шеи при беззвучной артикуляции слов и передает ответ пользователю через костную проводимость звука, образуя скрытый от окружающих цикл общения человека с ИИ【32†L88-L96】【32†L100-L108】. AlterEgo успешно распознает внутреннюю речь, что может помочь людям с нарушениями голосового аппарата и обеспечить бесшумное взаимодействие с ассистентами.
> 
> Другой пример – **EMG-контроллеры для жестов**. Стартап CTRL-Labs (приобретенный Meta) разработал браслет, считывающий нервно-мышечные сигналы запястья для управления компьютером силой мысли (точнее, легкими движениями пальцев)【28†L278-L287】【28†L290-L299】. В лаборатории Meta показали, что такой EMG-интерфейс может улавливать даже миллиметровое движение пальцев или намерение шевельнуть им – фактически превращая едва заметное движение в цифровую команду【28†L295-L303】. Это открывает путь к *ультра-низкопороговому вводу*: представьте, что для выбора пункта меню достаточно едва ощутимого импульса в мышце пальца【28†L260-L269】【28†L278-L287】. На изображении ниже показан экспериментальный прототип EMG-браслета Meta:
> 
> 【30†embed_image】 *Прототип нейроинтерфейса на запястье от Facebook Reality Labs (Meta), считывающего электрические сигналы мышц руки для управления AR-очками. Браслет использует электромиографию (ЭМГ) для преобразования нервных импульсов в цифровые команды【28†L290-L299】【28†L295-L303】.*
> 
> **Комбинация ЭМГ с другими сигналами.** В исследовательских прототипах ЭМГ часто объединяют с визуальными данными для повышения надежности распознавания. Например, в системе протезирования руки Erdogmus et al. (2024) использовали **мультимодальную фьюжн**: одновременно анализируются видео от камеры на голове, направление взгляда (eye-tracking) и ЭМГ предплечья, чтобы угадать, какой хват рукой задумал пользователь【14†L13-L21】. Предложен баесовский метод слияния, благодаря которому точность определения типа хвата возросла ~на 14% по сравнению с одной лишь камерой или одним ЭМГ【14†L19-L27】【14†L43-L52】. Иными словами, визуальная информация и сигналы мышц дополняют друг друга: когда рука только тянется к предмету, по видео видно форму объекта, а по ЭМГ – какая группа мышц активируется; вместе эти данные дают более ранний и точный прогноз, чем каждый по отдельности【14†L13-L22】【14†L39-L47】. Другая область – **беззвучное управление голосом**: известна проблема распознавания речи в шуме или невозможности говорить вслух (например, для людей с ограничениями). Решение – сочетать видеонаблюдение за артикуляцией губ (визуальная речь) и ЭМГ сигналов речевых мышц. В работе Yau et al. (2008) предлагалось распознавать гласные по ЭМГ с лица, а согласные – по видеодвижениям губ【15†L137-L146】【15†L168-L176】. Совсем недавно представлен открытый датасет **AVE Speech**, где записаны одновременно аудио, видео (движение губ) и поверхностная ЭМГ лица для 100 говорящих людей【25†L129-L138】【25†L131-L139】. Комбинирование всех **трех модальностей** значительно повышает надежность речевого ввода: так компенсируются недостатки каждой из модальностей (когда аудиосигнал заглушен шумом – выручает чтение по губам или ЭМГ с лица, а камера не различает похожие артикуляции – мышечные сигналы их различат)【25†L83-L92】【25†L89-L98】. Таким образом, ЭМГ в составе мультимодальных интерфейсов уже доказала эффективность для жестового управления, бесшумных команд и распознавания действий в сочетании с видео и аудио.
> 
> ### Видео: жесты, мимика и движения как каналы ввода
> 
> Визуальное восприятие человека ИИ-системой открывает целый спектр *невербальных сигналов*. **Распознавание жестов** широко исследуется для управления интерфейсами: современные алгоритмы (например, медиапайп от Google или OpenPose) в реальном времени отслеживают позу тела, положение рук и пальцев, позволяя понимать указательные жесты, жесты «да/нет» головой и т.д. Интеграция жестов с голосовыми помощниками дает более естественный опыт: пользователь может спросить голосом: *«Что это?»*, одновременно указывая рукой на объект – и ассистент (при помощи камеры) поймет контекст вопроса. В системах дополненной реальности такая мультимодальность критична【20†L543-L551】. 
> 
> **Мимика лица** – еще один важный канал. Выражение эмоций (улыбка, удивление, хмурость) может неявно сигнализировать ИИ о реакции пользователя. Современные модели компьютерного зрения способны распознавать базовые эмоции по лицу; активно исследуется совмещение этого с анализом голоса и физиологии для **аффективных вычислений** – оценки эмоционального состояния. Например, для определения уровня стресса или интереса объединяют данные камеры (микромимика, контакт глаз) с пульсом, кожно-гальванической реакцией и т.п.【20†L531-L538】【24†L149-L157】. Такие системы могут адаптировать подачу информации под настроение пользователя (например, обучающие программы, замечая усталость по взгляду и выражению лица, сделают паузу).
> 
> **Видео движений тела** дает контекст о ситуации пользователя: идет ли он, сидит, чем занимается руками. Для ИИ-ассистента это ценные сведения. Например, умный домашний помощник может по камерам понять, что вы лежите (и, возможно, засыпаете) и автоматически приглушить музыку. Или бортовой ИИ автомобиля, наблюдая за водителем через инфракрасную камеру, распознает признаки усталости по морганию и позе, и предложит отдых. Такие возможности уже используются в системах **Driver Monitoring** (например, в коммерческих решениях Smart Eye, Seeing Machines).
> 
> **Технические решения.** Для обработки видео разработаны специализированные нейросети: сверточные сети и трансформеры для анализа изображений, рекуррентные сети или 3D-CNN для динамики жестов. Есть обширные датасеты: от наборов для распознавания действий (Kinetics, NTU RGB+D) до баз мимики (FER2013, RAF-DB) и жестов (например, датасеты для языков жестов с видео). Инженерная задача – сжатие и передача видео в реальном времени для анализа (требует оптимизации или работы на периферии). Кроме того, **конфиденциальность**: постоянное видеонаблюдение может вторгаться в частную жизнь, поэтому нередко пытаются переносить максимум компьютерного зрения на само устройство (например, нейронные чипы в камерах смартфонов).
> 
> ### Речь и голосовые сигналы с эмоциональной окраской
> 
> Голос – одна из самых развитых модальностей взаимодействия с ИИ (вспомним успех голосовых помощников). Новейшие разработки идут дальше простой расшифровки слов: **паралингвистические признаки** (тембр, интонация, скорость, паузы) используются для извлечения эмоций и состояния говорящего. Например, ИИ может уловить раздражение или фрустрацию по тону пользователя и скорректировать стиль ответа. Известно, что Alexa от Amazon училась распознавать “фрустрированный” тон, когда не понимает команд, чтобы извиняться и уточнять. Датасеты вроде RAVDESS, CREMA-D содержат речевые фразы, выразительно произнесенные с разными эмоциями, для обучения моделей эмоционального анализа. 
> 
> **Совмещение с видео лица:** даёт еще лучший результат в определении эмоций (мультимодальные эмоцие-распознающие системы показывают более высокую точность, чем только аудио или только видео【24†L149-L157】). Так, серия работ по мультимодальному анализу эмоций объединяет аудиоспектр с видеокадрами лица и даже с физиологией (ЭЭГ, ЭМГ лица). Обзор 2025 года Pillalamarri et al. отмечает, что ключевой вызов – **свести воедино разные представления сигналов** и найти коррелированные паттерны эмоций【24†L61-L69】. Иногда признаки эмоций слабы и размыты, но комбинированный анализ (например, легкое дрожание голоса + микродвижение бровей + учащенное сердцебиение) дает уверенную классификацию настроения.
> 
> **Голосовые команды в шуме:** как упоминалось выше, проблема шумной среды решается добавлением других модальностей. В дополнение к видео губ и ЭМГ, также применяют направленные микрофоны, ультразвуковые датчики речи (как в работе Freitas et al., использующей ультразвуковой доплер для улавливания движений речевых органов【15†L173-L182】). В итоге мультимодальные системы могут распознавать речь даже когда человек шепчет или окружающий шум громкий – за счет “подглядывания и подслушивания” сразу несколькими способами.
> 
> ### Другие биосигналы: ЭЭГ, ЭКГ, взгляд, дыхание и т.д.
> 
> **Мозговые сигналы (ЭЭГ)** предоставляют прямой канал к мыслям пользователя, хотя и сложный в расшифровке. Неинвазивные BCI (brain-computer interfaces) на основе ЭЭГ уже позволяют выполнять простые команды – например, мысленно выбирая между несколькими опциями, различимыми по образцу электроэнцефалограммы. В мультимодальных интерфейсах ЭЭГ становится перспективным дополнительным источником контекста: она улавливает уровень концентрации, усталости, эмоционального возбуждения. Обзор по BCI в HCI отмечает, что интеграция ЭЭГ с другими модальностями (речь, жест) обещает более глубокие и **иммерсивные** сценарии – особенно в VR/AR, играх и для людей с ограниченной подвижностью【21†L23-L31】【21†L25-L33】. Например, в VR можно совмещать отслеживание движений тела и глаз с ЭЭГ, чтобы игра реагировала на уровень страха или напряжения пользователя.
> 
> **Глазодвигательные сигналы (EOG) и трекинг взгляда.** Направление взгляда – мощный способ указать интерес или объект внимания. Очки дополненной реальности и современные VR-шлемы часто оснащаются отслеживанием глаз (технология **eye tracking**). Оно уже используется для управления интерфейсом (фиксируя взгляд на кнопке, пользователь может выбрать ее морганием) и для контекстуализации команд. EOG – электрическая активность глазных мышц – тоже применяется, хотя чаще в медицинских/исследовательских системах. В связке с ЭМГ и EEG EOG дает картину того, **куда** пользователь смотрит, когда, например, подает мысленную команду. Так, в упомянутой выше системе Galea (см. далее) есть и EOG-датчики, и в работе Erdogmus (2024) взгляд помогал понять, на какой предмет направлено действие руки【14†L13-L21】.
> 
> **Сердечный ритм и дыхание.** Эти сигналы отражают физиологическое состояние – стресс, спокойствие, усталость. Например, участившийся пульс и дыхание могут указывать на волнение, даже если голос и лицо стараются это скрыть. В диалоговых системах будущее видят в учете таких неочевидных признаков: ИИ-помощник в автомобиле мог бы почувствовать растущий стресс водителя (по пульсу через умные часы) и мягко вмешаться, напомнив о перерыве. Уже существуют носимые фитнес-датчики, которые передают пульс и частоту дыхания смартфонам – следующий шаг интеграции их показаний в реальный диалог или поведение приложений.
> 
> **Пример – интегрированная гарнитура Galea.** Компания OpenBCI совместно с Varjo создала гарнитуру **Galea** – модуль, совмещенный с VR-очками, включающий полный набор сенсоров: 10 каналов ЭЭГ, 4 канала ЭМГ, 2 канала EOG (глазные движения), PPG (пульс), EDA (кожно-гальваническая реакция)【35†L112-L119】. Таким образом, устройство одновременно пишет активность мозга, глаз, мышц, сердца и кожи, давая исследователям богатый поток синхронизированных данных**【35†L112-L119】. Galea позиционируется как платформа для новых интерфейсов в XR (расширенной реальности): по реакциям мозга и тела на виртуальные события можно динамически подстраивать контент или измерять эмоциональное вовлечение пользователя【35†L97-L104】【35†L100-L107】. Несмотря на высокую стоимость и пока ориентированность на R&D, Galea демонстрирует техническую реализуемость мульти-сенсорных гарнитур для человеко-машинного взаимодействия.
> 
> **Методы синхронизации и слияния данных.** Сбор данных с разных датчиков требует синхронного времени. В лабораторных установках нередко один прибор подает тактовый сигнал остальным – например, система ЭМГ может рассылать метку синхронизации камерам и аудиорекордерам【15†L159-L168】【15†L162-L170】. В программной обработке применяются методы *временного выравнивания*: от простой интерполяции и растяжения сигналов до алгоритмов вроде Dynamic Time Warping, или обучения моделей, которые сами находят соответствие (через механизмы внимания, корреляции особенностей). Фьюжн-алгоритмы делят на **ранние** (объединение сырьевых данных или признаков в единый вектор перед подачей в модель) и **поздние** (отдельные модели по модальностям, чьи результаты комбинируются решающим правилом)【3†L102-L110】. Последний подход (late fusion) проще и долго преобладал, но ограничен – он не позволяет модели учиться глубоким межмодальным зависимостям【3†L102-L110】【3†L103-L111】. Поэтому тренд – **унифицированные модели** раннего слияния, где все входы представлены в общем формате. Например, токен-базированное представление: изображения разбиваются на визуальные «токены», аналогичные словам, и единая трансформер- модель обучается на последовательностях из текста, изображений и других символов【3†L109-L117】【3†L118-L125】. Так достигается **бесшовная интеграция**: модель сама решает, как распределять внимание между словами и пикселями. Однако раннее слияние предъявляет высокие требования к обучающим данным и ресурсам – для покрытия всех комбинаций modalities нужны огромные мультимодальные корпуса и вычисления【33†L129-L137】【33†L119-L127】. Ниже, в разделе про модели, мы подробнее рассмотрим такие архитектуры.
> 
> **Datasets.** Для обучения мультимодальных систем созданы разнообразные датасеты. Помимо упомянутых наборов для эмоций, жестов и речи, существуют наборы объединяющие физиологию с видео: *DEAP* (ЭЭГ + ЭКГ + видео лица для эмоций), *WESAD* (носимый набор для стресса: пульс, дыхание, акселерометры и т.д.), *MAHNOB-HCI* (ЭЭГ, видео и аудио эмоциональных реакций). Вызов в том, что такие сборы обычно относительно небольшие (десятки участников), трудно собрать «массовый» датасет с физиологией из-за сложности сенсоров и приватности. Тем не менее, даже ограниченные по объему данные позволили добиться улучшений: по обзору 2025 г., комбинирование ЭЭГ с поведением (мимика/речь) чаще повышает точность эмоцие-распознавания, хотя успех зависит от правильно выбранных методов слияния【24†L61-L69】【24†L65-L73】. 
> 
> ## 2. Коммерческие продукты и стартапы в области мультимодальных интерфейсов
> 
> Мультимодальность и нейроинтерфейсы – горячая тема не только в науке, но и в индустрии. Ниже перечислены примеры компаний и продуктов, которые уже предлагают решения с использованием **нескольких модальностей** во взаимодействии человека и машины:
> 
> - **Neuralink** – стартап Илона Маска, разрабатывающий инвазивные нейроимпланты. Цель – прямое считывание сигналов мозга (ЭСГ) и передача команд на компьютер. Пока это экспериментальная технология (импланты испытываются на животных и первых волонтерах), но в перспективе – полноценный BCI для парализованных людей и, возможно, массовый интерфейс “силой мысли”. Neuralink фокусируется больше на медицине, но внимание к нейроинтерфейсам подогрело целое поле стартапов.
> 
> - **OpenBCI** – компания, ориентированная на открытые нейротехнологии. Помимо вышеупомянутой гарнитуры Galea (EEG/EMG/EOG для VR)【35†L112-L119】, OpenBCI выпускает недорогие платы и гарнитуры для снятия ЭЭГ/ЭМГ (например, Ultracortex и Cyton) и программные инструменты. Это стимулирует стартапы и исследователей разрабатывать свои приложения: от игр с контролем вниманием до биофидбэк-тренажеров. OpenBCI – движитель **open-source нейроинтерфейсов** с сообществом разработчиков.
> 
> - **Emotiv** – коммерческие EEG-гарнитуры (Epoc, Insight) с сухими электродами, нацеленные на простоту использования. Emotiv продает решения для исследований и корпоративных клиентов (например, мониторинг концентрации сотрудников). Их SDK определяет базовые метрики по ЭЭГ: уровень стресса, нагрузка, интерес. Это один из первых игроков, делающих BCI более прикладным.
> 
> - **Meta (Facebook)** – активно инвестирует в мультимодальные AR-интерфейсы. После покупки CTRL-Labs Meta работает над **EMG-браслетом** для управления AR-очками. В их концепции AR Future Interface, как обсуждалось, браслет считывает нервные импульсы при микродвижениях пальцев, позволяя набрать текст или кликнуть “силой мысли”【28†L290-L299】【28†L295-L303】. Плюс, AR-очки Meta (Project Nazare, Ray-Ban Stories) уже оснащены камерами, микрофонами и в перспективе получат отслеживание глаз – то есть полный набор модальностей (видео+аудио+EMG). Meta также исследует **мультимодальный ИИ** (см. модель Chameleon далее) для понимания таких разнородных входных данных.
> 
> - **Affectiva (Smart Eye)** – стартап, лидер в области **AI-эмоций**, уже около десятилетия продает SDK для распознавания эмоций по лицу и голосу. Их технологии используются в автомобилях (отслеживать состояние водителя по мимике и тону речи) и в исследованиях рекламы (реакция зрителей). Хотя они фокусируются на двух модальностях (видео + аудио), это пример коммерческой реализации мультимодального анализа. В 2021 г. Affectiva была приобретена шведской компанией Smart Eye, что говорит об интересе рынка к таким технологиям.
> 
> - **Tobii** – мировой лидер в **eye-tracking**. Выпускает как отдельные устройства (настольные и накладные трекеры взгляда), так и интегрирует свои модули в ноутбуки, VR-шлемы (например, HTC Vive Pro Eye). Tobii продвигает взгляд как интерфейс: в играх (целиться глазами), в assistive-tech (управление компьютером взглядом для парализованных), в исследованиях UX. Хотя сам по себе трекер – не мультимодальный, он часто используется совместно с другими вводами (клик глазами + клавиатура, или взгляд + жест).
> 
> - **Microsoft HoloLens** – устройство смешанной реальности, которое поддерживает *мультимодальный ввод*: у него есть камеры для жестов (распознает движения рук и пальцев), микрофоны для голоса (голосовые команды), и встроенный трекер глаз. HoloLens дает пример интеграции: пользователь, глядя на голограмму, может сказать “окей” и сделать жест «сжать руку» – сочетание взгляда, речи и жеста обеспечивает естественную манипуляцию 3D-объектами. Конкуренты (Magic Leap, будущий Apple Vision Pro) аналогично комбинируют несколько сенсоров.
> 
> - **Автомобильные системы мониторинга**: Уже упомянуты Smart Eye, Seeing Machines – камеры в салоне авто, которые отслеживают и предупреждают о засыпании, отвлечении. Новые модели авто также интегрируют **голосовых ассистентов**, распознающих команды водителя. Стартап **Cerence** (спин-офф Nuance) разрабатывает мультимодальный автомобильный AI: одновременно анализирует речь, взгляд (куда водитель смотрит на навигатор) и даже жесты (некоторые BMW понимают жесты руками для управления мультимедиа). Таким образом, автомобиль становится мультимодальным ассистентом на колесах.
> 
> - **Робототехника и цифровые аватары:** Компании как Furhat Robotics предлагают гуманоидных роботов-ассистентов с мультимодальным общением – робот имеет подвижное лицо с мимикой, камеры для зрения человека и микрофоны для его речи, а сам говорит синтезированным голосом с выражением. Стартап **Soul Machines** создает *цифровых людей* – анимированные на экране аватары с ИИ, которые читают выражение лица собеседника и сами демонстрируют эмоциональные реакции, поддерживая разговор. Это коммерческое применение мультимодальности в обслуживании клиентов, образовании и т.д.
> 
> - **Устройства для здоровья и спорта:** Умные часы (Apple Watch, Fitbit) и трекеры сейчас снабжаются множеством сенсоров: пульс, ЭКГ, SpO2, акселерометр, микрофон. Эти гаджеты еще не активно используются для взаимодействия с AI (кроме голосового ассистента в них), но собираемые ими данные уже применяются в здоровье и фитнесе. Стартап **NeuroSky** ранее продвигал гарнитуры с ЭЭГ для игр и медитации (например, знаменитая игрушка-«силой мысли» Star Wars Force Trainer). Новые компании, как **NextMind** (куплена Snapchat) делали нейрогарнитуру для AR (считывала зрительную кору мозга, позволяя выбирать объект, на который сконцентрирован взгляд). Коммерциализация этих штук трудна, но интерес высок: AR/VR гиганты хотят добавить нейроэлементы в свои продукты, чтобы повысить восприимчивость к пользователю.
> 
> **Итого:** коммерческий сектор активно исследует мультимодальные ассистенты. Tech-гиганты (Meta, Google, Microsoft, Apple) вкладываются в устройства, способные *видеть, слышать и чувствовать* пользователя, а AI-стартапы предлагают нишевые решения: от эмоционального AI для call-центров (распознавать настроение клиента по голосу) до устройств для молчаливой связи (как AlterEgo). Многие такие продукты находятся на ранних этапах, но тренд очевиден – интерфейсы становятся все более обогащенными сенсорами, чтобы ИИ мог **понимать контекст и намерения**, выходящие за рамки текста.
> 
> ## 3. Open-source проекты, библиотеки и доступные модели
> 
> Сообщество с открытым исходным кодом играет большую роль в продвижении мультимодальных технологий. Существует ряд проектов и библиотек, которые разработчики и исследователи могут свободно использовать для своих систем:
> 
> - **OpenBCI & BrainFlow:** Как отмечалось, OpenBCI предоставляет открытые аппаратно-программные решения для био-сигналов. Дополняет это библиотека **BrainFlow** – унифицированный SDK на Python/C++, умеющий подключаться к разным биосенсорным платам и выполнять базовую обработку ЭЭГ, ЭМГ, ЭКГ и др.【17†L25-L33】. BrainFlow абстрагирует работу с датчиками, позволяя фокусироваться на анализе сигналов. Плюс, проекты NeuroTechX и другие ведут списки *awesome BCI* с ссылками на открытые инструменты (фильтры сигналов, алгоритмы классификации состояний и т.п.)【17†L1-L8】.
> 
> - **OpenFace, OpenPose, MediaPipe:** Для визуальных модальностей есть популярные open-source библиотеки. **OpenPose** (CMU) – открытый код для детекции позы человека, скелета тела, рук, лица по видео. **OpenFace** – библиотека для анализа выражения лица (распознает действия мышц лица – Action Units – по видео). **MediaPipe** (Google) – фреймворк с готовыми модулями: детекция рук, отслеживание лицевых точек, определение позы, трекинг глаз и даже базовое определение эмоций. Эти инструменты можно интегрировать в свои приложения для получения потоков высокоуровневых визуальных признаков (координат суставов, вероятностей эмоций и пр.) без обучения с нуля.
> 
> - **OpenSMILE, PyAudioAnalysis:** Для аудио модальности есть **open-source** библиотеки выделения голосовых характеристик. OpenSMILE – набор для извлечения тысяч аудиодескрипторов, использовался в многих исследованиях по эмоциям из речи. PyAudioAnalysis – Python-библиотека для обработки аудио, включая эмоциональный классификатор. А **SpeechBrain** и **ESPnet** – полные фреймворки для разработки моделей распознавания речи, озвучивания текста и др., позволяющие также создавать мультивходовые модели (например, аудио+видео для lip reading).
> 
> - **Hugging Face Transformers & Multimodal Models:** Платформа HuggingFace предоставляет доступ ко многим открытым мультимодальным моделям. Например, есть репозитории:
>  - **CLIP** (OpenAI) – модель связывания изображения и текста в общее пространство эмбеддингов. Хотя сама обучена OpenAI, открыты веса аналогов (например, от LAION). CLIP используется для задач поиска изображений по описанию и наоборот.
>  - **BLIP/BLIP-2** – открытые модели от Salesforce для генерации описаний к изображениям и визуальный QA.
>  - **LLaVA** (Large Language and Vision Assistant) – открытая реализация, объединяющая языковую модель LLaMA с визуальным энкодером (результат – модель, умеющая отвечать на вопросы по картинке, подобно GPT-4V)【3†L143-L151】【33†L153-L161】.
>  - **OpenFlamingo** – проект с открытым кодом от LAION, воспроизводящий подход DeepMind Flamingo (мультимодальный few-shot Learner, приученный на паре изображение-текст). Он позволяет взять любую свою язык. модель и «скрестить» с визуальным энкодером.
>  - **MMF (Multimodal Framework)** от Facebook AI – фреймворк (ныне называется TorchMultimodal), облегчающий эксперименты с объединением текста, изображения и др. 
> 
> - **Проекты для мультимодального общения:** Например, **Rasa** – открытая платформа для чат-ботов – теперь поддерживает не только текст, но и голосовые каналы (ASR и TTS подключаются). **Mozilla DeepSpeech** (ASR) и **Mozilla TTS** – открытые модели, которые можно объединить для голосового ввода-вывода. Их можно интегрировать с визуальными подсистемами для создания собственного локального голосового ассистента, который видит (через OpenCV) и слышит.
> 
> - **OpenDatasets:** Помимо программ, ценно наличие открытых датасетов. В 2023–2024 появилось несколько новых: уже упомянутый **AVE Speech** (аудио-видео-EMG для речи)【25†L129-L138】; **MuonDB** (условно, набор физиологии + видео для VR); **TEAP** – аудио+текст+физиология в диалогах и т.п. А международные соревнования (Emotion Recognition in the Wild, Multimodal Sentiment Analysis Challenge) стимулируют выкладку мультимodal данных. Исследователи активно делятся кодом на GitHub для своих мультимodal моделей – что упрощает воспроизведение передовых результатов.
> 
> - **Jina AI и мультимодальный поиск:** Проект Jina (Python framework) предлагает реализовать **поиск по мультимодальным запросам** – например, найти документ по комбинации текста+изображения. Он имеет модули для обработки разных типов данных и объединения их в едином поисковом индексе【16†L27-L34】. Это пример инфраструктурного инструмента: т.е. разработчики могут строить приложения, где пользователь, скажем, загружает фотографию и пишет текст – а система находит релевантный контент, учитывая оба сигнала.
> 
> В целом open-source экосистема покрывает почти все составляющие: от низкоуровневой обработки сырого сигнала (BioSig, OpenBCI), до готовых высокоуровневых моделей (CLIP, LLaVA) и фреймворков для обучения своих моделей. Это крайне важно, так как **мультимодальные исследования требуют интердисциплинарности** – и открытые инструменты помогают исследователям из разных областей (например, врачам, лингвистам) использовать алгоритмы слияния данных без глубокого погружения в код с нуля.
> 
> ## 4. Мультимодальные модели ИИ нового поколения (Google Gemini, Meta Chameleon, OpenAI Sora и др.)
> 
> Развитие больших моделей (LLM) привело к появлению **фундаментальных ИИ**, способных работать сразу с несколькими типами данных. Ниже рассмотрены ключевые мульти-модели и их особенности, а также как они реализуются в интерфейсах.
> 
> **GPT-4 и последователи.** Модель GPT-4 от OpenAI, выпущенная в 2023, стала одним из первых массовых примеров мультимодальной LLM – она способна принимать на вход не только текст, но и изображения. В версии ChatGPT Vision пользователи могут отправлять картинку и получать ответ на естественном языке с описанием содержимого или анализом изображения. Хотя архитектурно GPT-4V – это языковая модель с дополнительным визуальным энкодером, сам факт интеграции уже демонстрирует ценность: единый ИИ может и видеть, и рассуждать. OpenAI пошла дальше – в конце 2024 представлена модель **Sora**, генерирующая видео по тексту【36†L165-L173】. Sora – это, по сути, мультимодальная модель: она принимает текстовые (и визуальные) подсказки и выдает *динамические визуальные ряды* (видео). Согласно Reuters, Sora была анонсирована в феврале 2024 как исследовательская, а в декабре запущена для пользователей ChatGPT Plus (режим *Sora Turbo*)【36†L175-L183】【36†L177-L185】. Пользователи могут вводить текстовый сценарий, а ИИ создаст 20-секундный видеоролик 1080p по описанию【36†L181-L189】. OpenAI позиционирует это как расширение возможностей ChatGPT и конкуренцию аналогичным разработкам от Meta и Google【36†L169-L177】. Пока Sora ограниченно доступна (не запущена в ЕС из-за регуляций и имеет ограничения по контенту)【36†L187-L194】, но факт интеграции генерации видео в популярный интерфейс знаменует новый этап – мультимодальный **генеративный** ИИ для широкого круга задач (текст, изображение, видео, аудио – все в одном флаконе).
> 
> **Google Gemini.** Ответ от Google – семейство моделей **Gemini**. В декабре 2023 Google анонсировала Gemini 1.0, изначально обученный как мультимодальный фундамент (текст, код, изображения и т.д.)【9†L383-L392】【9†L385-L393】. Gemini проектировался совместно командами Google Research и DeepMind “с нуля как мультимодальный” – в отличие от более ранних систем, где модальности добавлялись к готовой языковой модели позднее【8†L25-L33】【8†L49-L52】. Уже первая версия Gemini Ultra (самая большая из трех масштабов модели) показала выдающиеся результаты: превосходит GPT-4 на множестве текстовых и кодовых бенчмарков【9†L402-L410】【9†L413-L421】, а также установила SOTA на мультимодальном тесте MMBench (59.4%, противостоя GPT-4V)【9†L422-L431】【9†L434-L441】. Gemini **поддерживает на входе текст, изображения, аудио, видео, код**【9†L383-L392】【9†L385-L393】. Google отмечает, что традиционно мультимодели строили по принципу «отдельный модуль на каждую модальность + слияние», что ограничивает глубокое объединение знаний【8†L23-L31】【8†L25-L33】. В Gemini вместо этого применено единое токен-пространство: изображения дробятся на токены как слова, и единый Transformer обучен на последовательностях с **интерливингом** – произвольным чередованием текста и визуальных токенов【3†L102-L110】【3†L109-L117】. Это позволяет модели **генерировать смесь текста и изображений на выходе** – т.е. написать осмысленный ответ и вставить сгенерированную картинку в нужном месте【3†L100-L107】【33†L159-L167】. Например, Gemini может по запросу создать целый документ с текстом и иллюстрациями. Обучение такого раннего слияния – нетривиальная задача (Google сообщает о множестве технических решений для масштабирования)【33†L129-L137】. Gemini 1.0 обучался на колоссальном наборе данных (~4.4 трлн токенов различных комбинаций текст+изображения) и запущен в трех размерах (Ultra, меньший Pro для широких задач и Nano для мобильных)【9†L394-L402】. В 2024 вышла версия Gemini 2.0, продолжающая развитие (Google уже предоставляет API к Gemini для разработчиков). Мультимодальные способности Gemini уже интегрируются в продукты: например, в сервис **Bard** (чат-бот Google) добавили функцию отправки изображений с последующим описанием или анализом. Кроме того, Gemini нацелена на “агентов” – Google называет ее моделью для **agentic AI**, способной управлять инструментами (поиском, API) комбинируя различные типы информации【6†L9-L16】. По сути, Google стремится к универсальному мозгу, одинаково хорошо понимающему и текст, и визуальную информацию, и, возможно, аудио.
> 
> **Meta Chameleon (CM3leon).** Meta AI также создает свои мультимодальные основы. В 2023 Meta представила **CM3leon** (кодовое название, от Chameleon) – архитектуру генеративной модели, умеющей как понимать, так и порождать текст и изображения единым образом【2†L23-L31】【3†L86-L94】. Особенность Chameleon (название в статье – “Mixed-modal early-fusion models”) – **раннее слияние на уровне токенов** (как и у Gemini) и **отсутствие отдельных декодеров**: та же сеть генерирует и текст, и картинку токен за токеном【3†L109-L117】【3†L117-L125】. Эксперименты Meta показали, что Chameleon с ~34 млрд параметров достигает SOTA в описаниях изображений и визуальных вопросах, превосходя Flamingo и другие модели, а на чисто текстовых задачах не сильно уступает профильным LLM【3†L91-L99】【3†L143-L151】. Архитектура *унифицированного токен-пространства* делает Chameleon крайне гибкой: она может продолжить изображение текстом или наоборот в любой последовательности【3†L109-L117】【33†L159-L167】. На рисунке ниже показана схема кодирования и декодирования Chameleon:
> 
> 【26†embed_image】 *Архитектура Meta Chameleon (CM3leon) с единым токен-пространством для текста и изображений. Модель применяет раннее объединение модальностей: пиксели изображений кодируются как последовательность дискретных токенов, объединяемых с токенами слов. Единый Transformer обучен одновременно на текстово-визуальных последовательностях и способен генерировать смешанные выходы【3†L109-L117】【3†L117-L125】. (Источник: Meta AI)*
> 
> Chameleon пока находится в превью (Meta публиковала исследование【3†L86-L94】, но модель не выложена открыто). Предполагается, что со временем Meta откроет весы или выпустит версию под лицензией для исследователей【33†L163-L172】. Интересно, что Chameleon сопоставляют с Google Gemini по подходу: отличие в том, что у Gemini отдельный блок декодирования картинок при генерации, а Chameleon – полностью end-to-end【3†L115-L122】【3†L117-L125】. Таких различий в нюансах архитектуры сейчас много, и они – поле активных исследований, так как **ранее слияние** требует новых методов обучения (решение проблем, когда одна модальность доминирует или “топит” другую, и т.п.)【33†L169-L177】.
> 
> **Другие модели и направления:** Помимо “большой тройки” (OpenAI, Google, Meta), над мультимодальными AI работают и другие:
> - **Microsoft** разработала в 2023 модель **Kosmos-1**, которая позиционируется как мультимодальный наученный на веб-данных (умеет решать визуальные задачи, читать текста, и даже разбирать оптические тесты). Продолжения Kosmos, возможно, интегрируются в Bing Chat.
> - **Alibaba, Baidu (ERNIE-ViLG)** и другие китайские корпорации тоже анонсировали мультимодальные большие модели, обученные на своих языках и данных. В частности, Baidu ERNIE 4.0 заявлен как мультимодальный (текст+изобр+аудио).
> - **Anthropic** (создатели Claude) пока фокусировались на тексте, но по слухам также ведут R&D насчет визуального понимания.
> - **Модели для видео.** Помимо OpenAI Sora, известны исследования от Google – **Imagen Video** и **Phenaki** (текст-видео генерирование), от Meta – **Make-A-Video**, от стартапа Runway – **Gen-2** (модель генерации видео по описанию). Эти модели пока узко специализированы (только создают видео), но постепенно могут встраиваться как “модальность” в более общие системы.
> - **Voice multimodality:** Например, Meta представила **Voicebox** (2023) – универсальную модель обработки речи, способную порождать аудио в разных стилях, диалектах, делать условный TTS. Хотя Voicebox – аудио-модель, ее можно рассматривать как компонент мультимодального AI-помощника, который понимает текст, видит образ и **звучит реалистично** на выходе.
> 
> **Интерфейсные реализации.** Все эти модели – мощный бэкэнд, но важно, как пользователь с ними взаимодействует. Несколько примеров:
> - **ChatGPT с голосом и изображениями:** В 2023 ChatGPT получил обновление – возможность **разговаривать голосом** и **анализировать картинки**. За речью стоит модель (вероятно) Whisper для ASR и TTS от партнеров, а за видением – GPT-4V. Фактически, это прообраз мультимодального ассистента: пользователь может показать фото и спросить вслух, ИИ ответит голосом, ссылаясь на детали фото. Это качественно новый UX диалоговой системы.
> - **Google Lens + Bard:** Google объединил свои технологии: если раньше Lens обрабатывал изображение и выдавал результаты, то теперь в Bard (чатботе) вы можете отправить фото. Bard, опираясь на мультимодель (Gemini), **опишет или ответит на вопросы по изображению**. Это интеграция визуального поиска в общение.
> - **Meta Demo (Example):** Meta на конференции Connect могла показывать демо очков Ray-Ban с ИИ-помощником: вы смотрите на вывеску на иностранном языке, говорите «что здесь написано?» – камера считывает текст, модель переводит и асистент голосом переводит вам. Тут связаны модальности: взгляд/камера + OCR + LLM + голос.
> - **Роботы с LLM-мозгом:** Пример – робот-аватар Tesla Bot или Nvidia’s Toy Jensen, у которых внутри языковая модель, а снаружи – датчики. Они слышат голос (микрофоны) + видят (камеры) + осязают (сенсоры), а отвечают речью и действиями. В тестовых видео от Tesla видно, как гуманоид воспринимает голосовую команду, использует компьютерное зрение, чтобы понять окружение и выполнить задание. Все это – применение мультимодели (возможно, оптимизированной версии GPT) для управления реальным робототехническим устройством.
> 
> Таблица ниже кратко сравнивает некоторые особенности упомянутых мультимодальных моделей:
> 
> | **Модель** | **Разработчик** | **Модальности** | **Особенности** |
> |--------------------|-----------------|--------------------------------------|---------------------------------------------------------|
> | **GPT-4** (2023) | OpenAI | Текст, Изображения (вход) | LLM с оптическим модулем; используется в ChatGPT Vision. |
> | **Sora** (2024) | OpenAI | Текст, Изобр. (вход); Видео (выход) | Генерация видео по описанию; интегрирована в ChatGPT【36†L165-L173】【36†L177-L185】. |
> | **Gemini** (2023–25)| Google (DeepMind)| Текст, Код, Аудио, Изобр., Видео (вход); Изобр.+Текст (выход)| Единое токен-пространство, несколько масштабов (Ultra/Pro/Nano); state-of-the-art результаты【9†L402-L410】【9†L422-L431】. |
> | **Chameleon** (CM3leon, 2023) | Meta AI | Текст, Изобр. (вход/выход) | Раннее слияние, генерирует смешанные медиа; пока в предрелизе【3†L109-L117】【33†L159-L167】. |
> | **Kosmos-1** (2023)| Microsoft | Текст, Изобр. (вход) | Мультимодальный трансформер для восприятия, мало деталей. |
> | **Make-A-Video** (2022) | Meta AI | Текст (вход); Видео (выход) | Генеративная модель видео, демо-версия. |
> | **Imagen Video** (2022) | Google | Текст (вход); Видео (выход) | Ведомственное исследование Google по генерации видео. |
> | **Voicebox** (2023)| Meta AI | Аудио/Речь (вход/выход) | Генерирует речь в различных стилях, применима для TTS. |
> 
> *Примечание:* Многие модели из таблицы недоступны публично (только статьи или бета-доступ). Однако их возможности демонстрируют направление: универсальные ИИ, понимающие сразу **весь спектр чувственного ввода** (слух, зрение и пр.) и генерирующие различные типы контента. В дальнейшем ожидается появление еще более мульти-modal систем – например, объединяющих речь, зрение и касание для робототехники, или анализирующих не только пиксели камер, но и показания других датчиков (радара, лидаров – для автопилотов, медицинских приборов – для врачебных AI и т.д.).
> 
> ## 5. Реализованные системы с комбинированием ЭМГ и других модальностей
> 
> Особого внимания заслуживают решения, где **электромиография сочетается с видео, речью или другими каналами**. Такие системы трудно построить, но они показывают новые способы взаимодействия. Рассмотрим несколько конкретных примеров:
> 
> - **AlterEgo (MIT):** Уже описанный носимый аппарат, который читает ЭМГ сигнал с лицевых мышц (и мышц челюсти) во время внутренней речи. Инженерно AlterEgo объединяет: *(a)* ЭМГ-гарнитуру на лице, *(б)* наушники костной проводимости (для ответа), *(в)* модуль распознавания и генерации речи на основе ИИ. Хотя видео не используется, этот проект стоит упомянуть как комбинацию **нетрадиционной модальности (мышцы) и традиционной (языковой ИИ)**. Человек, надев гарнитуру, может мысленно сказать “ответь на звонок” – сигналы мышц передаются модели, та распознает команду и выполняет ее, а ответ (например, контекст разговора) возвращается пользователю прямо в внутреннее ухо【32†L86-L94】【32†L88-L96】. Это демонстрирует бесшовный интерфейс, как бы вплетенный во внутренний голос человека.
> 
> - **Мультимодальный протез руки:** Проект Erdogmus et al. (2024) – система управления протезом кисти, угадывающая намерение хвата. Здесь комбинируются **3 источника**: видео от глазковой камеры (то, что видит пользователь), трекинг взгляда (на что именно смотрит) и ЭМГ с мышц руки【14†L13-L21】. Когда пользователь тянется схватить предмет, алгоритм анализирует: изображение предмета (форма – стакан или ручка?), точку фиксации взгляда (куда целится рука), и паттерн активации мышц предплечья. На базе нейросети и баесовского фьюжна система классифицирует тип хвата (из 14 возможных жестов руки) за доли секунды до фактического схватывания【14†L19-L27】【14†L43-L52】. Протез, получив классификацию, своевременно принимает нужную форму (например, щепоть vs обхват). Эксперименты показали, что **совместное использование камер+глаз+ЭМГ дает лучшую точность и быстродействие**, чем каждый сигнал порознь【14†L19-L27】【14†L43-L52】. Это реальный пример кибернетической системы «человек-машина»: электронный протез фактически подключен к нервно-мышечным импульсам, дополненным зрительным анализом сцены.
> 
> - **Silent Speech Interface с ЭМГ + видео:** В области распознавания беззвучной речи (например, для людей, которые не могут говорить громко или вообще) исследуются комбинации сигналов от речевых органов. В работе Freitas et al. описана система с *четырьмя модальностями*: видео лица, глубина (с 3D-камеры Kinect) для отслеживания движений рта и языка, поверхностная ЭМГ с нескольких точек на лице/шее, и ультразвуковой доплеровский сигнал, отражающий движения внутри рта【15†L168-L177】【15†L173-L182】. Все сенсоры синхронизированы аппаратно – центральным устройством выбран EMG-рекордер, который посылает выравнивающий импульс остальным потокам【15†L159-L168】【15†L162-L170】. Записана база данных параллельных сигналов при произнесении фраз. Такая много-модальная система показала, что каждый канал захватывает разные аспекты речи: видео/глубина – движение губ и челюсти, ЭМГ – мышечные сигналы от языка и гортани, доплер – микродвижения внутри полости рта. В совокупности это позволило распознавать некоторые слова, произнесенные без звука, значительно надежнее, чем по одному какому-либо каналу【15†L135-L143】【15†L174-L182】. Практически такие интерфейсы могут помочь людям с поврежденными голосовыми связками “говорить”, используя остаточные движения.
> 
> - **AR/VR жестовое управление с EMG:** Упомянутая ранее технология Meta (CTRL-kit) пока не доступна пользователям, но есть похожие продукты. Несколько лет назад был коммерческий **Myo Armband** (Thalmic Labs) – браслет с 8 EMG-датчиками и гироскопами, распознававший жесты руки. Его энтузиасты интегрировали в игры и презентации: напр. поворот кулака = листать слайды, разжатие ладони = пауза видео. Когда добавляем VR/AR-очкам знание о позе руки из EMG + знание о положении контроллера в пространстве (IMU), получаем четкую систему жестового ввода. Хотя Myo снят с производства, разработки в этом направлении продолжаются в открытом виде (проекты **OpenEMG**, **Gesture Decoders** в сообществе).
> 
> - **Мультимодальные ассистенты для инвалидов:** Интересные реализации возникают в Assistive Technology. Например, для людей с синдромом “замкнутого в себе” (парализованных) делали системы с **ЭЭГ + управление глазами**: пользователь выбирает буквы взгляном на экран, а ЭЭГ подтверждает выбор (моргание мысленное). Или **EMG + синтез речи**: у человека с парализованными связками могут оставаться слабые сигналы от мышц речи – их улавливают электроды на шее и щеках, модель декодирует в текст, а затем TTS озвучивает. Такие устройства пока в опытных образцах (например, прототипы в исследованиях MS, ALS【32†L96-L100】), но они комбинируют несколько технологий сразу: биосигналы, компьютерное зрение (если нужно отслеживать глаза) и речь.
> 
> - **Спортивные и игровые устройства:** Есть примеры, где **EMG + движение** используются для захвата активности. Компания **Valve** экспериментировала с BCI для VR: измеряла ЭЭГ и ЭМГ игроков, чтобы адаптировать игровой процесс. Некоторые VR-контроллеры (Valve Index) уже умеют отслеживать давление пальцев (косвенно это “EMG low-fi”, т.е. движение мышц). Если добавить реальную EMG-накладку на предплечье, можно получить еще более тонкое управление аватаром – скажем, в VR-игре с боевыми искусствами EMG различит, напрягаете вы бицепс или трицепс, и определит тип удара.
> 
> В целом, **системы, объединяющие EMG с другими сигналами, показывают огромный потенциал**: от восстановления речи и движений до совершенно новых видов ввода. Но они и наиболее сложны – требуют калибровки под каждого пользователя, чувствительны к помехам (электрическим и визуальным), и порой инвазивны. Тем не менее, прогресс налицо: за последние ~5 лет появились рабочие демонстрации, где человек управляет устройством практически “силой мысли/намерения”, подкрепленной несколькими датчиками. 
> 
> ## 6. Актуальные направления, ограничения и открытые вопросы
> 
> **Текущие вызовы.** Несмотря на впечатляющие достижения, мультимодальные интерфейсы далеки от совершенства. Некоторые ключевые ограничения и проблемы на 2025 год:
> 
> - **Синхронизация и выравнивание данных:** Одной из главных трудностей остается *выравнивание по времени*. Разные модальности имеют различную природу: аудио и EMG – быстрые временные ряды, видео – последовательность кадров (30 FPS), текст – дискретный поток слов. Совместить их так, чтобы модель поняла, какие события совпадают, непросто【22†L406-L414】. Решения типа трансформеров с вниманием частично адресуют это, но требуют много данных. Ошибки синхронизации приводят к тому, что модальности “противоречат” друг другу вместо дополнения. Инженеры стремятся либо аппаратно синхронизировать сбор (пример с триггером от EMG)【15†L159-L168】, либо использовать алгоритмы динамического совмещения признаков. Это активная область исследований, ведь для реального времени нужно еще учитывать **задержки** сенсоров и сетей.
> 
> - **Сложность и ресурсоемкость моделей:** Мультимодальные ИИ – гораздо более тяжелые системы, чем модальные. Они содержат больше параметров (нужно “память” под визуальные и аудио признаки одновременно и их взаимодействие) и требуют смешанных датасетов. Обучение Gemini, например, потребовало TPUv5 суперкомпьютера и 5 млн часов GPU【33†L135-L143】. Не у всех игроков есть такие ресурсы. Даже open-source модели типа LLaVA – весомые и сложные в дообучении. Кроме того, сложно *отлаживать* такие модели – непонятно, из-за какой модальности ошибка. Решение видится в **модульности**: некоторые предлагают обучать отдельные эксперты по модальностям, затем объединять (так проще обновлять один модуль при улучшении, не переобучая все)【22†L426-L434】【22†L436-L439】. Другие – в развитии более эффективных архитектур (например, использовать сжатые представления, капсульные сети, чтобы не взрывалась размерность).
> 
> - **Нехватка данных и проблема разноформатных данных:** Качественный мультимодальный датасет собрать сложно. В результате многие модели либо обучены на синтетических данных (например, генерированных паре описание-картинка), либо на несбалансированных (например, миллиард изображений с подписями, но только тысячи аудио-фрагментов с метками эмоций). **Дисбаланс** данных приводит к перекосу: модель может научиться хорошо работать с одной модальностью, а игнорировать другую. Учитывая, что “слабых” датасетов (типа физиология+что-то) мало, открытый вопрос – как **переносить знания** между модальностями. Одно из направлений – *кросс-модальный self-supervised learning*, когда модель учится предсказывать одну модальность по другой (например, звук по видео) и так вычленяет общие черты. Также **обобщение**: модель, обученная на одном наборе сенсоров, должна бы работать с другими (например, подменили камеру – все развалилось?). Пока такие изменения требуют тонкой настройки.
> 
> - **Надежность и помехоустойчивость:** Биосигналы очень шумные и вариативные. Даже у одного человека ЭЭГ или ЭМГ меняется день ото дня, зависит от положения электродов, от усталости, и т.п. В мультимодальном наборе всегда есть риск, что какой-то канал даст мусор (камера заслонена, микрофон с шумом, электроды отошли). Поэтому системы должны быть **устойчивыми к пропаданию модальности** – уметь работать, если остались только другие каналы. Современные архитектуры часто не делают этого явно – если обучена на слиянии, то потеря одного входа ломает работу. Требуются специальные методы, например, обучать “дропая” случайно некоторые модальности, чтобы модель училась справляться без них (аналоги модального dropout). 
> 
> - **Интерпретация решений и доверие:** Еще один аспект – сложность понимания, *почему* система приняла то или иное решение, учитывая множество входов. Это важно для областей как медицина или автомобильное управление: мультимодальный ИИ может, скажем, оценить, что водитель засыпает, но на основании чего – не ясно (по глазам или по ЭЭГ?). Неинтерпретируемость затрудняет отладку и доверие пользователей. Необходимо развитие методов XAI (Explainable AI) применительно к мультимоделям – например, тепловых карт одновременно на видео и на графиках сигнала, чтобы показать, что повлияло.
> 
> - **Этические и приватные вопросы:** Мультимодальные ассистенты, которые *всегда слушают и смотрят и даже читают биосигналы*, вызывают понятные опасения. Обработка таких данных должна строго контролироваться: утечка видео из дома или ЭЭГ пользователя – серьезная проблема. Пока компании аккуратно подходят – например, OpenAI не запускает Sora (видео) в ЕС сразу【36†L187-L194】, т.к. законы требуют оценить риски. Нужно разрабатывать и стандарты хранения/шифрования для таких комплексных личных данных. Этика также касается **вторжений в свободу**: если ИИ научится определять эмоции, не станет ли он манипулировать (например, видя, что пользователь расстроен, предлагать ненужные ему покупки для утешения)? Вопросы открыты и требуют участия не только инженеров, но и общества в целом.
> 
> **Перспективы и будущие исследования.** Невзирая на сложности, направление мультимодального взаимодействия стремительно движется вперед. Некоторые перспективные ветви:
> 
> - **Контекстно-адаптивные ассистенты:** ИИ, который действительно *понимает контекст*. Например, офисный помощник, видя что вы хмуритесь перед экраном, сам предложит помощь (“вижу, вы затрудняетесь, давайте уточню задачу”). Или учебный ИИ, заметив по EEG рост вашей усталости, переключит формат обучения на более интерактивный. Для этого нужны надежные распознаватели когнитивных и эмоциональных состояний – а это сочетание множества сигналов (мимика, голос, пульс, мозг). Уже закладываются основы: исследования, где по комбинации физиологических сигналов классифицируют ментальную нагрузку или эмоции, показывают неплохие результаты【24†L53-L61】【24†L149-L157】.
> 
> - **Интеграция в AR/VR:** Будущие гарнитуры (AR-очки, VR-шлемы) рассматриваются как основной хаб мультимодальных интерфейсов. Они уже имеют камеры, микрофоны, IMU; планируется добавлять отслеживание глаз и мимику (для аватаров). Возможно появление *сенсорных обручей или браслетов* в комплекте – для чтения ЭЭГ/ЭМГ (OpenBCI Galea как предвестник). Это позволит, находясь в виртуальной среде, использовать максимально естественный интерфейс: говорить, делать мимику, использовать руки, и даже посылать сигналы усилием мысли. Такие системы откроют “прямой канал” к ощущениям пользователя – например, игра сможет динамически подстраиваться, делая опыт более захватывающим.
> 
> - **Малозаметные интерфейсы (ambient interfaces):** Мультимодальность позволяет интерфейсам уйти в фон, стать менее навязчивыми. Не нужно будет кричать “Окей, Google” – достаточно взгляда и нейтральной фразы, помощник сам поймет, что обращаются к нему, учитывая сочетание голос+контекст (взгляд на колонку). Или управление устройствами без рук – комбинация голосовой команды с жестом, опять же, более бесшовно впишется в нашу жизнь, чем доставание телефона. Исследования ambient computing рассматривают, как множество сенсоров в окружении (умный дом: камеры, микрофоны, датчики на теле) совместно могут понимать потребности человека и помогать без явных команд.
> 
> - **Расширение чувственного диапазона ИИ:** На горизонте – добавление новых модальностей. Например, **обоняние** (электронные носы) и **вкус**: хотя сложно представить, но уже есть датчики летучих веществ, ИИ можно научить “чувствовать” химические сигналы (применимо в медицине, безопасности). Такую информацию тоже придется комбинировать с остальными. Или осязание: ИИ-робот, трогая предмет, получает сенсоры давления и текстуры – эти данные нужно объединять с зрением и звуком окружения. Универсальные модели, вероятно, будут расширяться, чтобы включать и эти типы. Например, Meta представила модель **ImageBind**, которая связывает в общем эмбеддинге 6 модальностей: изображение, аудио, текст, данные ИМУ, глубину и тепловизор – фактически закладывая фундамент для будущего, где любая новая модальность может быть “привязана” к уже изученным через общее представление.
> 
> - **Обучение с учителем – сам человек:** Интересный исследовательский вопрос – можно ли эффективно подстраивать мультимодальные интерфейсы под конкретного пользователя через обратную связь. Например, система выводит подсказку, видит по вашим биосигналам негатив (раздражение) – корректирует модель диалога, “обучаясь” вашим реакциям. Это своего рода замкнутое обучение reinforcement learning, где награда – физиологическая/эмоциональная реакция. Пока это гипотетично, но при наличии надежного онлайн-распознавания эмоций вполне реализуемо. Тем самым ИИ-ассистент станет более **персонализированным и эмпатичным**.
> 
> **Открытые вопросы:** В завершение отметим несколько вопросов, на которые пока нет четкого ответа:
> - *Сколько модальностей достаточно?* – Всегда ли больше = лучше? Возможно, есть diminishing returns: добавление, скажем, ЭЭГ к уже имеющимся видео+аудио дает +1% пользы ценой огромных усложнений. Надо понимать, какие комбинации реально усиливают друг друга (например, взгляд+жесты+речь – да, а вот одновременно пульс и ЭЭГ для понимания эмоции – может быть избыточно, или одно можно выводить из другого).
> - *Унификация vs специализированные системы:* Большие универсальные модели (типа Gemini) впечатляют охватом, но могут уступать узким, заточенным под задачу (скажем, специальная модель для медицинской диагностики по мульти-сигналам ЭЭГ+МРТ). Где найти баланс между общим интеллектом и специализированным? Возможно, будущее – **гибридные системы**, где крупная LLM координирует несколько экспертных мультимодальных моделей (такой подход, кстати, использован в проектах как HuggingGPT).
> - *Человеко-машинное обучение:* Как эффективнее обучать ИИ на примерах, задаваемых пользователем через разные модальности? Например, я показываю роботу как действовать, одновременно комментируя словами – как синхронизировать эти способы обучения (imitation learning + language instruction)? Пока такие области обучения (imitation vs instruction) идут разными путями.
> - *Генерация мультимодального контента:* Мы уже умеем генерировать текст, изображения, аудио, видео. Но как насчет **синхронной генерации**? Например, создать виртуального персонажа, который одновременно говорит (с синхронизированным движением губ), жестикулирует руками и выражает эмоции лицом, – и все это под управлением одной модели. Сейчас обычно делают раздельно: модель генерирует речь, другая анимирует лицо по аудио, третья – жесты. Интересно, придем ли мы к модели, которая сама “проигрывает сценку” целиком, контролируя все модальности выхода. Это потребует новых архитектур декодеров и огромных данных (записи людей на видео+аудио+motion capture).
> - *Удобство и инвазивность:* Много модальностей = много датчиков. Будем ли мы готовы носить кучу приборов? Вероятно, все придет к миниатюризации и слиянию: очки+браслет смогут собрать почти все нужное. Но пока, например, ЭЭГ требует либо геля и десятка электродов, либо сухих контактов, что не супер удобно. Возможно, будут появляться **альтернативные способы извлечения тех же сигналов**: напр., считать пульс камерой по микроколебаниям кожи (такие алгоритмы уже есть), определять эмоции без контакта (по тепловизору или динамике речи), а то и распознавать мозговую активность по фМРТ (совсем фантастика в бытовом плане). Инженерам предстоит сделать мультимодальные сенсоры *комфортными и незаметными*.
> 
> Подводя итог: мультимодальные интерфейсы стремятся охватить **всю полноту человеческого выражения** – слова, жесты, эмоции, физиологию – чтобы ИИ лучше понимал человека. Современные научные работы заложили для этого прочный фундамент: появились архитектуры, способные объединять данные, и примеры, доказывающие, что от этого есть выигрыш (робот лучше распознал жест, ассистент учел ваше настроение). Коммерческие усилия приближают эти идеи к реальности – возможно, уже в ближайшие годы мы получим первых потребительских ассистентов, которые *видят и слышат нас одновременно*. Преодоление текущих ограничений – задача междисциплинарная: тут и улучшение алгоритмов слияния (чтобы модальности дополняли друг друга, как кусочки паззла【14†L23-L30】), и новые сенсоры, и вопросы социальной приемлемости. Но направление ясно: взаимодействие человека и машины станет более **естественным, интуитивным и богатым**, чем когда-либо, благодаря использованию мультимодальных технологий.
> 
> **Источники:** Использованы материалы исследований, обзоров и новостных публикаций, включая: обзор мультимодальных интерфейсов в HCI【20†L527-L536】【20†L540-L549】, работу по слиянию EMG+зрения для протезов【14†L13-L21】【14†L19-L27】, описание проекта AlterEgo (MIT)【32†L86-L94】【32†L100-L108】, блог Meta о нейроинтерфейсе на запястье【28†L290-L299】【28†L295-L303】, анонсы моделей Meta Chameleon【3†L109-L117】【3†L117-L125】, Google Gemini【9†L383-L392】【9†L422-L431】, OpenAI Sora【36†L165-L173】【36†L177-L185】 и др. Полный список источников приведен по тексту в формате цитат.
<!-- UID: e73d2552-63af-4e4a-a73a-a460b1815616 -->