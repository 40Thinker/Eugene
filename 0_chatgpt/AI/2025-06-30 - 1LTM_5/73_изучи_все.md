>[!nexus_user] **User** - 01.07.2025 at 17:14
> изучи все
<!-- UID: 03f0bf4e-ac53-4b21-b861-aad0b496d1b8 -->




>[!nexus_agent] **Assistant** - 01.07.2025 at 17:15
> Отлично. Я изучу все 10 направлений, связанных с механиками ручного селектора в LLM (например, Qwen) — от пользовательского выбора токенов и beam search с вмешательством до интеграции с gaze tracking, клавиатурой и обучением через LoRA или RL. Также поищу конкретные реализации, интерфейсы, библиотеки и проекты на GitHub, подходящие для локального или гибридного применения.
> 
> Как только соберу информацию, я дам тебе подробный отчёт с примерами и выводами.
<!-- UID: 5378b5d5-f406-4625-9332-09aee96e3248 -->

---

>[!nexus_agent] **Assistant** - 01.07.2025 at 17:24
> # Человеческий выбор токенов в LLM: 10 направлений исследований
> 
> ## 1. Интерфейсы LLM с ручным выбором следующего токена (Top-k)
> Существуют прототипы интерфейсов, позволяющие пользователю **интерактивно выбирать следующий токен** из списка наиболее вероятных вариантов (top-k). Например, энтузиасты реализовали скрипты на Python, где модель выводит вероятнейшие токены, а человек выбирает один из них вручную【6†L134-L142】. Открытая библиотека *Backtrack Sampler* демонстрирует такой подход: она интегрируется с **llama.cpp** и предоставляет стратегию *HumanGuidance*, при которой пользователь указывает, какой токен использовать, и генерация продолжается с учётом этого выбора【23†L391-L399】. В сообществе LocalLLaMA было показано, что даже небольшая модель LLaMA-3B способна решить сложную задачу, если на ключевых шагах человек выбирает правильный токен из top-3 вариантов (даже если у него низкая вероятность)【1†L210-L218】【23†L343-L351】. Другой пример – проект **LLooM** (The LLooM) – экспериментальный интерфейс, где **пользователь выступает в роли семплера LLM**. LLooM работает подобно *“выбери своё приключение”*, визуализируя логиты модели и позволяя человеку выбирать ветвления повествования на уровне слов【10†L250-L258】. Автор LLooM прямо описывает идею как *«что если использовать человека в роли выборщика токенов для LLM»* – это и легло в основу данного проекта【10†L250-L258】. Таким образом, хотя у популярных готовых чат-ботов такой возможности нет, энтузиасты уже создали **ручные селекторы токенов** для локальных LLM-моделей.
> 
> ## 2. Выбор между несколькими сгенерированными словами/фразами (пользовательский Beam Search)
> Альтернативный подход – предоставлять не один токен, а **несколько вариантов продолжения** (слова или целые фразы), из которых пользователь выбирает понравившийся. Это сродни beam search с вмешательством человека: модель генерирует несколько кандидатов, а человек решает, какой из них использовать для дальнейшей генерации. В сообществе разработаны демо, где **LLM генерирует несколько вариантов следующего шага текста одновременно**. Например, упомянутый выше LLooM реализует интерактивный **поиск по лучам**: он строит дерево продолжений фразы, визуализирует различные ветви, и пользователь кликом выбирает, по какому лучу (beam) продолжить текст【3†L583-L591】. В более поздних версиях LLooM добавили функцию *“Auto-depth”*, позволяющую автоматизированно углублять ветвления до определённого условия, что удобно для оценки творческого потенциала модели【12†L284-L292】. Подобные механики напоминают игры или системы генерации историй, где AI предлагает несколько продолжений, а пользователь направляет сюжет. Сообщается, что даже коммерческие сервисы для повествования вроде **NovelAI** реализуют нечто похожее – предоставляют несколько альтернативных продолжений текста, среди которых пользователь может выбрать подходящее【23†L343-L351】. Таким образом, **beam search с участием человека** – перспективная техника: человек эффективно выполняет роль “оценщика”, выбирая лучший из сгенерированных AI вариантов на каждом шаге.
> 
> ## 3. Библиотеки и UI-фреймворки с поддержкой ручной вставки токена
> Несмотря на интерес к подобным возможностям, **популярные UI для локальных LLM пока не имеют из коробки функции пошагового выбора токена**. Например, известный веб-интерфейс [Oobabooga Text Generation WebUI](https://github.com/oobabooga/text-generation-webui) (на основе Gradio) предоставляет множество настроек семплирования, но не даёт пользователю вручную подтверждать каждый токен – вывод генерируется автоматически по заданным параметрам【19†L1-L9】. Тем не менее, технически реализовать ручной селектор можно поверх таких фреймворков. Разработчики обсуждают добавление фронтенда для **пошаговой генерации**: к примеру, в ветке обсуждения Backtrack Sampler автор отметил, что для широкого использования нужна удобная GUI/веб-интерфейс, возможно отдельное приложение с WebSocket API【23†L445-L453】【23†L459-L467】. На практике это можно сделать с помощью Gradio или Streamlit: написать цикл генерации, который после каждого токена выводит пользователю top-k варианты и ждёт его выбора. *Прямого готового решения* пока нет, но есть смежные инструменты. Так, **KoboldAI** и **SillyTavern** – популярные локальные приложения для AI-повествования – позволяют останавливать генерацию и вручную редактировать или дописывать текст, прежде чем продолжить. Это не совсем выбор из списка токенов, но даёт пользователю контроль над выводом. В целом, реализация интерфейса с ручной вставкой токена требует кастомной доработки существующих UI. Однако сообщество активно над этим работает: энтузиасты готовы создать такой фронтенд на базе существующих библиотек【23†L459-L467】, используя гибкость open-source стеков (Gradio, etc.). Итого: **на сегодня ручной селектор токенов реализован в виде отдельных скриптов и экспериментальных UI (LLooM, BacktrackSampler)**, а интеграция в массовые UI пока в планах.
> 
> ## 4. Внедрение пользовательского выбора в алгоритм beam search
> В классическом beam search модель генерирует несколько наиболее правдоподобных последовательностей (beam’ов) параллельно и в конце выбирает лучшую. **С участием человека beam search можно модифицировать так, что выбор лучшей ветви делает пользователь** на основе смысла или желаемого результата. Процесс может выглядеть так: модель порождает *N* вариантов продолжения текущей фразы; эти варианты показываются человеку; человек выбирает один – соответствующий beam помечается как “продолжаемый”, а остальные отбрасываются; затем модель снова генерирует *N* продолжений уже от выбранного состояния, и цикл повторяется. Такой подход совмещает силу поиска (модель предлагает разнообразие вариантов) с интуицией человека (он отбирает осмысленный или целевый вариант). В результате получается **интерактивный beam search**, где **человек выполняет роль эвристики оценки лучей**. В практике разработок этот принцип уже проявляется. Например, упомянутый LLooM фактически реализует дерево возможных продолжений и позволяет человеку “ходить” по этому дереву, делая выбор на узловых точках【10†L250-L258】. Аналогично, автор Backtrack Sampler отметил, что если дополнить его скрипт генерации поддержкой одновременного формирования нескольких токенов (beam), то получится CLI-версия LLooM【3†L583-L591】. То есть идея понятна: **на каждом шаге не один токен, а несколько фрагментов текста**, и человек решает, какой оставить. Реализовать это можно в рамках генерации с помощью stop-контролей: после генерации каждого “слоя” beam search останавливать процесс и ждать выбора пользователя, затем продолжать только по выбранному пути. Таким образом, beam search из чисто алгоритмической процедуры превращается в *гибрид человека и машины*, что может повысить осмысленность результатов (особенно для творческих задач, сценариев и др.). В научной литературе этот подход перекликается с идеями *human-in-the-loop decoding* – человек как часть декодера LLM.
> 
> ## 5. Поддержка Qwen и других моделей в фреймворках (vLLM, LMDeploy, llama.cpp) для вставки пользовательских токенов
> Модель **Qwen** от Alibaba упоминается как пример современной LLM. Рассмотрим, позволяют ли популярные системы её запуска осуществлять интерактивный выбор токенов:
> - **vLLM** – производительный серверный движок для генерации (от компании Databricks). Qwen официально рекомендует vLLM для деплоя своих моделей【24†L9-L12】, и vLLM обеспечивает потоковую выдачу токенов в реальном времени. Хотя в стандартном API vLLM нет специальной функции “вставить токен по выбору пользователя”, реализовать это можно на прикладном уровне: останавливать генерацию после 1 токена (или запрашивать `max_tokens=1`), показывать его пользователю, при необходимости заменять его другим токеном и продолжать запрос. vLLM ориентирован на throughput, но технически **“ручной режим”** возможен, если вы управляете генерацией пошагово через его Python API.
> - **LMDeploy** – открытый фреймворк от Alibaba для развертывания LLM (на основе TurboMind). Он поддерживает Qwen и другие модели, обеспечивая высокую скорость вывода. Однако LMDeploy, будучи сервером, тоже не имеет встроенного интерактивного UI. Тем не менее, можно отправлять последовательные запросы: каждый раз с новым префиксом, включающим ручной выбор пользователя. Иными словами, **пользовательский выбор можно внедрить, манипулируя последовательностью запросов**, хотя автоматизированной поддержки нет.
> - **llama.cpp** – популярная CPP-библиотека для локального запуска LLaMA-подобных моделей. Сообщество уже расширило её для поддержки Qwen (включая версии Qwen-2 VL) – добавлены необходимые изменения токенизации и слоёв, есть конвертированные веса Qwen в формат GGUF【46†L241-L249】. Это значит, что Qwen-модель можно загрузить в llama.cpp и использовать через Python-обёртки (например, *llama-cpp-python*). Последнее как раз продемонстрировано в примере Backtrack Sampler: там LLaMA-3B загружена через *llama_cpp*, и вручную управляется генерация токенов【23†L399-L407】. Функции llama.cpp позволяют получить логиты следующего токена и даже задать **принудительный токен** (через механизм grammar или **logit bias**). Таким образом, llama.cpp по сути поддерживает вмешательство: можно программно заменить сгенерированный токен на выбранный пользователем и продолжить работу модели с обновленным контекстом. 
> - **Другие системы**: Аналогичные возможности присутствуют во многих библиотеках. Например, HuggingFace Transformers предоставляет метод `generate` с параметром `stopping_criteria` – через него можно останавливать генерацию на каждом токене и реализовать интерактив. Фреймворки типа **LMDeploy** также могут быть модифицированы: поскольку исходники открыты, можно встроить колбэк после шага декодера.
>  
> Важно, что **поддержка самой модели Qwen** в этих фреймворках уже есть (в vLLM и LMDeploy официально, в llama.cpp – сообществом), значит, вопрос лишь в программной логике для пользовательского вмешательства. Пока что это требует кастомизации, но пример *HumanGuidanceStrategy* в Backtrack Sampler показывает реализуемость: модель (в том числе Qwen) может работать в интерактивном режиме через существующие движки【23†L391-L399】.
> 
> ## 6. Управление logit bias и sampling bias в реальном времени на основе выбора пользователя
> **Logit bias** – механизм, позволяющий целенаправленно изменить вероятности определённых токенов при генерации. Многие API LLM поддерживают это: например, OpenAI API принимает параметр `logit_bias` – словарь, задающий сдвиг вероятности для указанных токенов от -100 до +100 (чем больше значение, тем вероятнее токен, отрицательные – запрещают токен)【30†L115-L123】. В контексте ручного выбора токена **logit bias может служить мягкой формой вмешательства**: вместо прямого указания токена, система может усиливать предпочтения пользователя. Например, если пользователь выбрал токен “apple” среди предложенных, можно на следующих шагах повысить bias для слов, связанных с “apple”, чтобы продолжение соответствовало выбору. Точно так же, logit bias применяется для **исключения нежелательных вариантов** – по команде пользователя можно динамически занести определённое слово в “стоп-лист” (добившись практически нулевой вероятности его появления)【30†L82-L89】【30†L115-L123】. 
> 
> В реальном времени подобные манипуляции требуют управления на уровне каждого шага генерации. В *кастомном* цикле генерации это возможно: библиотека HuggingFace Transformers предоставляет интерфейс *LogitsProcessor*, который вызывается на каждом поколении токена и может произвольно преобразовывать вектор логитов【32†L7-L15】. С его помощью можно реализовать **динамическое изменение bias во время инференса**. Например, можно написать процессор, который будет смотреть, выбрал ли пользователь токен X, и на следующем шаге добавлять положительный bias к логитам связанных токенов или, наоборот, отрицательный к альтернативам. Таким образом получается *feedback loop*: выбор человека сразу влияет на распределение вероятностей перед следующим шагом. Аналогичные техники применяются для контролируемой генерации – напр., последовательный контроль стиля: как только пользователь выбрал вариант более “официальный”, процессор может снизить вероятность сленговых слов далее. 
> 
> Стоит отметить, что некоторые UI уже частично используют статический sampling bias. В Oobabooga WebUI есть настройки *ban tokens* или *stop sequences*, которые фактически выставляют logit bias = -100 для указанных токен-идентификаторов, предотвращая их генерацию. Но именно *интерактивное* управление (по ходу сессии) пока редкость и требует кастом-кода. Тем не менее, технические средства для **ручного управления распределением токенов** есть во всех крупных фреймворках: от OpenAI (logit_bias) до Transformers (логит-процессоры). Их можно задействовать, чтобы внедрить пользовательские предпочтения в реальном времени и тем самым направлять модель без полного ручного выбора каждого токена.
> 
> ## 7. Управление генерацией с помощью взгляда или специальных устройств (gaze tracking, клавиатурный селектор)
> Помимо традиционной клавиатуры и мыши, исследуются **непрямые методы выбора слов** в LLM с участием человека – например, взглядом (eye-tracking) или сигналами мозга (EEG). Эти подходы особенно актуальны для пользователей с ограниченными возможностями и в системах дополненной реальности:
> - **Eye-tracking (управление взглядом)**: В 2024 году представлен проект **SpeakFaster** – интерфейс дополненной коммуникации для пациентов с БАС, интегрирующий крупную языковую модель【35†L85-L94】. Пользователь при помощи глазного трекера выбирает лишь некоторые буквы или сокращения, а LLM предсказывает остальной текст, значительно уменьшая число необходимых вводов. В симуляциях этот подход сократил количество действий на ~57% по сравнению с обычной экранной клавиатурой【35†L89-L97】. В пилотных тестах с двумя реальными пользователями (управляющими компьютером взглядом) скорость ввода текста выросла на 29–60% относительно базовой, благодаря тому что LLM угадывала слова, намерение которых пользователь лишь частично сообщил【35†L89-L97】. Это отличный пример синергии: **LLM ускоряет ввод текста, а взгляд используется для подтверждения или выбора предложений**. 
> - **Gaze-based token selection**: Есть эксперименты, где сам принцип выбора следующего слова осуществляется взглядом. Исторически известна система **Dasher** – интерфейс, в котором пользователь непрерывно управляет курсором (например, глазом или джойстиком) по буквам, а программа на основе статистической языковой модели предлагает наиболее вероятные продолжения слов. Опыт показал, что на Dasher опытный пользователь может печатать взглядом до ~25 слов в минуту【38†L65-L73】. Современные реализации могут применить этот подход к LLM: на экране отображаются несколько слов, модель подстраивает их вероятности, а пользователь выбирает просто задерживая взгляд на нужном варианте. В **GazeNoter (2025)**, например, предложена AR-система заметок, где при помощи взгляда и жеста пальцем пользователь выбирает один из вариантов продолжения, предложенных моделью, чтобы соответствовать своему замыслу (эта работа была представлена на CHI 2025 и демонстрирует *“gaze selection of LLM suggestions”*) – то есть **взгляд служит указателем для выбора варианта генерации**. Такая бесконтактная форма взаимодействия особенно ценна в мобильных или носимых устройствах, где руки заняты или недоступны.
> - **Клавиатурный селектор**: Более тривиальный способ – привязать варианты к клавишам. Например, если модель предложила [1] вариант, [2] вариант и [3] вариант продолжения, то нажатием цифры пользователь выбирает соответствующий. Это уже применяется в мобильных клавиатурах (suggestions bar): там три подсказки слов, и пользователь тапает одну. Для целых фраз подобное тоже реализуемо. В контексте LLM можно во время генерации отображать несколько слов и дать возможность нажатием или стрелками выбрать. Хотя открытых проектов специально под LLM нет, подобная функциональность может быть добавлена в существующие чат-интерфейсы как **“режим многовариантного ответа”**.
> - **EEG и BCI**: Управление силой мысли пока находится в зачатке для текстовых задач, но прогресс есть. Инвазивные нейроимпланты позволили парализованным пациентам мысленно вводить текст, однако скорость низка – порядка нескольких символов в минуту【35†L141-L148】. Чтобы ускорить, опять же, привлекают языковые модели: BCI распознаёт ограниченный набор сигналов (например, выбор между 2–3 вариантами), а LLM развивает мысль. В неинвазивных EEG-интерфейсах можно пытаться различать согласие/несогласие пользователя. К примеру, модель генерирует токен, а по сигналам мозга определяют, воспринимает ли это мозг как желанный вариант или нет – и на основе этого решают, принять токен или заменить. Пока это во многом экспериментальная область, требующая улучшения точности распознавания. Но комбинация **“EEG + LLM”** уже обсуждается в литературе как способ улучшить коммуникацию: человек *мысленно* выбирает среди предложений. Например, имеются датасеты с одновременной записью взглядов и EEG во время набора текста【35†L108-L116】 – они могут быть использованы для обучения моделей, учитывающих сразу оба сигнала. 
>  
> Подводя итог, **человеческий взгляд и другие биосигналы могут служить альтернативным интерфейсом для ручного выбора слов в генерации текста**. Открытые проекты (SpeakFaster) и исследования (GazeNoter) показывают эффективность такого подхода в ускорении и упрощении ввода. Вероятно, в будущем гибридные UI (взгляд + LLM) станут более распространены, расширяя доступность LLM-технологий.
> 
> ## 8. Учет пользовательских выборов при обучении (LoRA fine-tuning, RAG)
> Интерактивный выбор пользователя не только влияет на конкретную сессию генерации, но и представляет ценный сигнал о **предпочтениях**, который можно использовать для дообучения модели. Есть несколько направлений:
> - **Fine-tuning на основе выборов**: Если собрать данные о том, какие продолжения или токены выбирали люди, эту информацию можно превратить в датасет для обучения. Самый прямой способ – рассматривать человеческий выбор как правильный ответ. Например, в ходе диалога LLM может на каждый шаг генерировать несколько вариантов ответа; пользователь выбирает один – значит именно этот вариант наиболее предпочтительный. Со временем можно накопить множество пар (контекст, *preferred continuation*) и запустить fine-tuning (в том числе лёгкий, типа LoRA) модели, чтобы она сама научилась генерировать именно такие продолжения, какие обычно выбирает человек. Таким образом, модель адаптируется под **человеческий вкус**. Фактически, это близко к RLHF (обучение с подкреплением от человеческой обратной связи), но реализовано как **супервизированное обучение на уровнях токенов/фраз**, отмеченных человеком. В сообществе уже звучала идея записывать все решения пользователя в интерактивном семплере и затем тренировать на них более мелкую модель-семплер, чтобы она имитировала выбор человека【10†L275-L282】. Пока это на уровне эксперимента, но принцип ясен: данные “человек предпочёл вариант X перед Y” – это метка, по которой можно настроить веса LLM.
> - **Учёт выборов в RAG**: В Retrieval-Augmented Generation модель сперва ищет внешние документы, затем формирует ответ. Здесь тоже возможен human-in-the-loop: если из 5 найденных статей пользователь явно кликает/читает/подтверждает 1, то можно улучшить ретривер, подстроив его ранжирование под такие случаи. К примеру, включив выбор пользователя в loss при обучении ретривера (т.е. повысив вес документа, который человек выбрал). Также, при генерации ответа, если пользователь указал, какой источник надёжный, модель могла бы перенести этому факту больший вес (вплоть до цитирования именно выбранного источника). Таким образом, **человек помогает системе решать, каким знаниям доверять**, и эту помощь можно запомнить. В режиме онлайн без обучения некоторые системы уже делают подобное (например, Bing Chat может спросить “какой из этих результатов вам подходит?”). Но и офлайн обучение через LoRA тоже возможно – например, дообучить модель вопрос-ответ, чтобы она сильнее опиралась на те документы, которые раньше отбирали люди.
> - **Предпочтения как цель fine-tuning**: В более общем плане, пользовательские выборы можно рассматривать как проявление скрытых предпочтений (стиля, тона, уровня детализации). Интеграция таких сигналов в обучении может быть похожа на настройку модели под конкретного пользователя (персонализация). Если при интерактивном письме пользователь всегда выбирает более формальные фразы, можно fine-tune модель именно на формальный стиль для этого пользователя. LoRA удобно применять для таких узких задач – например, накладывать слой, который заставляет модель следовать одному из вариантов ответа, если известно, что человек его обычно предпочитает.
> - **Практические реализации**: Прямо “из коробки” решений мало, но элементы подхода видны в проектах вроде **ChatGPT Plugins + Feedback** или в экспериментах Anthropic по обучению на трансскриптах диалогов с пользователями. В open-source пока нет публичного набора *token-level human preferences*, но те же разработчики LLooM упоминали, что логируют выборы пользователей именно с целью обучения *“индивидуальных стратегий семплирования”*【10†L275-L282】. Это шаг в направлении интерактивного обучения.
> 
> В итоге, **внедрение пользовательских выборов в обучение LLM – перспективный путь повышения качества**. Он позволяет учесть человеческие ценности и интуицию на более тонком уровне, чем просто лайк/дизлайк для готового ответа. Хотя методология ещё формируется, сочетание интерактивных сессий и последующего fine-tuning (в т.ч. LoRA) может дать модели опыт “участия” человека и сделать её отклик более релевантным тому, что выбрал бы человек.
> 
> ## 9. Сценарии RL, где вознаграждение зависит от выбора человека во время генерации
> Стандартный RLHF обычно предполагает, что после генерации полного ответа человек даёт оценку, по которой модель обновляется. Однако можно представить более плотную интеграцию RL и человека: **на уровне токенов или шагов генерации**. В таких сценариях **каждый выбор токена может получать мгновенную награду от человека** – своего рода «ручное» обучающее подкрепление:
> - **Token-level RLHF**: Исследователи уже обсуждают и пробуют подходы, где вместо одного скалярного вознаграждения за всё высказывание моделируются *пословные награды*. Например, алгоритм **TOLE** (Token-Level Enforcement) вводит токен-специфические récompenses, позволяя более точно направлять генерацию по желаемому стилю【43†L52-L61】. Идея в том, чтобы каждая часть предложения могла быть отмечена как хорошая или плохая, что предотвращает ситуацию, когда модель получит запоздалое наказание лишь в конце фразы – ведь ошибка могла быть сделана раньше внутри предложения. В работе TOLE показано, что **токен-уровневое вознаграждение улучшает контролируемость текста** (например, соблюдение стиля или темпа) по сравнению с грубым оцениванием всего предложения【43†L55-L63】.
> - **Роль человека**: В токен-уровневом RL вместо автоматического критика можно поставить человека. Например, человек видит, как модель поэтапно решает задачку и **дает локальную обратную связь**: правильно ли выбран следующий шаг. Это похоже на преподавателя, который ведёт ученика, поправляя каждое действие. Такой метод упоминается в контексте *пошагового обучения с экспертом*: человек подтверждает или отклоняет каждый токен/шаг, и модель получает положительный или отрицательный reward соответственно. В идеале, модель через много эпизодов научится выбирать токены так, чтобы удовлетворять человека на каждом шагу.
> - **Примеры RL-сценариев**: Можно представить игру, где модель предлагает два варианта развития истории, и **человек выбирает понравившийся** – это бандитная формулировка RL, где награда (выбор) дисккретна и сообщается после каждого “хода” (фразы). Или диалог: модель генерирует ответ, человек реагирует мимикой или кнопкой “нравится/не нравится” на каждое предложение – это сигнал награды по ходу диалога, который можно использовать для онлайнового обучения. На практике OpenAI или DeepMind могли экспериментировать с такими режимами, хотя публично в основном описан RLHF с итоговыми оценками. В научном сообществе появляются работы, предлагающие методы перераспределения глобальной награды на токены (например, через обратное вычисление преимуществ каждого шага)【42†L19-L24】. Также есть подход **Inverse Reinforcement Learning на токеновом уровне** (Inverse-Q*), где пытаются напрямую вывести токеновую наградную функцию, максимизирующую предпочтения человека【42†L1-L8】. 
> - **Сложности**: Основная проблема – человеку тяжело давать фидбек на каждый токен в длительной последовательности. Это требует большого терпения и времени. Поэтому чаще думают о гибридных подходах: например, автоматическая модель-критик оценивает токены, но натренирована она была на человеческих суммарных оценках. Или человек вмешивается лишь в ключевые моменты (как в пункте 1, выбирает токен только когда модель сомневается). Тем не менее, концептуально **“reinforcement at the token level”** – активная тема. Она обещает сделать обучение более точечным и избежать ситуации, когда модель меняет всё поведение из-за оценки за одно слово. Если суметь получить надёжный сигнал от человека или прокси на каждом шаге, модель можно *дрессировать* гораздо эффективнее, поэтапно приближая её вывод к идеалу.
>  
> В итоге, **RL с участием человека на уровне токенов** – это естественное развитие идей RLHF. Уже есть алгоритмы, предлагающие как реализовать токен-регистрирующие награды и показывающие улучшения в контролируемой генерации【43†L55-L63】. Когда эти методы объединят с живым человеческим выбором (например, в интерактивных приложениях), получим режим обучения, где LLM учится *в момент использования*. Это приближает нас к системам, которые постоянно подстраиваются под оператора (вспоминая классические образы AI, который учится у пользователя во время работы).
> 
> ## 10. Архитектуры, где человек – часть механизма внимания или приоритизации (LLM/AGI)
> Представим более общую картину: **человек включён в цикл работы модели как компонент архитектуры**. Вместо того чтобы обучать LLM полностью автономно, мы позволяем человеку динамически управлять её *вниманием* – т.е. решать, на что модель должна обратить внимание, какие промежуточные результаты считать важными. Такие идеи появляются в контексте *AGI и сложных систем*, где требуется надежный контроль и синергия человека и ИИ:
> - **Human-in-the-loop Attention**: Человек может выступать как высокоуровневый фильтр, который указывает модели, какая часть контекста или какой шаг рассуждения важнее. Например, для длинного документа человек может подсветить несколько предложений, и модель фокусирует на них своё *attention*, игнорируя остальное. Это эквивалентно тому, как механизм внимания внутри трансформера выбирает ключевые токены, только в нашем случае **это делает человек вручную**. В некоторых исследованиях по интерактивному суммированию такое применялось: пользователю давали выделять важные предложения исходного текста, и модель генерировала краткое содержание, уделяя больше внимания выделенному (тем самым повышалась точность).
> - **Роль человека в Chain-of-Thought (CoT)**: При многошаговом решении задач LLM может генерировать цепочку рассуждений. Архитектуры вроде *Manual Correction System (MCS)* предполагают, что человек будет просматривать эти шаги и корректировать/направлять их перед тем, как модель перейдёт к следующему шагу【45†L69-L78】. Человек здесь – своего рода внешний модуль внимания, который решает, верен ли текущий логический переход. Исследование 2023 года показало, что такая *ручная корректировка суб-логики* существенно повышает способность модели решать сложные математические и логические задачи, хотя и требует некоторого человеческого участия【45†L69-L78】. Введена даже модель анализа *CAMLOP*, чтобы сбалансировать полезность и стоимость такого human-in-the-loop вмешательства, что подчёркивает серьёзность подхода (нужно решать, когда стоит привлекать внимание человека, а когда нет).
> - **Человек как менеджер внимания в AGI**: В гипотетических архитектурах AGI человека можно рассматривать как часть системы принятия решений. Например, в рамках **Iterative Amplification (IDA)** от OpenAI/Anthropic человек является одним из агентов, разбивающих задачу на подзадачи для модели – по сути, направляет, куда “внимание” AGI должно переключиться. В многоагентных системах человек может выполнять функцию арбитра: среди множества гипотез, сгенерированных разными агентами модели, человек выбирает наиболее перспективную. Это напоминает механизм приоритизации: человеческий мозг (оператор) решает, какую ветвь вычислений развивать далее. Таким образом, **человек интегрируется в архитектуру как узел принятия решений**, наравне с нейросетевыми модулями. 
> - **Практические примеры**: Уже упомянутый LLooM – упрощённый прототип такой схемы, где человек выбирает ветвление истории【10†L250-L258】. Можно сказать, **LLooM делает человека частью алгоритма генерации**, словно дополнительный слой внимания, отбирающий нужное. В более широком плане, проекты по типу *HLA (Human-Learned Attention)* пытаются обучать модели внимания на основе человеческих аннотаций, а проекты как *GopherCite (DeepMind)* зовут человека оценивать достоверность справок модели во время работы, после чего модель перераспределяет внимание на проверенные факты. В 2025 году появятся вероятно гибридные архитектуры, где внутри цикла inference могут быть точки останова для “человеческого внимания” – например, автономный агент генерирует план, просит человека утвердить его, затем продолжает. 
>  
> Подытожим: **архитектуры с человеком-в-цикле** превращают взаимодействие с LLM из схемы “ввод-вывод” в совместный процесс мышления. Человек может выступать как *контроллер внимания*, выделяя важное и отсекая лишнее, или как *приоритизатор*, выбирающий цели и проверяющий промежуточные результаты. Исследования показывают, что это улучшает качество (особенно в задачах, где у модели могут быть логические провалы)【45†L69-L78】. В перспективе такие архитектуры обеспечат большую безопасность и эффективность: **AGI, где человек – не пассажир, а штурман**, задающий правильный курс, пока сама модель выступает двигателем генерации. Это перспективная область на стыке AI и HCI, и мы видим лишь первые шаги к её реализации. 
> 
> **Источники:** Проекты и исследования, упомянутые выше, ссылаются на открытые репозитории и публикации: backtrack_sampler【23†L391-L399】, LLooM【10†L250-L258】, обсуждения в сообществе LocalLLaMA【3†L557-L565】【12†L338-L342】, статьи Nature/CHI о gaze-typing【35†L89-L97】, и работы по human-in-the-loop reasoning【45†L69-L78】 и token-level RL【43†L52-L61】 – все они доступны публично для подробного ознакомления.
<!-- UID: 818be2a9-9f2a-4ecb-94a7-d2579f9d894d -->