>[!nexus_user] **User** - 30.06.2025 at 21:20
> –ò–∑—É—á–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç—ç–∫–∏ —Å–æ—Ñ—Ç–∞ –ø–æ–¥ —É–±—É–∫—Ç—É, –∫–≤–µ–Ω –∏ –º–æ—é –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞
<!-- UID: 3bec40df-5fb4-4085-ac96-a7bc03628fb5 -->




>[!nexus_agent] **Assistant** - 30.06.2025 at 21:20
> –í–æ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–π **software-—Å—Ç–µ–∫ –ø–æ–¥ Ubuntu** –¥–ª—è –∑–∞–ø—É—Å–∫–∞ **Qwen‚Äë–º–æ–¥–µ–ª–µ–π** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–≤–æ–µ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã (NVIDIA –∏–ª–∏ AMD). –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç **–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∞–ø–ø–∞—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** –∏ **—Å–ø–æ—Å–æ–±—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è**.
> 
> ---
> 
> ## üß± 1. vLLM ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä
> 
> - **–ß—Ç–æ —ç—Ç–æ**: –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä LLM –¥–ª—è inferencing –∏ API‚Äë—Å–ª—É–∂–±—ã, —Å–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI‚Äë–ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º cite.
> - **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**:
>  - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–æ–¥–µ PagedAttention, continuous batching
>  - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ quantization (AWQ, GPTQ, INT4/8, FP8), tensor parallelism –∏ YARN –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ 128k cite
>  - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ NVIDIA + AMD ROCm (–≤ —Ç.‚ÄØ—á. AMD GPU —Å ROCm 6.3) cite
> - **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ Ubuntu 20.04/22.04**:
>  ```bash
>  sudo apt update && sudo apt install python3-pip git
>  pip install vllm
>  ```
> - **–ó–∞–ø—É—Å–∫ Qwen‚Äë–º–æ–¥–µ–ª–∏**:
>  ```bash
>  vllm serve Qwen/Qwen3-8B
>  ```
> 
> ---
> 
> ## ‚öôÔ∏è 2. LMDeploy ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Å –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –º—É–ª—å—Ç–∏-GPU
> 
> - **–ß—Ç–æ —ç—Ç–æ**: –§—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ç OpenMMLab, —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Qwen‚ÄëVL (–≤–∏–∑—É–∞–ª—å–Ω–æ+—Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏) cite.
> - **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**:
>  - pip install qwen_vl_utils / lmdeploy
>  - docker build, –∑–∞—Ç–µ–º:
>  ```bash
>  lmdeploy serve api_server Qwen/Qwen2-VL-7B-Instruct
>  ```
> - –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ Kubernetes-–æ–∫—Ä—É–∂–µ–Ω–∏–µ–º.
> 
> ---
> 
> ## üêã 3. Ollama ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å—Ç–æ—Ç–∞
> 
> - **–ß—Ç–æ —ç—Ç–æ**: CLI-—É—Ç–∏–ª–∏—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –º–æ–¥–µ–ª—è–º–∏ –±—ã—Å—Ç—Ä–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å Qwen3 –Ω–∞ Ubuntu cite.
> - **–£—Å—Ç–∞–Ω–æ–≤–∫–∞**:
>  ```bash
>  curl -fsSL https://ollama.com/install.sh | sh
>  ollama run qwen3:8b
>  ```
> - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º + –ª–æ–∫–∞–ª—å–Ω—ã–π API.
> 
> ---
> 
> ## üß† 4. llama.cpp / llama-cpp-python ‚Äî –º–∏–Ω–∏–º–∞–ª–∏–∑–º + quantization
> 
> - **–ß—Ç–æ —ç—Ç–æ**: –õ—ë–≥–∫–∏–π –¥–≤–∏–∂–æ–∫ –¥–ª—è CPU/GPU —Å quantized –º–æ–¥–µ–ª—å—é Qwen (GGUF) cite.
> - **–£—Å—Ç–∞–Ω–æ–≤–∫–∞**:
>  ```bash
>  git clone https://github.com/ggerganov/llama.cpp
>  cd llama.cpp && make
>  ./main -m qwen3-8b.gguf -p "..."
>  ```
> 
> ---
> 
> ## üîß 5. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GPU –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
> 
> - **NVIDIA GPU**:
>  - –£–±–µ–¥–∏—Å—å –≤ CUDA ‚â•11.8 –∏ –¥—Ä–∞–π–≤–µ—Ä–∞—Ö 535+ cite.
> - **AMD GPU**:
>  - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è vLLM —á–µ—Ä–µ–∑ ROCm 6.3+ cite.
> - **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å**:
>  - Tensor parallel –≤ vLLM: `--tensor-parallel-size 4` cite.
>  - Batch size, kv cache, gpu‚Äëmem‚Äëutilization ‚Äî –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ.
> 
> ---
> 
> ## üöÄ –û–±—â–∞—è —Ç–∞–±–ª–∏—Ü–∞
> 
> | –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Qwen | GPU NVIDIA/AMD | Multimodal | Quant | API-—Å–µ—Ä–≤–µ—Ä |
> |---------------|----------------|----------------|-------------|-------|------------|
> | **vLLM** | ‚úÖ –¥–∞ | ‚úÖ –¥–∞ | –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ | ‚úÖ –¥–∞ | OpenAI‚Äë—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π |
> | **LMDeploy** | ‚úÖ Qwen-VL | ‚úÖ –¥–∞ | ‚úÖ –¥–∞ | –¥–∞ | –î–∞ |
> | **Ollama** | ‚úÖ Qwen3 | ‚úÖ –¥–∞ | ‚Äî | —á–µ—Ä–µ–∑ quant | CLI + API |
> | **llama.cpp** | ‚úÖ Qwen3-gguf | üî∏ CPU/GPU | ‚Äî | ‚úÖ –¥–∞ | CLI |
> 
> ---
> 
> ## ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
> 
> 1. **–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å + –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –∏—Å–ø–æ–ª—å–∑—É–µ–º `vllm serve` —Å quant‚Äë–º–æ–¥–µ–ª—å—é Qwen, GPU ‚â• RTX 30XX –∏–ª–∏ AMD.
> 2. **–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å**: –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ Qwen‚ÄëVL ‚Äì –≤–Ω–µ–¥—Ä—è–µ–º —á–µ—Ä–µ–∑ LMDeploy —Å Docker/Kubernetes.
> 3. **–ü—Ä–æ—Å—Ç–æ—Ç–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏**: –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ –º–æ–¥–µ–ª—å Qwen3‚Äë8B ¬´–∏–∑ –∫–æ—Ä–æ–±–∫–∏¬ª.
> 4. **–õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã / CPU**: –≤—ã–±–∏—Ä–∞–µ–º llama.cpp –∏ quant‚Äë–º–æ–¥–µ–ª–∏.
> 
> ---
> 
> ## üõ†Ô∏è –ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –¥–ª—è —Ç–µ–±—è:
> 
> - –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å `docker-compose.yml` —Å vLLM –∏ quant‚ÄëQwen + nvGPU.
> - –°–ø–∏—Å–æ–∫ –∫–æ–º–∞–Ω–¥ –¥–ª—è LMDeploy‚Äë—Å–µ—Ä–≤–µ—Ä–∞ Qwen‚ÄëVL.
> - –ü—Ä–∏–º–µ—Ä Python-–∫–ª–∏–µ–Ω—Ç–∞, –¥–µ–ª–∞—é—â–µ–≥–æ OpenAI‚Äë–∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π API.
> 
> –•–æ—á–µ—à—å, –Ω–∞—á–Ω—ë–º —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤?
<!-- UID: e27dc6ff-6553-4ea0-8a5a-16f01aeb919a -->