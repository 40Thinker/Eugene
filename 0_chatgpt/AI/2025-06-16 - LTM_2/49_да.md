>[!nexus_user] **User** - 17.06.2025 at 3:05
> Да
<!-- UID: f2963dd8-3c3e-4f9a-970a-d3ec4f14e1ed -->




>[!nexus_agent] **Assistant** - 17.06.2025 at 3:06
> Отлично, запускаю исследование по фактическим данным: кто, когда и как проводил эксперименты, пересекающиеся с архитектурой LTM, проблесками мышления в малых LLM, эффектами структуры подачи данных, self-reflection и сценическим/архетипическим мышлением.
> 
> Скоро вернусь с результатами.
<!-- UID: a4e1fabc-c54d-4005-af2c-c55a8a49ada4 -->

---

>[!nexus_agent] **Assistant** - 17.06.2025 at 3:12
> # 1. Curriculum Learning and Data Sequencing for Reasoning Quality
> 
> Researchers have found that the order in which training data or tasks are presented – a **curriculum** – can significantly impact a model’s reasoning abilities and the structure/consistency of its answers. For example, recent work on *Self-Evolving Curriculum (SEC)* in RL fine-tuning shows that a **poorly ordered curriculum** (e.g. hardest problems first) can severely limit a language model’s reasoning performance, whereas a well-designed or dynamic curriculum yields much better generalization on challenging reasoning tasks【1†L63-L71】【1†L133-L141】. In SEC, the training continually adjusts which problem categories (by difficulty or type) to present based on the model’s progress, leading to improvements like 13–33% higher accuracy on out-of-distribution reasoning problems compared to random or reverse order training【1†L133-L141】. This highlights that **sequencing from easier to harder problems (or aligning with model ability)** can foster more effective step-by-step reasoning strategies (the authors observed emergent behaviors like self-correction and backtracking in models with curriculum-RL training【1†L71-L79】).
> 
> Another example is the *WISDOM* framework (ICLR 2025), which explicitly uses curriculum learning to **synthesize Chain-of-Thought (CoT) data from easy to hard problems**. By progressively fine-tuning a model on increasingly difficult math reasoning prompts, WISDOM achieved large gains on math benchmarks – a 7B model finetuned this way matched GPT-4’s score on a challenging math test (AIME), and a 70B model even slightly exceeded GPT-4 on that test【2†L29-L37】. This suggests that gradually exposing models to more complex reasoning tasks (mirroring a human learning curriculum) can yield more structured, multi-step reasoning capabilities than training on mixed-difficulty data.
> 
> Even at the prompting level, a curriculum-inspired approach can improve reasoning. Luo et al. (2024) propose **LBS3**, where the model first generates and solves simpler “proxy” queries before tackling a hard query【3†L128-L137】. This step-by-step (easy-to-hard) prompting led to small but consistent accuracy boosts (around 2.5–4% absolute) on math and commonsense reasoning benchmarks across several model sizes (from a 14B to GPT-4)【5†L25-L33】. In other words, guiding the model through an *orderly reasoning curriculum* – even at inference – produces more reliable and structured solutions than one-shot prompting on the hardest questions.
> 
> More generally, curriculum learning surveys (e.g. Soviany et al. 2022) conclude that training from easy to hard often **improves robustness and performance without extra compute**【6†L126-L134】. For reasoning tasks in particular, a curriculum may help a model build **self-consistency** (by first mastering simpler reasoning patterns) and then apply those to complex problems. In summary, numerous projects (including recent ones from DeepMind, academia, etc.) demonstrate that *how* data is ordered or staged for a model can notably affect its reasoning quality and answer structure – lending support to the idea that an LLM benefits from a learning **sequence** rather than random bulk training.
> 
> # 2. Emergent "Thinking" in Smaller Models (≤1B–7B Parameters)
> 
> While many reasoning and self-reflection capabilities are most prominent in very large models, researchers have actively explored getting **small models** (under 1B or a few B params) to exhibit signs of “thinking,” such as self-evaluation and error correction. One strategy is *knowledge distillation from a larger model’s Chain-of-Thought*: Magister et al. showed that fine-tuning a 11B model on step-by-step solutions generated by a 540B model dramatically improved the smaller model’s performance (e.g. T5-XXL’s accuracy on GSM8K math went from 8% to 22% after learning from PaLM’s CoT)【13†L69-L77】. This **taught the small model to reason** through problems despite such reasoning only “naturally” emerging in much larger LLMs.
> 
> Other work focuses on enabling **self-correction and reflection** in small models via fine-tuning. Han et al. (2024) introduced *Intrinsic Self-Correction (ISC)* to fine-tune models as small as 6B to **verify and fix their own answers** without external prompts【9†L43-L51】【9†L52-L57】. They created a training pipeline where the model generates an initial answer, then a self-critique, and then a revised answer, and the model is finetuned on these self-generated critiques/revisions. The result was that even a 6B model – which **normally “lacks self-awareness” and cannot assess its outputs**【9†L119-L127】 – learned to spot mistakes in its reasoning and improve its answers, boosting accuracy on commonsense and factual QA tasks compared to no self-correction【9†L47-L55】. In short, with the right training, **small LMs can be nudged toward human-like self-reflection loops**.
> 
> There have also been efforts to pair small models with stronger “mentors” or verifiers. The *SuperCorrect* project (ICLR 2025) used a GPT-4-level teacher model to supervise a 7B student model’s reasoning steps and reflections【10†L27-L35】【10†L39-L47】. In a two-stage process, the teacher’s thought process is distilled into the student (providing **hierarchical thought templates** for the student to follow), and the teacher’s corrections of the student’s errors are used to train the student’s own self-correction ability. This yielded a **SuperCorrect-7B** model that achieved state-of-the-art results among 7B models on math reasoning benchmarks – outperforming prior fine-tuned 7B models by 5–15% absolute on GSM8K and MATH【10†L39-L47】. The clear takeaway is that even relatively small models can learn advanced reasoning and error-checking *if given appropriate guidance* (via either distilled traces from a larger model or specialized finetuning data).
> 
> Complementary to that, Zhang et al. (ACL 2024) showed small open models (≤13B) can **learn to critique and refine their answers** on reasoning tasks without always relying on a large external AI. They prompted a 7B model itself to generate explanations for why a given answer was wrong (using the true answer as a hint), collected these self-critiques, and fine-tuned the model to use them in answer refinement【11†L19-L27】【11†L23-L30】. The resulting model demonstrated notable gains on math and commonsense tasks when a strong checker (GPT-4) was available to verify its answers – roughly a 15% improvement – though with a weaker internal verifier the benefits were limited【11†L23-L30】【11†L25-L33】. This indicates **small models can internalize a form of “self-criticism”**, but they still struggle with knowing *when* they are wrong without an external signal. In practice, techniques like these (and others such as *program-aided reasoning* or *tool-use for small LMs*) are actively shrinking the gap in “cognitive” behaviors between small and large models. Small models, when carefully trained or assisted, **have shown sparks of multi-step reasoning, introspection, and even rudimentary self-correction**, which were once thought to be exclusive to models like GPT-3.5/4.
> 
> # 3. Incorporating Scenes, Roles, and Dramaturgy in Model Generation
> 
> Another line of research has treated **narrative structure and theatrical elements** (scenes, characters with roles or archetypes, dialogues, etc.) as first-class elements in generation and comprehension. For instance, DeepMind’s **Dramatron** system explicitly organizes the generation of a screenplay into hierarchical stages: given a brief premise (logline), it uses a language model (GPT-3) to generate a title and a list of characters, then a plot outline broken into scenes, then setting descriptions, and finally dialogue for each scene【22†L69-L77】. By chaining prompts in this structured way, Dramatron produces scripts with much greater long-range coherence than unguided text generation, since it **ensures consistency of characters and story beats across scenes**【22†L60-L68】【22†L69-L77】. In a user study with playwrights, the AI-generated scripts were considered *formulaic* and not ready for final use, but writers found the tool valuable for **world-building and exploring alternative storylines** by tweaking characters or plot points【21†L55-L62】. This suggests that embedding knowledge of **dramatic structure (e.g. distinct acts or scenes, explicit roles)** helps an LLM-supported system create more meaningful and manipulable narratives, albeit with human creativity still in the loop.
> 
> Researchers have also examined how well *pretrained* LLMs understand or reproduce classic **archetypal roles and narrative patterns**. Zervina et al. (2023) conducted an analysis of GPT-4 and Claude generating short stories for six Jungian archetypes (Hero, Wise Old Man, Shadow, Trickster, Everyman, Anima/Animus). Interestingly, they found these models **excel at using structured, goal-driven archetypes** like the Hero or Wise Old Mentor – those stories came out very close to human-written ones in thematic content – but they *struggle with more ambiguous or subversive archetypes* like the Trickster or Shadow【18†L1-L5】【18†L151-L158】. In other words, current LLMs can mimic the **clear-cut roles and narrative arcs** found in mythic or traditional stories, but they have difficulty capturing the psychological complexity and irony of more modern, nuanced characters. This kind of research treats roles/archetypes as “meaning units” and shows where AI generation aligns with human storytelling tropes (and where it falls short).
> 
> Beyond narrative generation, an exciting development is using LLMs to drive **situated, role-based simulations** – essentially *improvised drama* with multiple agents. Park et al. (2023) created *Generative Agents*, a system where each agent is an LLM with a defined persona, backstory, and **“wills” (goals/preferences)**, all situated in a shared virtual town【24†L53-L61】【24†L62-L70】. These agents continually record their experiences, **reflect** on them to form higher-level plans, and interact via dialogue. Remarkably, the emergent behavior resembled a believable **social drama**: for example, with only an initial prompt that one agent wanted to host a Valentine’s Day party, the agents autonomously spread invitations, made new acquaintances and romantic dates, and coordinated to attend the party together at the right time【24†L63-L70】. Such behavior required embedding **scene and role awareness** in the agent architecture – each agent “knows” its identity and relationships and has memory of past scenes. The result is a dynamic, theater-like simulation driven entirely by LLM-based characters. This project demonstrates that when models are given **explicit roles, personas, and a capacity for narrative memory**, they can generate coherent long-term interactions (essentially *acting out* a multi-character story with consistency). It’s a step toward treating *dramaturgical structures* – like plots, character intentions, and inter-agent dialogues – as fundamental to AI understanding and generation, not just surface labels or emotions.
> 
> In summary, whether through structured screenplay generation, analysis of archetypal content, or multi-agent simulations, we see that AI research is embracing **story and drama structures**. Elements like scenes, character archetypes, and narrative arcs are used to guide models, making their output more meaningfully organized (as opposed to flat text). This aligns with the intuition behind *LTM’s ideas* that an AI might benefit from understanding **scenography and roles** in language, not merely word sequences or sentiments.
> 
> # 4. Multi-Stage Training Mirroring Cognitive Evolution (Myth → Drama → Logic → Irony)
> 
> The concept of training an AI through stages analogous to the *evolution of human thought* – from mythic storytelling, to dramatic interpersonal narratives, to formal logical reasoning, to perhaps a self-aware ironic perspective – is highly intriguing. In our literature search, we did **not find any published project that explicitly implements a “myth → drama → logic → irony” curriculum** in these terms. Most curriculum learning research focuses on complexity (easy-to-hard) or modality (e.g. vision then language), rather than philosophical stages of cognition. However, a few developments resonate with the spirit of this idea:
> 
> - **Anthropic’s Constitutional AI** can be seen as a simple two-stage simulation of “ethical reasoning evolution.” First, a base model is fine-tuned to *criticize and improve its own outputs* according to a set of principles (a “constitution”) – essentially instilling a moral/reflective layer on top of its knowledge. Then a second stage uses reinforcement learning (with AI feedback) to further align the model’s responses with these principles【28†L83-L91】. Notably, during the supervised self-critiquing phase, the model was allowed to use *chain-of-thought reasoning* to explain and fix its answers【28†L85-L94】. This added a form of **meta-cognitive training stage** on top of the model’s base capabilities, somewhat analogous to moving from a purely “logical” responder to one that can **reflect on rules and higher-level intent** (though in this case the focus was on harmlessness). While not myth-to-irony per se, it exemplifies *structured layering of training*: a foundational stage (general language ability) followed by a *normative reasoning stage*.
> 
> - Some researchers have proposed or experimented with **mixing different genres and cognitive styles** in training to balance creativity and factuality. For example, OpenAI’s GPT-4 was trained on a blend of internet text that undoubtedly includes mythology/fiction, drama (human dialogues), and scientific or logical text. The training wasn’t separated into phases, but the *resulting model* can adopt different modes (creative storyteller vs. formal reasoner) as needed. There is anecdotal discussion in the community that a model might benefit from a **curriculum that first hones its imaginative narrative skills and then its precision reasoning** (or vice versa), but concrete studies are sparse. One related work is by Zhao et al. (2023), who found that *finetuning on diverse dialogue styles and personas* can make a model more robust and contextually aware (though this was not explicitly framed as mythic vs. logical stages)【no source found】.
> 
> - The **“neural assembly” analogy** has been touched on by modular or *mixture-of-expert* models. For instance, some research from DeepMind and others explores having different expert subnetworks within a transformer, each trained on a different type of data (e.g. code vs. narrative), and a gating mechanism that activates them as needed. This is reminiscent of assembling brain-like specialized circuits. One could imagine a future architecture where one module is specialized on “mythic imagination” (creating coherent narratives), another on “dramatic/social reasoning” (modeling motivations of characters or agents), another on formal logic/math, and another on reflective/ironic commentary – and the AI system could **route a query through these modules** in stages. While current systems like GPT-4 don’t explicitly do this, research is moving toward *compositional AI*. For example, IBM’s Project Debater and certain hybrid neuro-symbolic systems combined a language model with logic rule engines, which is a primitive version of mixing narrative understanding with logical reasoning.
> 
> In summary, **the LTM architectural vision of sequential cognitive stages is not yet an established training paradigm** in mainstream AI. Most work either focuses on one “stage” at a time (e.g. pure logical reasoning, or pure story generation) or blends them without a clear separation. That said, some projects incorporate elements of this layering idea: *models that self-reflect (a bit of an “ironic” self-critique stage)*, curricula that go from simple to complex tasks (which often means from intuitive narrative-like tasks to high logic tasks), and modular architectures that could be seen as assembling lower-level skills into higher-level “thought.” The results so far neither fully confirm nor refute the LTM idea – they simply haven’t explicitly tried it. It remains an open and fascinating research direction to construct an AI training regime that **mirrors the historical/child developmental progression** from mythic storytelling, to social drama, to scientific logic, to self-referential irony. Existing pieces like the ones from OpenAI, Anthropic, DeepMind, Meta, and academic labs provide partial evidence that *each stage has value*, but a unifying experimental validation of the full stage-wise evolution concept is still awaiting exploration. 
> 
> **Sources:**
> 
> - Bai et al., *“Constitutional AI: Harmlessness from AI Feedback,”* Anthropic (2022)【28†L83-L91】【28†L85-L94】 
> - Han et al., *“Small Language Model Can Self-Correct,”* arXiv preprint (Jan 2024)【9†L43-L51】【9†L119-L127】 
> - Luo et al., *“Let’s Be Self-Generated Step by Step (LBS3): Curriculum Learning for Reasoning with LLMs,”* arXiv preprint (2024)【3†L128-L137】【5†L25-L33】 
> - Park et al., *“Generative Agents: Interactive Simulacra of Human Behavior,”* arXiv (revised Aug 2023)【24†L53-L61】【24†L63-L70】 
> - Qiu et al., *“WISDOM: Progressive Curriculum Synthesis Makes LLMs Better Mathematical Reasoners,”* ICLR 2025 (2024)【2†L29-L37】 
> - Yang et al., *“SuperCorrect: Advancing Small LLM Reasoning with Thought Distillation and Self-Correction,”* ICLR 2025 (2025)【10†L27-L35】【10†L39-L47】 
> - Zervina et al., *“AI Narrative Modeling: How Machines’ Intelligence Reproduces Archetypal Storytelling,”* Information **16**(4):319 (2023)【18†L1-L5】【18†L151-L158】 
> - Zhang et al., *“Small Language Models Need Strong Verifiers to Self-Correct Reasoning,”* Findings of ACL 2024【11†L19-L27】【11†L23-L30】
<!-- UID: 034a75fe-de6c-49e7-8710-33c2f6a5e86c -->