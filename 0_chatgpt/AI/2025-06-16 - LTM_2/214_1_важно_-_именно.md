>[!nexus_user] **User** - 21.06.2025 at 23:08
> 1. Важно - именно chatgpt открытый в браузере через чат и скрины, генерацию файлов управляет gui ПК. 2. Что есть то и покажи, но обязатнльно с учетом 1 пункта, Мы делаем локальное AGI, это важно. 3. Хоть чтото. Лучше локально.
<!-- UID: bbb21ca0-16f7-4e84-afad-10d8b3b26dad -->




>[!nexus_agent] **Assistant** - 21.06.2025 at 23:08
> Хорошо, я изучу существующие решения и эксперименты, где ChatGPT или аналогичные модели управляют графическим интерфейсом операционной системы локально, используя взаимодействие через чат и скриншоты, включая генерацию файлов (например, AHK, JSON, CSV). Приоритет — локальные реализации, пригодные для создания AGI.
> 
> Сообщу, как только найду систематизированный обзор и подходящие примеры.
<!-- UID: 66b45680-7378-4a7d-9826-03a6ff6dd487 -->

---

>[!nexus_agent] **Assistant** - 21.06.2025 at 23:12
> # Управление локальным GUI с помощью LLM-агентов
> 
> Современные разработки позволяют использовать большие языковые модели (LLM) вроде ChatGPT для управления графическим интерфейсом локального компьютера через текстовый чат и скриншоты. Так, OpenAI представила «Computer-Using Agent» (CUA) – прототип агента, основанного на GPT‑4o с мультивизуальными способностями. Он получает скриншоты экрана (состояния ОС), анализирует их и виртуальной мышью/клавиатурой выполняет действия (нажатия, ввод текста и т.д.)【8†L165-L174】【8†L174-L182】. CUA проходит итеративный цикл «восприятие – рассуждение – действие»: сначала модель видит снимки экрана (пиксельные данные), затем строит пошаговый план (chain-of-thought) и, наконец, совершает нужные клики и ввод【8†L173-L182】. Аналогично Anthropic реализовал в Claude 3.5 (Sonnet) режим “computer use”: модель может «смотреть на экран, двигать курсор, кликать по кнопкам и печатать текст» на удалённом десктопе через API【17†L31-L39】. Claude умеет выполнять многозадачные процессы (поиск и заполнение форм и т.д.), экспериментально набирая неплохие результаты на задачах OSWorld【17†L119-L122】. 
> 
> **Проекты и исследования:** Hugging Face выпустила облачный инструмент *Open Computer Agent*, схожий с ChatGPT Operator. Он запускается в Linux-образе и может запускать приложения (например, браузер), чтобы по запросу «открыть Google Maps и найти Hugging Face HQ в Париже» самостоятельно открыть браузер и выполнить поиск【26†L133-L142】. Hugging Face подчёркивает, что современные визуальные модели (например, Qwen-VL) уже способны «быстро и точно настраивать координаты элементов на экране, чтобы кликнуть по ним»【26†L163-L172】. Академические работы идут в том же направлении. Например, проект **ShowUI** (Сингапурский ун-т, Microsoft) обучает мультимодальную модель распознавать элементы GUI по скриншоту и генерировать действие (клик, ввод)【5†L33-L44】. В ShowUI используется «UI-guided visual token selection» для эффективной обработки больших скриншотов и показано высокое качество «привязки к элементам» интерфейса в ноль-выстрел (zero-shot) сценариях【5†L33-L44】. 
> 
> **Структуры и фреймворки:** В индустрии также появились платформы для разработки и тестирования таких агентов. Microsoft выпустила **Windows Agent Arena** – среду для оценки агентов на реальном Windows, включая более 150 задач в браузере, офисных программах, Проводнике и настройках【12†L189-L197】. В WAA агенты могут отправлять в систему команды клавиатуры/мыши и получать скриншоты в ответ. В рамках этого проекта Microsoft представила open-source *UFO2: Desktop AgentOS* – многоагентную систему для Windows. В ней один «HostAgent» разбивает задачу на подзадачи и специальные **AppAgent** обращаются к нативным API и средствам UI Automation. UFO 2 использует гибридную схему распознавания: комбинирует возможности Windows UI Automation (доступность элементов) с визуальными моделями (в том числе Omniparser) для обнаружения и взаимодействия с элементами интерфейса【14†L22-L30】【14†L115-L122】. Как отмечено в документации, Omniparser преобразует пиксели экрана в структурированные регионы (текст, иконки), после чего GPT-4V может на основе этих данных сгенерировать конкретные команды для ОС【12†L191-L194】. Благодаря тесной интеграции с ОС такие решения устойчивее к изменениям интерфейса и эффективнее в реализации действий.
> 
> ## Интеграция через скрипты и инструменты
> 
> Часто LLM-агенты организованы как промежуточное звено: ChatGPT генерирует код или скрипты, а сторонний контроллер (например, AutoHotkey или xdotool) исполняет их. Например, на базе GPT-4V показан подход AskUI: Python-класс контроллера вызывает `move_mouse()`, `double_click_at_location()`, ввод текста и т.д. в зависимости от подсказок GPT-4V【7†L91-L100】【39†L1-L4】. Грубо говоря, GPT-4V «видит» пронумерованные элементы интерфейса на скриншоте и отвечает, какой элемент (по номеру) нужно активировать. Затем скрипт (на Linux – через xdotool) перемещает курсор и нажимает в нужном месте. AskUI демонстрирует трёхуровневую схему: сначала LLM разбивает задачу на шаги, затем GPT-4V анализирует текущий снимок экрана, а контроллер на Python выполняет реальные клики и ввод текста【7†L91-L100】【39†L1-L4】. 
> 
> Другой подход – использовать AutoHotkey (AHK). Были опубликованы проекты, где ChatGPT генерирует и запускает AHK-скрипты. К примеру, есть самодельный «Гранти» (Grunty) – десктопное приложение на Python+PyQt6, которое подключается к Claude 3.5 с включённым Computer Use. Пользователь задаёт устный/текстовый запрос, Claude генерирует команду, приложение создаёт AHK-скрипт с нужными действиями (клик, ввод) и исполняет его【23†L248-L257】. Аналогично другой проект (GitHub *autohotkey-chatgpt-voice*) с помощью голосового ввода (Whisper) превращает речь в текст, отправляет его ChatGPT, получает AHK-код и сразу запускает его – тем самым пользователь голосом управляет окнами и приложениями【35†L147-L154】. 
> 
> Наконец, есть примеры более прямой интеграции: на Habr был описан эксперимент, где ChatGPT («без API от OpenAI» через gpt4free) служит «записной книжкой» помощника. Вначале LLM получают системный промпт вроде «ты – умный помощник для Windows 11. Если для решения задачи текстовой инструкции недостаточно – генерируй код на Python»【29†L74-L81】. Когда в ответе появляется блок с кодом Python, приложение сохраняет его в файл, перезагружает модуль и вызывает функцию `answer()` из этого файла【38†L139-L147】. Таким образом нейросеть «писала» код, а скрипт на Python фактически выполнял его, управляя файлами и настройками. В авторском эксперименте такой подход позволял, например, создавать папки, переименовывать файлы, запускать процессы – ChatGPT получил «доступ к ПК» через исполнение сгенерированного им кода【38†L139-L147】. 
> 
> ## Итоги и перспективы
> 
> Таким образом, уже сегодня существует несколько подходов к связке LLM с управлением локальным GUI: одни используют *визуальные подсказки* (скриншоты) и симуляцию мыши-клавиатуры (OpenAI Operator, Claude computer use, HF Open Computer Agent, ShowUI, AskUI и др.); другие – *генерацию скриптов* (AHK, bash, Python) с последующим выполнением (проекты Grunty, AHK-ассистенты, PyQt-эксперимент с кодом). Многие из них работают через облако (GPT-4, Claude API), но появляются и более локально-ориентированные решения. В частности, Microsoft открыла исходники UFO 2 (Desktop AgentOS) с интеграцией UI Automation API, а Open Source-сообщество экспериментирует с локальными мультимодальными моделями (например, Qwen‑VL) для подобных задач. Все упомянутые системы позволяют LLM «видеть» текущий экран (снимок), генерировать инструкции (скрипты) для действий и автоматически их выполнять, получая обратную связь о состоянии ОС【8†L165-L174】【38†L139-L147】. Эта область бурно развивается: новые бенчмарки (Windows Agent Arena, OSWorld) и проекты (AskUI, UFO2, Claude Computer Use) закладывают основу для будущих «агентных» интерфейсов, способных управлять ПК практически как человек.
> 
> **Источники:** исследовательские работы и статьи о GUI-агентах【5†L33-L44】【8†L165-L174】【17†L31-L39】【12†L191-L194】【14†L22-L30】【14†L115-L122】; а также блоги и проектные описания (AskUI【7†L91-L100】, Grunty【23†L248-L257】, ChatGPT+AHK【35†L147-L154】, Habr-статья【29†L74-L81】【38†L139-L147】 и пр.).
<!-- UID: 95da43ea-748b-4e07-b6db-e315cd034998 -->