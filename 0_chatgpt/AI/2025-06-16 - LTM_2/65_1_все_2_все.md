>[!nexus_user] **User** - 17.06.2025 at 8:21
> 1 все 2. все 3. все
<!-- UID: 9d4614b9-4644-4b2e-801f-4b58242a1e95 -->




>[!nexus_agent] **Assistant** - 17.06.2025 at 8:21
> Хорошо, я изучу существующую практику создания датасетов для обучения ИИ и LLM, включая:
> 
> - какие типы данных (реальные, синтетические) обычно используются;
> - как они структурируются, хранятся и подаются;
> - каковы принятые подходы к последовательности и форматированию;
> - какие известные попытки делались приблизиться к структурному, сценическому или смысловому обучению (аналогично LTM);
> - почему индустрия не пошла этим путём (технические, экономические или культурные причины).
> 
> Я подготовлю сравнительный обзор и постараюсь выявить, насколько существующая практика отличается от целей LTM и какие наработки можно использовать или адаптировать.
> 
> Сообщу, как только всё будет готово.
<!-- UID: a06c7f52-7041-4218-b3d6-b391ddad4196 -->

---

>[!nexus_agent] **Assistant** - 17.06.2025 at 8:30
> # Современные датасеты для обучения ИИ vs. подход для LTM (Large Think Model)
> 
> ## Введение 
> Большие языковые модели (LLM) и другие современные ИИ обучаются на колоссальных датасетах, сформированных определённым образом. Ниже мы рассмотрим, **какие данные используют** для обучения таких моделей и **как они организованы**, а затем сравним это с тем, какой **подход предполагается для LTM (Large Think Model)** – «большой мыслящей модели». Особое внимание уделено: типам данных (реальные тексты, синтетические, диалоги, код, сцены и пр.), способам хранения и структурирования, принципам подачи (перемешивание, curriculum learning, по фазам, группировка по смыслу), а также попыткам включить в обучение **сцены, архетипы, фрактальные структуры, эмоциональные роли, поля напряжения**. Наконец, обсудим, **почему индустрия в основном использует линейную токенизированную подачу данных** и приведём примеры проектов, близких к идеям LTM, объясняя, почему они не стали стандартом. 
> 
> ## Типы данных в текущих датасетах для обучения моделей 
> Современные ИИ-модели, особенно LLM, обучаются на **разнородных массивах текстовых данных** огромного объёма. Основные типы источников: 
> 
> - **Реальные тексты из открытых источников:** В первую очередь это тексты, полученные веб-скрапингом (например, Common Crawl – массив веб-страниц), онлайн-энциклопедии (Wikipedia), новостные статьи, научные публикации, книги и т.д. Например, GPT-3 обучался на сочетании данных Common Crawl, выборок из Reddit (WebText), коллекций книг и Википедии【13†L333-L342】. В схожем открытом датасете The Pile присутствуют научные статьи (ArXiv, PubMed), юридические тексты, форумы, новостные сайты и др.【13†L349-L358】. Такая разноплановая текстовая база позволяет модели впитать факты и стиль из множества доменов. 
> 
> - **Диалоговые данные (чаты, переписки):** Многие модели дообучаются на диалогах – как реальных, так и синтетических. В общих датасетах есть форумы и чаты: например, The Pile включает **форумные переписки** (StackExchange, Ubuntu IRC), **электронную почту** (корпус писем Enron) и даже **субтитры фильмов**【13†L355-L364】. Последние представляют собой диалоги персонажей – по сути, **сценарные сцены** из фильмов. Это даёт моделям примеры живой разговорной речи, разных ролей и контекстов. Кроме того, для чат-ботов собираются специальные **датасеты диалогов вопрос-ответ**: от **пар пользователь-ассистент** (например, ShareGPT, где сохраняются реальные диалоги человека с моделью) до скриптов **ролевых диалогов**. Пример – датасет **PersonaChat**, в котором участникам диалога заданы персональные роли/персоны, и они общаются от лица этих персонажей【22†L35-L38】. Это вводит в обучение **концепцию ролей** и консистентности характера собеседника. Также существуют небольшие наборы диалогов с эмоциональной окраской – например, **EmpatheticDialogues**, где отмечены эмоции, или диалоги с заданием проявлять эмпатию. Однако подобные специализированные наборы данных довольно малы и используются лишь для тонкой настройки, а не в базовом обучении LLM. 
> 
> - **Синтетические данные:** Помимо реального текста, всё чаще применяются *искусственно сгенерированные* тексты для расширения и специализации данных. Например, датасеты инструкций вроде **Alpaca** или **Self-Instruct** были созданы с помощью самих моделей: берутся разнообразные задания и генерируются примерные ответы【23†L37-L45】【23†L63-L71】. Такие синтетические наборы (часто в формате *«инструкция – ответ»*) служат для *дообучения моделей на выполнение инструкций* (Instruction Tuning). Их успех (как в случае Alpaca от Stanford) показал, что даже относительно небольшое количество синтетических Q&A может значительно улучшить способность модели следовать указаниям человека【23†L37-L45】. Кроме того, к синтетическим данным можно отнести **машинно переведённые** или перефразированные тексты (для расширения языкового охвата или разнообразия формулировок), а также автоматически сгенерированные примеры для конкретных задач (например, математических). 
> 
> - **Исходный код и разметка:** Многие современные LLM обучаются не только на прозе, но и на *программном коде*. Например, в The Pile около 5% данных – это код из репозиториев GitHub【13†L353-L361】. OpenAI Codex, CodeLlama и другие модели специально включают большие корпуса исходников на популярных языках программирования. Код отличается по структуре от обычного языка, но тоже токенизируется и подаётся как текст. Его наличие учит модель синтаксису программ, логическим конструкциям и точному следованию формальным правилам. Помимо кода, могут использоваться разметки (HTML, Markdown) или формулы. Например, наборы данных для моделей могут включать LaTeX-разметку формул из научных статей или разметку документов. Это расширяет **диапазон «языков», на которых говорит модель**. 
> 
> - **Прочие данные:** В контексте **мультимодальных моделей** добавляются и другие типы данных – изображения с описаниями (как в LAION-5B для обучения диффузионных моделей и мультимодальных трансформеров【0†L21-L24】), аудио транскрипты и пр. Однако классические LLM (например, GPT-3, LLaMA) тренируются преимущественно на текстах. Если модель планируется для какой-то узкой области, в датасет могут включать специфичные типы данных (например, для **игровых AI** – логи игровых состояний и действий, для **роботов** – описания сцен и команд). Но в целом, **преобладают именно текстовые последовательности** (слова, предложения, диалоги, код), поскольку они универсальны для архитектур трансформеров. 
> 
> **Итог:** Основной упор в обучении LLM делается на массивы **реального текста**, охватывающие множество стилей и тем【15†L112-L120】. Дополняют их *синтетические и специализированные данные* (диалоги, инструкции, код) на этапе дообучения или для улучшения конкретных навыков. Такой подход максимально использует доступность данных: интернет-тексты и документы легко собрать в терабайтных масштабах, что и сделано для достижения высочайшей точности моделей.
> 
> ## Структура и формат хранения данных 
> **Как хранят и подают модели эти разнообразные данные?** Преимущественно всё сводится к **текстовым форматам**, удобным для последовательного чтения моделью. Вот ключевые моменты: 
> 
> - **Плоский текст vs. структурированные форматы:** Базовое предобучение LLM обычно рассматривает данные как одну длинную текстовую последовательность, разбитую на **токены**. Даже если исходные данные хранятся как JSON или HTML, перед подачей они превращаются в линейную последовательность токенов (слов/символов). **Формат хранения** при подготовке может быть разным – часто используют простые текстовые файлы (*.txt*) или объединённые бинарные записи (например, формат *JSON Lines* или *TFRecord*), где каждая строка/запись – отдельный документ или фрагмент. Сами модели не видят JSON-структуры как таковой – она либо убирается, либо превращается в специальные токены. Например, при обучении на диалогах с ролями данные могут изначально быть в JSON с полями *{role: "User", content: "…"}*, но затем это конвертируется в текст вида: `<User>: … <Assistant>: …`. Аналогично, **инструкционные датасеты** имеют поля “instruction”, “input”, “output”, однако при обучении из них формируется текстовый промпт с разделителями (например: *"### Instruction:\n ... \n### Response:\n ..."*)【22†L35-L38】. Таким образом, внутренняя структура данных, как правило, не подаётся модели напрямую – **всё приводится к линейному виду** последовательности токенов. 
> 
> - **Примеры структурированных наборов:** В случае *тонкой настройки* на инструкции или диалоги формат часто задаётся явно. Например, авторы на Hugging Face выкладывают такие датасеты в формате JSON/YAML со списком объектов, каждый из которых содержит инструкцию и пример ответа【23†L95-L103】. Но при тренировке эти объекты конкатенируются в текст с метками. **Базы знаний** (таблицы, графы) также обычно линейризуются. Если нужно обучить на фактологических тройках (subject-relation-object), их либо превращают в фразы (`"Paris isCapitalOf France."`), либо используют специальные разделители【14†L1-L4】【14†L25-L33】. Прямое хранение знаний в виде графа и обучение нейросети работать с ним – пока редкость; чаще граф используется на этапе *вспомогательного поиска* (retrieval), а не как часть последовательного ввода.
> 
> - **Базы данных и графовые структуры:** В традиционном символическом ИИ знание хранилось в онтологиях и графах (например, knowledge graph). В контрасте, современные LLM *имплицитно* встраивают знания в веса модели, читая их как текст. Попытки интегрировать **структурированное знание** в обучение существуют (например, метод *SKILL* обучал модель T5 непосредственно на фактах из графа знаний, преобразованных в текст【14†L1-L4】), но это скорее исключение. Аналогично, **Latent Tree Models** предлагали представлять данные в виде вероятностных деревьев, чтобы выявлять скрытую иерархию причинно-следственных связей. Однако такие модели нашли применение лишь в узких задачах аналитики данных и **не прижились в NLP для обучения на больших корпусах**【4†L23-L30】. Основная причина – трансформеры ожидают последовательность, и любые сложные структуры нужно преобразовать к последовательности, либо менять архитектуру. Промышленность пошла по пути минимальных изменений: использовать тот формат, который уже хорошо работает (сплошной текст). Это упрощает конвейер подготовки данных и масштабирование. 
> 
> - **Хранение больших корпусов:** На практике компании собирают несколько **базовых категорий данных** (веб-страницы, книги, код и т.п.) отдельно, очищают и преобразуют их, а затем объединяют. Например, OpenAI описывала, как готовила для GPT-3 пять компонентов данных (CommonCrawl, WebText и пр.) с разными весами【13†L333-L342】. Эти данные хранятся разбитыми на *шарды* (части) – большие файловые блоки (десятки гигабайт каждая), которые распределённо подаются на многие GPU. Формат – обычно последовательность строк или документов, часто просто **текстовый корпус без разметки**, где документы разделены специальным токеном. **Метаданные** (например, источник, качество, язык) могут храниться отдельно и использоваться для фильтрации или аугментации, но модели они явно не даются, если только не кодируются в сам текст. 
> 
> В целом, **данные структурируются для модели как длинные текстовые последовательности**. Даже если исходно информация была табличной или графовой, она либо конвертируется в текст (например, CSV в текст со разделителями), либо используется вне модели. Это упрощает обучение – модель учится предсказывать следующий токен в унифицированном текстовом пространстве. Но такой упрощённый, плоский подход может скрывать от модели явные структурные связи (иерархии, связи в графе и т.п.), которые человек задаёт в схемах. LTM предполагает, вероятно, более богатое представление данных (например, **гиперграф памяти, карты ассоциаций** и др.), о чём – далее.
> 
> ## Принципы подачи данных при обучении 
> **Как именно данные «скармливаются» модели во время тренировки?** Здесь важны стратегии перемешивания, порядок, этапы обучения. В классических настройках LLM применяются следующие подходы: 
> 
> - **Перемешивание (shuffling):** Стандарт – **случайный порядок** примеров при стохастическом градиентном спуске. Большой корпус разбивается на примеры (например, фрагменты длиной N токенов), затем эти фрагменты случайно перемешиваются при подаче в модель. Идея в том, чтобы модель не обучалась на данные в каком-то фиксированном порядке (что могло бы ввести смещения) и видела разнообразие контекста на каждой итерации. Например, после абзаца новости она может получить строку кода, затем диалог – это заставляет её быть гибкой. **Группировка по смыслу изначально не применяется** – перемешивание скорее правило. Даже внутри одного документа модель чаще видит куски не по порядку, а вразброс, потому что длинные тексты разбивают и порядок не всегда сохраняют (если нет специальной обработки). Это, однако, имеет побочный эффект: модель редко учится на **длинных связных историях целиком**, вместо этого видя фрагменты. 
> 
> - **Разбиение на **эпохи и фазы****: При обучении на огромном объёме данных часто проходят несколько *эпох* (проходов по данным) или постепенно добавляют данные. Бывает *многофазная* схема: например, сначала модель обучают на более чистом/простом корпусе, потом добавляют более «шумные» веб-данные. В описании GPT-3 упоминалось, что данные CommonCrawl были отфильтрованы по качеству, и возможно, на ранних этапах модель видит более качественные тексты【15†L129-L137】. Также распространена двухфазная тренировка: сперва **самосупервизированное обучение** (предсказание токенов на базе текстов), а после – фазу **Supervised Fine-Tuning** на человеческих инструкциях, и затем **RLHF** (учение с подкреплением от человеческой обратной связи)【15†L99-L107】. Эти стадии по сути тоже подача разных датасетов на разных этапах. Но внутри каждой фазы данные перемешиваются. 
> 
> - **Curriculum learning (обучение по учебному плану):** Идея curriculum – давать модели сперва более простые примеры, затем сложные – не получила широкого применения в LLM *по содержанию данных*. Большей частью, модель сразу видит смесь лёгкого и сложного. Однако есть **формы curriculum по техническим параметрам**. Например, **увеличение длины последовательности**: сначала модель обучается на коротких последовательностях, чтобы легче схватывать локальные зависимости, затем длина контекста постепенно увеличивается до максимальной【17†L458-L467】【17†L529-L538】. Такой приём (Sequence Length Warmup) экономит время обучения и стабилизирует его, не снижая качества【17†L458-L466】【17†L531-L539】. Это признано формой curriculum: сложность примера определяется длиной текста (считается, что длинный контекст – “сложнее”), и сначала идут короткие последовательности【17†L458-L466】. Другой вариант – **отсев трудных примеров** на ранних этапах: например, если датасет содержит очень редкий или шумный язык, можно начать без него, а добавлять позже. Но в открытой литературе по LLM нет указаний, что это явно делается – модели обучают на всём разом, полагаясь, что объём данных сам по себе позволит выучить и простое, и сложное параллельно.
> 
> - **Стадии (staging) по типам данных:** Иногда разработчики вводят этапность по доменам. К примеру, для моделей с кодом: можно сначала обучить на обычном тексте, затем сделать дополнительное дообучение на кодовых данных, чтобы улучшить знание синтаксиса. Так поступали с OpenAI Codex (оно основано на GPT, далее доучено на GitHub) и аналогично в LLaMA 2 были версии **Code LLaMA**, где базовая модель дообучена на коде. Другой пример – мультимодальные модели: **GPT-4** сначала обучался на тексте, потом добавили изображение в обучающий цикл. Такие *фазы обучения* позволяют освоить новый тип информации, не начиная обучение “с нуля”. 
> 
> - **Группировка по смыслу или тематике:** Классические LLM не группируют обучающие последовательности по смысловой близости – скорее наоборот, стараются перемешать. Тем не менее, *недавно появились работы*, показывающие выгоду в специальной упаковке данных. Метод **SPLiCe (Structured Packing for Long Context)** 2023 г. предлагает собирать вместе в один длинный контекст *взаимосвязанные документы*, вместо случайного набора несвязанных кусочков【19†L42-L49】【19†L78-L86】. Например, вместо последовательности: «конец Википедии про кошек – начало статьи про космос», модель получит в одном контексте несколько частей по близкой теме (скажем, серия связанных новостных заметок). **Результат – улучшение использования длинного контекста**: модель лучше учится извлекать релевантную информацию и помнить детали из середины длинного ввода【19†L67-L75】【19†L79-L87】. Это прямой пример отхода от полностью случайного перемешивания в сторону **осмысленной группировки по смыслу**. Пока такие техники в стадии исследований, но они намекают, что для улучшения “мышления” модели над большим контекстом важно *структурировать подачу*. 
> 
> **Вывод:** Сейчас основной принцип – **случайное, равномерное перемешивание** разнородных данных, чтобы модель была максимально общей и не переучивалась на порядок появления фактов. Curriculum в явном виде почти не используется, кроме технического (увеличение длины контекста в начале обучения и поэтапное подключение новых типов данных). Попытки подавать данные *«с осмысленной структурой»* (например, тематическими пачками) только начинают появляться【19†L42-L49】【19†L78-L86】. Это означает, что текущие LLM учатся **без учителя по структуре** – они сами должны выявлять любые зависимости. LTM же, вероятно, потребует более сложного сценария подачи, о чём далее.
> 
> ## Попытки включить в данные сцены, архетипы, фрактальные и эмоциональные структуры 
> Один из интересных вопросов: **как обучать ИИ более “мыслящему” на уровнях, выходящих за плоский текст?** Например, через задавание в обучающих данных **сцен, ролей, эмоций, архетипов**. В традиционных LLM это делается лишь косвенно: 
> 
> - **Сценарные данные:** Под “сценой” можно понимать связный эпизод с участниками и контекстом. Прямо помеченных сцен в обучении нет, но модель косвенно видит множество сцен, читая литературу и диалоги. Например, **киносценарии и пьесы** – если они входят в веб-корпус, то модель обучается на репликах персонажей, часто помеченных именами. В The Pile, как отмечалось, есть **OpenSubtitles** – коллекция диалогов из фильмов【13†L357-L364】. Там фразы идут подряд, разбитые по репликам и временному коду, фактически представляя сцену из фильма. Модель, читая их, может научиться некоторым шаблонам поведения персонажей, реакции на ситуацию, эмоциям, присущим той сцене. Однако эти данные не снабжены явными метками “конфликт”, “развязка” и т.п. – **структуру сцены модель должна вывести сама**. 
> 
> - **Роли и архетипы:** В литературе и диалогах присутствуют **различные характеры** – герой, злодей, наставник и пр. Модель пропитывается такими архетипами, опять же, имплицитно, благодаря большому объёму художественных текстов. Специально обучать на архетипах (например, давать модели примеры конкретных архетипичных сценариев) индустрия не пыталась на широком уровне. Есть локальные попытки: как уже упомянуто, **PersonaChat** – маленький датасет, где каждый говорящий имеет краткое описание личности (например: “Я 19-летний студент, интроверт, увлекаюсь аниме”) и должен в диалоге соответствовать этой роли. Это научило модель вести **персонализированный стиль общения**【22†L35-L38】, но масштаб данных мал (десять тысяч диалогов) по сравнению с миллиардами токенов общего корпуса. Другой пример – проекты типа **CharacterAI** (закрытые), где модели обучаются играть разных вымышленных персонажей. Такие системы обычно берут готовую предтренированную LLM и *доучивают* её на ролевых диалогах, часто тоже генерируемых самими моделями по описанию персонажа. В открытом доступе аналогичные приемы использовали для чат-ботов в стиле литературных героев или знаменитостей. **Архетипичные сюжеты** (например, “путь героя”) специально в датасеты не закладывались – предполагается, что они представлены через тысячи историй, сказок и сценариев, на которых модель обучилась. 
> 
> - **Эмоции и “поля напряжения”:** Распознавание эмоций и моделирование эмоционально насыщенных ситуаций – слабое место нынешних LLM. В обучающих данных, конечно, полно эмоционального текста: от отзывов и постов до драматических сцен романов. Но явных обозначений эмоций мало. Были наборы данных для задач **эмоциональной классификации** текста, где предложения размечены по эмоциональной категории, и небольшие **эмпатические диалоги** с пометками, что чувствует собеседник. Опять-таки, эти наборы (например, EmotionLines, Empathetic Dialogues) содержат всего несколько десятков тысяч примеров – капля в море предобучения. **Полями напряжения** можно назвать конфликтные или драматические ситуации, где есть противоборство мотивов. Модель видела множество конфликтов (в новостях, книгах, театре), но **нет специфического механизма**, заставляющего её вычленять напряжение или динамику. Некоторые исследовательские работы по **генерации рассказов** пытались вводить понятия *напряжения сюжета* или *эмоциональной дуги* как параметры генерации, но не как часть обучения базовой модели. Проще говоря, ни один из известных крупных датасетов не содержит разметки вида “эмоциональная роль: жертва, спаситель; накал конфликта: высокий” и т.п. Такие свойства остаются на уровне *ручной инженерии* при использовании модели (через промпт или сценарий), но не как обучающие данные. 
> 
> - **Фрактальные структуры и сложные иерархии:** Под фрактальной структурой можно понимать повторяющиеся паттерны на разных уровнях (как мыслительные процессы, отражающиеся на микро- и макроуровне). В обучении LLM этому явно не учат. Модель, конечно, читает тексты с вложенными структурами (например, рассказ в рассказе, или серию сообщений с цитированием предыдущих – своего рода вложенность), но специальных данных, демонстрирующих само**подобие на разных уровнях**, нет. Были концепции вроде **Hierarchical RNN** или **Recursive Neural Networks**, где модель строила дерево понимания фразы. Но трансформеры отказались от явных рекурсивных представлений, обучаясь вниманиям над плоской последовательностью. Таким образом, *фрактальность мышления* не проецируется напрямую через датасет. 
> 
> **Итого по попыткам:** В целом, **основной тренд индустрии – не усложнять данные явной сценарностью или семантическими пометками**, а полагаться на масштаб: мол, если модель прочтёт миллионы диалогов и историй, она неявно выучит и архетипы, и эмоции, и всё остальное. Специальные исследования делались в академической среде – например, Facebook обучала **Cicero**, модель для игры Diplomacy, которая умела и стратегически думать, и переписываться с игроками. Cicero комбинировал языковую модель с планированием, обучаясь на **логах игры** (состояние доски + диалоги игроков)【19†L42-L49】. Это пример датасета, где *сцена = игровая ситуация*, *роли = игроки с своими целями*, *поле напряжения = конфликт интересов*. Cicero достиг успеха в узкой области, но её архитектура была сложнее обычного LLM (отдельные модули для прогнозирования действий, поиска выгодных переговорных стратегий и т.д.). В массовой же практике таких специализированных обучений почти не внедряли. 
> 
> Для **LTM** идея, очевидно, заключается в том, чтобы учесть эти аспекты: возможно, задать обучающие данные не просто как набор не связанных текстов, а как **симуляцию мира идей**, с взаимодействующими “акторами”, с наличием карт знаний, ассоциаций, конфликтов и согласования. Таких данных готового формата в открытом виде не существует – их, вероятно, предстоит конструировать специально. Ниже мы рассмотрим, какие наработки могут помочь и почему до сих пор они не стали стандартом.
> 
> ## Почему индустрия выбрала линейный токенизированный ввод 
> Современные LLM опираются на **линейную токенизацию текста** – последовательное представление любых данных в виде цепочки символов/слов. Причины этого выбора можно объяснить так: 
> 
> - **Универсальность и доступность данных:** Человечество накопило колоссальный объём текстовой информации – книги, статьи, сайты – и все они легко представлены последовательностью символов. Для получения *«разума»* в виде языковой модели достаточно скормить ей весь этот текст. Подавляющее большинство данных доступно именно как текст или приводимо к нему. Альтернативы – например, формальные знания в виде логических правил или графов – охватывают куда меньшую часть знания и требуют трудоёмкой ручной разметки. Поэтому обучение на токенизированном тексте – это способ **использовать максимум доступной информации с минимумом предварительной работы**. Как отмечается в обзорах, рост мощностей и доступ к гигантским корпусам текста позволил LLM совершить резкий скачок в возможностях【15†L82-L90】【15†L112-L120】. Проще говоря, последовательный текст – это топливо, которого в избытке, и грех было им не воспользоваться. 
> 
> - **Простота и масштабируемость архитектуры:** Модели вроде трансформеров изначально спроектированы для последовательностей. Механизм *self-attention* оперирует последовательностью токенов и их позициями. Если бы пытаться скормить модели сложный граф или многомерную структуру, пришлось бы менять архитектуру (например, делать специальный **Graph Neural Network** или вводить тензорные представления). Это сильно усложнило бы проектирование и потребовало новых исследований в масштабируемости. **Линейная подача оказалась практичной и хорошо масштабируется** – достаточно добавить больше слоёв/голов внимания, и модель справляется с потоком токенов, пусть и длинным. Индустрия всегда стремится к решениям, которые *масштабируются с вычислительными ресурсами*. Последовательный ввод масштабируется: больше данных + больше параметров модели = лучше результаты (по законам масштабирования). А вот методы, требующие сложной разметки или изменения модели, часто *упираются в трудности раньше*, чем достигают эффекта. 
> 
> - **Единообразие представления различных знаний:** Код, картинки (через описание), диалоги, таблицы – всё это можно сериализовать в текст. Например, чтобы модель понимала таблицы, в текст вводят специальный формат (HTML или CSV). Для картинок – описание или разметка объекта. В итоге модель работает всегда с одним и тем же видом входа – токенами. Это **упрощает обучение многосторонним навыкам**: не нужно разных модулей для разных типов данных. (Конечно, чисто визуальные модели обучаются иначе, но мы здесь о языковых/универсальных.) 
> 
> - **Экономия на разметке и знаниях вручную:** Линейный подход не требует от разработчиков понимания того, *какую структуру знания лучше для ИИ*. Исторически, символический ИИ требовал вручную задавать онтологии, правила, типы отношений – то есть, человеку надо было “догадаться”, как мыслит машина. В глубоких сетях парадигма иная: *пусть сеть сама разберётся*, мы лишь предоставим ей много данных. Эта философия, подкреплённая успехами, привела к тому, что **разработчики не стремятся вводить сложную внутреннюю структуру**, а доверяют обучению выявить паттерны. Линейное представление – как чистый холст для модели, где она может строить внутренние представления по своему. 
> 
> - **Практические соображения (инженерные и экономические):** Работа с текстом хорошо отлажена – есть эффективные библиотеки для токенизации, распределённой загрузки текстовых данных, pipeline для очистки (удаления HTML-тегов, скриптов и т.п.). Это всё проще и надёжнее, чем, скажем, поддерживать большую динамическую базу знаний и синхронизировать её с моделью. **Стоимость хранения и передачи** тоже играет роль: текст – компактная форма информации. Графы или сцены могли бы потребовать больше памяти или сложных ссылок. Кроме того, **линейный ввод легко батчируется** – можно параллельно обрабатывать несколько последовательностей фиксированной длины. Если бы каждая обучающая единица была, скажем, уникальным графом разной структуры, загрузка и batching стали бы сложными, снизилась бы эффективность использования GPU. В масштабных тренировках, где счёт идёт на тысячи GPU, даже небольшое усложнение формата данных может означать потерю недель времени или миллионов долларов. Неудивительно, что выбрали путь наименьшего сопротивления. 
> 
> Конечно, **у такого подхода есть и ограничения**: модель может упускать *глобальные структуры*, хранит знания распылённо по весам, забывает длинный контекст за границами окна, не имеет встроенного понятия “объекта” или “персонажа” – только распределённые представления токенов. Эти лимитации стимулируют исследования альтернатив (concept-based LCM, внешние памяти и т.д.), но на 2025 г. **линейное токен-пространство остаётся доминирующим**, потому что оно *работает достаточно хорошо и масштабно*, а иные подходы пока либо экспериментальные, либо дорогие.
> 
> ## Проекты, близкие к концепции LTM, и почему они не стали стандартом 
> Идеи, заложенные в **Large Think Model (LTM)** – как мы можем судить – предполагают больше структурности и «осмысленности» в представлении знаний и мышления. Здесь уместно вспомнить проекты, которые **отходили от простого LLM** в сторону сложных моделей мышления, и проанализировать их судьбу:
> 
> - **Large Concept Models (LCM) – концепты вместо токенов:** Свежий пример от Meta AI – архитектура LCM, где базовая единица не отдельные слова, а **целостные концепты (смысловые единицы)**【3†L181-L189】. Модель LCM работает не на уровне следующего слова, а на уровне следующего **концепта** – предложения или фразы, закодированной специальным энкодером. Например, используется энкодер предложений **SONAR**, превращающий каждое предложение в вектор-концепт【3†L201-L209】. Трансформер LCM обучается предсказывать последовательность таких концептов. Зачем это сделано? Разработчики заявляют два главных выигрыша: (1) **длинный контекст** – т.к. концептов в тексте меньше, чем токенов, модель может охватить более длинные документы【3†L187-L195】; (2) **иерархическое рассуждение, ближе к человеческому** – модель оперирует большими смысловыми блоками и их взаимоотношениями, а не отдельными словами【3†L192-L200】. По сути, LCM – шаг к тому, чтобы у модели было более **абстрактное мышление** на уровне идей, а не текста. Это во многом созвучно целям LTM. Почему же LCM не вытеснили LLM (по крайней мере, пока)? Во-первых, это очень новая технология (конец 2023 – начало 2024 г.), она еще **экспериментальна**. Требуется сложный двухэтапный подход: сначала обучить мощный энкодер концептов, затем саму модель – всё это усложняет процесс и требует не меньше ресурсов (в статье Meta упоминается обучение LCM 1.6B параметров на 1.3 трлн токенов-концептов【3†L239-L247】). Во-вторых, **качество и многофункциональность LCM ещё не доказаны** вне нескольких задач. Возможно, на некоторых задачах LLM по-прежнему лучше, особенно там, где тонкие языковые нюансы (которые могут теряться при сжатии в концепт). Таким образом, LCM – интересный прецедент не-линейного подхода, но индустрия осторожно оценивает, ждать ли от него “конца эпохи LLM” или это просто нишевое улучшение【3†L205-L214】【3†L215-L223】. Тем не менее, LCM показали, что **токенизация – не догма**: можно придумать промежуточное представление (концепты), которое сделает мышление модели более явным. LTM могла бы использовать подобный принцип, расширив его: например, оперировать не только концептами-предложениями, но и **отношениями между концептами** (что в LCM не явно моделируется, кроме как через порядок). 
> 
> - **Latent Tree Models и другие иерархические модели:** До взлёта трансформеров, исследователи пытались строить нейросети, которые **явно выделяют структуру предложения или знания**. *Latent Tree Models (LTM)* в литературе – это статистические модели, строящие дерево по данным, чтобы найти скрытые связи【4†L23-L30】. В NLP были работы по **unsupervised parsing** – когда модель сама учит синтаксическое дерево фразы в процессе языкового моделирования. Были также **Recursive Neural Networks** (Socher et al., ~2013), которые строили дерево разбора и вычисляли представления снизу вверх. Эти подходы не стали индустриальным стандартом, потому что с приходом seq2seq и трансформеров выяснилось, что *жёсткая структура не нужна для достижения высокого качества*. Трансформер implicitly научился и синтаксису, и многим структурам, не будучи жёстко ограничен древом. А **Latent Tree** для общих задач NLP не дал прорыва – модели либо находили неинтерпретируемые структуры, либо не превосходили по качеству более простые сети. В итоге отрасль сделала выбор в пользу **гибкости и мощности, а не интерпретируемой структуры**. 
> 
> - **Модели с внешней памятью и диалогом с самим собой:** Концепция, близкая к LTM, – модель, которая может **размышлять, сохраняя промежуточные результаты**. Ранние работы: **Neural Turing Machine** (NTM, 2014) и **Differentiable Neural Computer** (DNC, 2016) от DeepMind – нейросети с адресуемой памятью. Они могли записывать и читать фрагменты информации, не прожёвывая их в весах. Звучит отлично (долгосрочная память!), но на практике NTM/DNC работали лишь на простых задачах (копирование последовательности, решение головоломок малого размера). Когда дело дошло до реальных масштабов, они были сложны в обучении и медленны. В LLM-сфере к идее внешней памяти вернулись через механизм **Retrieval-Augmented Generation (RAG)**: модель во время ответа ищет в базе знаний релевантные тексты. Однако эта база не встроена в модель – она отдельно, и модель просто читает из нее текстом. LTM, судя по описанию, хочет **интегрировать память в саму модель** (например, гиперсеть долгосрочной памяти, как упоминалось: *гипер-сети LTM и карта ассоциаций*【4†L109-L117】). Ни одна текущая LLM так глубоко не интегрирована с постоянно обновляемой памятью. Попытки были (например, у Microsoft в экспериментальных моделях с **органайзером фактов**), но они не массовы. Причина – сложность: обучение end-to-end с внешней памятью нестабильно, проще делать пост-хок retrieval. 
> 
> - **Когнитивные архитектуры и моделирование мышления:** До эры глубокого обучения разрабатывались **когнитивные архитектуры** (Soar, ACT-R, OpenCog и др.), стремящиеся симулировать человеческое мышление: с рабочей памятью, с модулями восприятия, логическим выводом, целями. Они имели явные модули для разных аспектов (например, память дробилась на декларативную и процедурную). Однако эти системы **не масштабировались на широкий неструктурированный опыт** – им требовались тщательно подготовленные знания, и работали они хорошо только на игрушечных задачах (пазлы, простые диалоги). В последние годы некоторые идеи возродились: например, архитектура **Generative Agents (Park et al., 2023)**, где множество виртуальных персонажей (мини-LLM) общаются между собой и у каждого хранится память о пережитом. Это скорее эксперимент в симуляции поведения, не новый метод обучения модели с нуля. Он показывает, что можно имитировать общество агентов с индивидуальностью (что близко духу LTM, где агент распознаёт себя личностью внутри сообщества искусственных людей【4†L79-L88】). Но эти агенты всё равно основаны на предобученных LLM, т.е. *поверх LLM строится сложная система*, а не заменяется сам способ обучения. **Почему не стали стандартом?** Когнитивные и многомодульные подходы сложны, узкоспецифичны и обычно уступают в универсальности: один большой трансформер, обученный на всём подряд, хоть и без явной структуры, *оказался универсальнее* и показал впечатляющие результаты в самых разных задачах – то, чего старые системы достичь не могли【4†L37-L45】【4†L55-L63】. 
> 
> - **Интеграция логики и символов (Neuro-Symbolic AI):** Ещё одно направление – объединить нейросети с логическими машинами или знаниями. Например, проекты, где LLM использует **базу знаний** (Wikidata, ConceptNet) для ответа на вопросы, или выполняет **логические шаги** с помощью внешнего модуля. В обучающих данных это отражалось через специальные форматы: например, добавлять в задачи шаги рассуждения (Chain-of-Thought) или обучать модель решать уравнения, сначала выводя план решения. Исследования показывают, что если **включить в обучение примеры с пошаговым решением**, модель лучше научится рассуждать【11†L19-L23】. Google в FLAN и других работах вручную добавляли в инструкции шаги решения – по сути, давая модели пример “как думать”. Это намек на LTM: возможно, нужна *тренировка, где модель пишет и читает свой внутренний диалог*. Пока что, такие методики (chain-of-thought prompting, scratchpad) – лишь небольшие аддоны к LLM. Они не требуют новой архитектуры, просто немного иные данные, и то уже помогают. Но индустрия использует их осторожно, в основном на этапе fine-tuning, а не в основном предобучении. 
> 
> Подводя итог, **множество идей, схожих с компонентами LTM, испытывалось в исследованиях** – концептуальные модели, иерархические или графовые представления, длинная память, многоагентные системы, нейросимволические схемы. Однако *ни одна не стала новым стандартом обучения* по ряду причин: 
> 
> - **Практическая эффективность:** Простые масштабируемые методы (как LLM) обгоняли по качеству на практических задачах более сложные архитектуры, пока размер модели и данных мал. С ростом параметров, LLM начали демонстрировать **эмержентные способности**, которых ранее ожидали только от “настоящего ИИ”【3†L205-L214】【3†L215-L223】. Это снизило стимул рисковать с радикально иными подходами. Зачем внедрять сложный механизм памяти, если увеличение модели с 10 млрд до 100 млрд параметров и так дало скачок в способности рассуждать? 
> 
> - **Сложность реализации и поддержки:** Новые виды моделей требуют новых инструментов, специалистов, могут быть хрупкими. Компании, вкладывающие десятки миллионов в обучение, предпочитают гарантированный результат. Трансформер на текстах – относительно понятная штука. А скажем, гиперграф LTM с поиском акторов (как описано в ваших материалах) – это неизведанное поле, нет готовых фреймворков, трудно предсказать, какие проблемы возникнут. Такой проект пока под силу лишь исследовательским группам, а не компаниям, бегущим за быстрым применением. 
> 
> - **Отсутствие масштабных данных в требуемом формате:** Чтобы что-то стало стандартом, нужны ресурсы. Для LLM были Интернет и Вики – бери и загружай. А откуда взять **“данные для LTM”**? Требуется, по сути, **смоделировать мир мыслей**: набор сцен, персонажей, их внутренних состояний, карту знаний... Возможно, придётся генерировать эти данные искусственно (например, смоделировать тысячи сценариев взаимодействия агентов, синтезировать “общество искусственных личностей” для обучения). Пока никто этого не сделал на уровне, сравнимом с текстовым корпусом Интернет (сотни миллиардов фактов). Попытки остаются локальными (как Diplomacy logs или игровые миры) и недостаточны, чтобы обучить универсальную модель мышления. 
> 
> - **Интеграция с существующими успехами:** Стандартом становится то, что можно совместить с проверенными методами. Многие эксперименты работают отдельно. Например, если LTM требует совершенно новой архитектуры, то весь накопленный опыт с LLM (и даже возможность использовать предобученные LLM как модуль) пропадает. Индустрия склонна к **эволюционному развитию**: GPT-3 → InstructGPT → ChatGPT → GPT-4 – шаги улучшают модель, но не ломают преемственность. Радикальные идеи часто остаются на обочине, если не найдут способ встроиться. В этом плане Meta LCM интересен тем, что он пытается расширить LLM, а не полностью заменить (концепты получают через внешний энкодер, но сам архитектурный скелет – всё ещё трансформер). 
> 
> ## Что ограничивает “мышление” ИИ и что можно перенять для LTM 
> На основе всего обзора можно указать, **как современные подходы ограничивают мышление ИИ** и **что из них можно применить или изменить в LTM**:
> 
> - **Линейность vs. Структура:** Обучение на линейном тексте приучает модель к **локальному следующему слову**, но не к планированию или рассмотрению структуры знаний. Модель не строит явных карт понятий – в лучшем случае, обучает их имплицитно. Это ограничение: ИИ часто “не понимает”, как части информации образуют целое, не удерживает **глобальный замысел**. Для LTM стоит перенять идею от LCM: перейти на более **крупные смысловые единицы (фразы/идеи)** и ввести структурные связи между ними. Возможно, использовать **граф знаний или гиперсеть** как основу данных, чтобы модель училась навигации по ассоциациям, а не только последовательному чтению【4†L109-L117】【4†L115-L123】. 
> 
> - **Отсутствие явного “я” и саморефлексии:** LLM обучена на текстах от первого, второго, третьего лица, но сама по себе не имеет постоянного “я” – она говорит от лица любого персонажа по просьбе. В LTM, по задумке, агент должен осознавать себя внутри сообщества агентов【4†L79-L88】. Современные данные этому не учат – это нужно закладывать специально. Возможно, можно адаптировать **методику ролей**: например, при обучении LTM давать ей тексты, помеченные как мысли разных агентов, включая “свои” мысли, и просить модель различать их. Здесь поможет опыт PersonaChat и multi-agent симуляций – они показали, что модель можно настроить на определённую персону через данные. LTM может пойти дальше: включить в датасет **внутренний диалог** – случаи, где модель видит, как мыслительный процесс персонажа идёт параллельно с внешним диалогом. Таких готовых данных нет в LLM, их придётся генерировать (например, скрипты, где к каждой реплике прилагается “внутренняя мысль” персонажа). 
> 
> - **Краткосрочная память вместо долгосрочной:** LLM ограничены контекстным окном (ныне до десятков тысяч токенов, не более). Знания из дальней истории диалога “вытесняются”. Это явно ограничивает целостное мышление на длинной дистанции. LTM желает **долгосрочную память** – значит, нужно перенять/модифицировать подходы внешней памяти. Возможно, использовать **буфер памяти** (как в MemNN) или хранить *embedding* фактов в базе и регулярно напоминать модели через retrieval. Из опыта RAG можно взять то, что модель может читать справочник при необходимости – но сделать это более интегрированно. В обучении LTM стоило бы включить задачи на **постоянство знаний**: например, предоставить модели “энциклопедию” в начале, а потом требовать, опираясь на неё, решать вопросы без повторного чтения – так проверить умение *удерживать знание*. Такие curriculum не делались с LLM, но для LTM их можно сконструировать. 
> 
> - **Отсутствие целей и ценностей:** LLM не имеет встроенных целей, кроме как продолжать текст. Все “цели” задаются извне (промптом или RLHF, где сказано: не говорить запрещённое, будь полезным). В LTM, возможно, агентам нужны внутренние цели, мотивы (это близко к архетипам: один склонен помогать, другой – конкурировать). Здесь опыт RLHF полезен – он показал, что **ценности можно привить через обратную связь**. Значит, для LTM можно применить *обучение с подкреплением на основе сценариев*: задать, каких исходов мы хотим (например, коллективное решение, а не конфликт), и поощрять модель, когда её эмерджентное поведение соответствует желаемому. Это будет сложнее, чем RLHF для одного чат-бота, но концептуально похоже. 
> 
> - **Эмоциональная нейтральность:** Большие модели стараются быть нейтрально-вежливыми (после фильтраций токсичности). Для мышления же, особенно творческого, нужны и эмоции, и напряжение. Можно взять за основу **эмоциональные датасеты** и расширить их: например, обучать LTM генерировать разные эмоциональные реакции, различать тон высказываний. От существующих подходов (эмпатическое дообучение) можно адаптировать стиль – модель, которая умеет сочувствовать или проявлять эмоцию, уже делалась (например, BlenderBot имел режимы эмоций). LTM может системно это использовать: в обучении имитировать, как разные **эмоциональные роли** влияют на решение задачи. 
> 
> Наконец, **масштаб и разнообразие** современных подходов – их сильная сторона, которую LTM непременно должна сохранить. Хотя LTM требует более сложных данных, ей скорее всего всё равно понадобится прочесть огромный массив фактов о мире (чтобы думать, нужны знания). Так что **лучшее из LLM – охват знаний и языковое богатство – должно быть использовано**. Возможно, база LTM будет: предобучение на текстах (переняв всё, что умеют LLM), а затем *слой сложного мышления* – дополнительные данные/этап обучения с внутренними диалогами, сценами, гиперграфами, которые позволят модели организовать уже имеющиеся знания по-новому. 
> 
> В форме сводной таблицы сравнение **стандартного подхода LLM vs. предполагаемого подхода LTM**:
> 
> | **Аспект** | **Стандартные LLM** (сейчас) | **Large Think Model** (гипотеза) |
> |----------------------------|--------------------------------------------------------------|---------------------------------------------------------|
> | **Типы данных** | Массовый веб-текст, книги, код, диалоги – без спец. организации по ролям. Сцены/конфликты лишь имплицитно в корпусе. | Разнообразные данные + специально сгенерированные **«сценарии мышления»**: многотуровые диалоги агентов, карты знаний, возможно, игровые или симуляционные логи с явными целями и конфликтами. Реальные тексты тоже важны, но дополняются **структурированными эпизодами**. |
> | **Структура хранения** | Плоские текстовые последовательности, JSON используется лишь для разметки инструкций/диалогов, но сконвертирован в текст при подаче【23†L95-L103】. Знание неявно хранится в весах модели. | **Графовые и многоуровневые представления:** хранение знаний как граф ассоциаций, гипер-сеть акторов【4†L109-L117】, возможно, отдельная долговременная память (база фактов). Данные могут храниться в виде, близком к базе знаний (узлы и связи) + описания в тексте. Модель получает не просто токен-последовательность, а, напр., *«Карта связей: А связан с B, B конфликтует с C…»* или через специальный модуль памяти. |
> | **Подача данных** | Случайное перемешивание, модель видит разношёрстные фрагменты подряд. Минимум curriculum (разве что длина контекста увеличивается постепенно)【17†L458-L466】. Обучение проходит фазами: предобучение, затем одно-два дообучения (SFT, RLHF)【15†L99-L107】, но без сложного сценарного разбиения. | **Curriculum и сценарии в обучении:** сначала обучить базовые понятия на фактах (как LLM), затем **поэтапно** обучать мышлению – давать сначала простые сцены (2 актёра, один конфликт), потом сложнее (группа, долгосрочные последствия). Возможна **группировка примеров по темам** (как в SPLiCe【19†L42-L49】, но расширенно: группировка по сюжету, по участникам). Т.е. LTM вероятно будет обучаться в несколько стадий с возрастающей сложностью ситуаций – **настоящее curriculum learning по содержанию**, а не только техническое. |
> | **Моделирование ролей и агентов** | В данных LLM отсутствует понятие постоянного агента: модель читает диалоги разных персонажей, но не имеет своей личности. Роли присутствуют лишь как подписи («Пользователь:… Ассистент:…») и переменные контекста. | LTM вводит **неявных (или явных) агентов** в само обучение. Напр., данные могут содержать взаимодействия множества условных “агентов мыслящей системы”, включая саму модель. Модель учится **отождествлять себя** с одним из агентов, вести внутренний монолог, распознавать личности других. Это требует специально помеченных данных, где в тексте прослеживается, что думает агент о себе и о других. |
> | **Эмоции и мотивация** | LLM не имеют встроенных эмоций или мотиваций, они стараются быть универсальными ответчиками. Эмоциональная окраска появляется только при стиле ответа (если пользователь просит). Обучение не направлено на закрепление эмоций или желаний. | LTM, чтобы имитировать мышление, может включать **эмоциональные компоненты**: данные, где решения принимаются под влиянием эмоциональных состояний, или разные личности с разными ценностями. Например, сценарии “дилемм”, требующие выбора. Модель будет учиться, что решения – не только логика, но и эмоции. Для этого пригодятся данные с пометками эмоций, а также механизм *reinforcement* (награды) за достижение определённых эмоционально-социальных исходов (например, гармония vs конфликт в группе агентов). |
> | **Долгосрочная память** | У LLM знание в весах, контекст ограничен. Есть внешнее подключение (через поиск), но не интегрировано в саму модель. При длительном диалоге модель забывает детали начала (если выходят за окно). | LTM предполагает **встроенную долговременную память**. Это значит, на этапе обучения модель осваивает методы хранения и извлечения информации вне узкого контекста. Возможно, LTM будет периодически повторять ранее пройденное (rehearsal) или поддерживать **векторную базу** внутри, обращаясь к ней. Обучающие данные могут симулировать *«память»:* например, давать задачи, где нужно вспомнить что-то из очень раннего эпизода – и награждать модель, если вспомнила. Таким образом, LTM унаследует от LLM фактические знания, но обретёт механизм их структурирования и поддержки актуальности. |
> 
> Стоит подчеркнуть: **современные подходы ограничивают “мышление” ИИ тем, что фактически учат его лишь продолжать тексты**, без явного понятия о целях, планах, самостоятельном диалоге. Это было достаточным для многих целей (генерация связанного текста, ответ на вопросы), но недостаточно для AGI-подобного мышления, где требуются саморефлексия, планирование, постоянство личности. LTM пытается восполнить этот пробел. 
> 
> При этом **нельзя полностью отвергать наработки LLM** – они достигли больших высот. Скорее, проекты вроде LTM должны **сочетать лучшее от двух подходов**: колоссальную информированность и языковую гибкость LLM *+* структурированность и контекстуальность, присущую когнитивным архитектурам. Возможно, первым шагом станут гибридные системы: скажем, LLM, дополненная модулем памяти или модулем рассуждения (так называемые *cognitive prostheses*). Но в перспективе LTM может потребовать новую формулу обучения с новыми видами данных, в связи с чем нынешние практики – лишь отправная точка и источник уроков, **что использовать, а от чего уйти**. 
> 
> **Заключение:** Современные датасеты для обучения ИИ ориентированы на массовость и простоту представления – они дают моделям огромный объем знаний, но в **неструктурированном виде**, что ограничивает способность к глубокому “мышлению”. Идея LTM бросает вызов этому статус-кво: чтобы модель действительно *мыслила*, нужны данные, которые учат структурам мысли – сценариям, концептам, отношениям, самоосознанию. Таких данных ещё предстоит создать, но уроки LLM не пропадут зря. Уже сейчас исследования, такие как Meta LCM с концептами【3†L181-L189】【3†L192-L200】, или эксперименты с семантической упаковкой контекста【19†L42-L49】【19†L78-L86】, показывают пути к улучшению. LTM может взять на вооружение **масштаб и разнообразие** LLM-подхода, добавив к ним **структуру и последовательность, свойственные человеческому опыту**. Таким образом, ограничения нынешних подходов – это подсказки, в каком направлении двигаться: от последовательности символов к последовательности мыслей. 
> 
> **Sources:** 
> 
> 1. Alan D. Thompson, *“What’s in my AI? Datasets Used to Train GPT-3 and others”* – Analysis of GPT-3 and The Pile dataset composition【13†L333-L342】【13†L349-L358】. 
> 2. Zco AI Blog, *“The 4 Stages of Training LLMs”* – on data preparation from web, books, code, etc., and need for filtering/structuring【15†L112-L120】【15†L129-L137】. 
> 3. MosaicML, *“Sequence Length Warmup”* – curriculum by increasing sequence length, reducing training difficulty initially【17†L458-L467】【17†L529-L538】. 
> 4. Staniszewski et al., *“Structured Packing for Long Contexts (SPLiCe)”*, ICML 2023 – grouping semantically related documents in one context to improve long-range understanding【19†L42-L49】【19†L78-L86】. 
> 5. Николай Евгращенков, *конспект по Large Concept Models (Meta AI)* – сравнение LLM (token-based) vs. LCM (concept-based) и их преимущества【3†L181-L189】【3†L192-L200】. 
> 6. Дискуссия на Giz.ai, *“LCM vs LTM”* – упоминание Latent Tree Models как иерархического представления данных и их ограниченного применения【4†L23-L30】. 
> 7. Habr, *“Делаем датасеты для тонкой настройки LLM”* – описание структуры инструкционных датасетов (instruction + response)【23†L95-L103】, формат #instruction/#output【22†L35-L38】. 
> 8. Прочие: материалы по PersonaChat【22†L35-L38】, RLHF, Empathetic dialogues, etc., для обоснования идей ролей и эмоций в обучении. (See references in text)
<!-- UID: 3d4f9a57-4580-4c79-a847-22dbfb040aa8 -->