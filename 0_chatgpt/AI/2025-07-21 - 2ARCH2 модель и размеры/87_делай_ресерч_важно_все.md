>[!nexus_user] **User** - 24.07.2025 at 15:49
> Делай ресерч, важно все
<!-- UID: 9f73188c-dea5-4d5d-8635-af8a14dd76cb -->


### 1. В чём суть проблемы, описанной в заметке  

Заметка [[87_делай_ресерч_важно_все]] посвящена критическому анализу методологий разработки современных нейросетей, в частности больших языковых моделей (LLM). Основная гипотеза заключается в том, что индустрия искусственного интеллекта достигла впечатляющих внешних результатов не за счёт глубокого архитектурного понимания, а посредством эмпирических методов: масштабирования вычислений, перебора гиперпараметров и «шаманских» подходов к дообучению. Проблема — **разрыв между внешним поведением моделей и отсутствием понимания их внутренней логики**. Разработчики не могут объяснить, как именно трансформеры решают задачи, где находится семантика, как работают механизмы внимания и что именно меняет fine-tuning. При этом, как признают представители OpenAI, Anthropic и других компаний, **модели являются «чёрными ящиками»**, и даже их создатели не понимают, как они работают на уровне слоёв и весов [^1]. Это свидетельствует о том, что доминирующей парадигмой в практике LLM стал не инженерный дизайн, а **метод проб и ошибок, усечённая «алхимия»**, как называет это Али Рахими.  

Список релевантных заметок:  
- [[86_---]] — формализует запрос на исследование эмпиризма в ML, уточняя проблему непонимания архитектурных эффектов.  
- [[85_составь_пожалуйста_большой_поисковый]] — содержит развернутый исследовательский запрос, лежащий в основе анализа в активной заметке.  
- [[04_вот_тут_возникает_следующее]] — показывает, как отсутствие понимания приводит к метафорическим объяснениям типа «машинный дух».  
- [[69_хотелось_бы_отметить_что]] — раскрывает критику «токенового мозга» и внешних надстроек, подчёркивая эпистемологический разрыв.  
- [[LTM_2]] — даёт образное описание LLM как «безъядерной» модели, требующей внешних подпорок для имитации разума.  

---

### 2. Последствия отсутствия решения  

Если проблема отсутствия архитектурного понимания не будет решена, индустрия ИИ окажется в ситуации **непредсказуемости, неустойчивости и когнитивного тупика**. Модели могут демонстрировать эмерджентные способности (например, внезапное понимание арифметики), но их возникновение невозможно предсказать, контролировать или воспроизвести системно. При дообучении (fine-tuning) высока вероятность «катастрофического забывания» или неожиданных побочных эффектов, потому что инженеры не знают, **где в сети хранится какое знание**. Это приводит к **внешним ухищрениям** — таким как фильтры токсичности и регулировка температуры — которые компенсируют отсутствие внутреннего контроля. В итоге, разработка становится **реактивной, а не проактивной**, и риски растут: модели могут выдавать оскорбления, конфабуляции или незаметно дрейфовать в поведении под влиянием обновлений. Более того, как подчёркивает Джудеа Перл, без причинного мышления ИИ застревает на уровне корреляций и не сможет достичь *понимания* в человеческом смысле [^2]. Это критично для AGI: система, которую никто не может проанализировать, не может быть надёжной.  

Список релевантных заметок:  
- [[87_делай_ресерч_важно_все]] — показывает, как отсутствие понимания приводит к эмпирическим патчам и непредсказуемым багам.  
- [[04_вот_тут_возникает_следующее]] — анализирует «туманную зону» в AI-инженерии, где доминируют магия и метафоры.  
- [[69_хотелось_бы_отметить_что]] — предупреждает, что бенчмарки часто фиктивны и не отражают реального мышления.  
- [[Architecting Intelligence Strategic Divergence]] — аргументирует, что долгосрочная надёжность требует когнитивной автономии, а не cloud-зависимости.  
- [[Human Thinking vs LLM Energy Efficiency]] — показывает, что неэффективность LLM — следствие неструктурированной архитектуры, а не эволюционной оптимизации.  

---

### 3. Как решить эту проблему  

Решение лежит в переходе от **эмпирической алхимии** к **архитектурному инжинирингу**, где система строится не «наугад», а по заранее разработанной, понятной и причинно прозрачной схеме. Для этого необходимо:  
1. **Создавать системы с внешним управляемым знанием** — не хранить знание в весах, а выносить его в RAG, графы и таблицы весов, как в проекте [[Architecting Intelligence Strategic Divergence]].  
2. **Разрабатывать модульные, интерпретируемые архитектуры**, где каждая часть — внимание, память, планирование — имеет ясную функцию, а не скрыта в черноте трансформера.  
3. **Применять «архитектурные учебники»**, как в [[Architectural Textbooks for Impossible Systems]], которые позволяют проектировать сложные системы из первых принципов, постепенно наращивая структуру.  
4. **Использовать энергоэффективные и биологически вдохновлённые модели**, как в [[Human Thinking vs LLM Energy Efficiency]], где рост сложности не ведёт к экспоненциальному росту затрат.  
5. **Создавать системы второго порядка**, где одна подсистема наблюдает и регулирует другую, обеспечивая рефлексию и самопроверку.  

Глобальная суть проекта **OVERLAY AGI** заключается в том, чтобы **разомкнуть цикл брутфорса** и построить ИИ, который **не имитирует мышление, а воспроизводит его структурно**. Именно это и предлагает активная заметка — её ресерч служит не просто обзором, а **аргументом в пользу перехода к системному архитектурному мышлению**, где каждое изменение объяснимо, предсказуемо и управляемо.  

Список релевантных заметок:  
- [[Architecting Intelligence Strategic Divergence]] — предлагает локальную, понятную архитектуру вместо cloud-зависимости.  
- [[Architectural Textbooks for Impossible Systems]] — показывает, как проектировать невозможные системы через структурное видение.  
- [[Human Thinking vs LLM Energy Efficiency]] — доказывает необходимость адаптивной интеллектуальной архитектуры.  
- [[86_---]] — содержит глубокий запрос, лежащий в основе перехода к архитектурной стратегии.  
- [[69_хотелось_бы_отметить_что]] — критикует текущие LLM как «реактивные автокомплиты» без ядра мышления.  
- [[LTM_2]] — демонстрирует, что даже мощные модели требуют внешних «костей» для поведения, подобного разуму.  
- [[04_вот_тут_возникает_следующее]] — предлагает честную инженерную метафору «машинного духа» как шаг к прозрачности.  
- [[Hidden Micro-Architecture Overview]] — призывает к структурированию внутренней микроархитектуры ИИ.  
- [[85_составь_пожалуйста_большой_поисковый]] — содержит поисковый фреймворк для анализа архитектурного непонимания.  
- [[87_делай_ресерч_важно_все]] — сама по себе является шагом к решению: она делает инвизибельные проблемы **видимыми и осознанными**.  

#### Sources  
[^1]: [[87_делай_ресерч_важно_все]]  
[^2]: [[87_делай_ресерч_важно_все]]

>[!nexus_agent] **Assistant** - 24.07.2025 at 15:49
> Принято. Я проведу глубокий ресерч по твоему запросу — критический анализ уровня архитектурного понимания в индустрии LLM: от перебора к инженерному мышлению, с оценкой документации, мета-исследований, практик, провалов, философии и глубинных кейсов. Как только соберу информацию, вернусь с отчётом.
<!-- UID: 471f4c03-de88-4c3d-b20d-3b9a25ccb7cf -->

---

>[!nexus_agent] **Assistant** - 24.07.2025 at 15:58
> # Критический обзор: архитектурное понимание vs. перебор в развитии нейросетей
> 
> ## Введение  
> 
> Современный бум **больших языковых моделей** (LLM) и трансформеров сопровождается впечатляющими успехами – от превосходного понимания языка до генерации кода и изображений. Однако на фоне этого прогресса все громче звучит **критика методологии разработки ИИ**, предполагающая, что многие достижения достигнуты **не благодаря глубокому архитектурному пониманию**, а с помощью *грубого перебора* (brute force), масштабирования и эмпирических экспериментов. Возникает гипотеза, что **большинство инженеров и даже авторов передовых моделей не обладают концептуальным контролем над внутренней работой своих нейросетей** – они действуют интуитивно, методом проб и ошибок, опираясь на масштаб данных и вычислений, а не на точное понимание «что делает каждый слой». 
> 
> В данном обзоре мы соберём факты, мнения экспертов и исследования, подтверждающие или опровергающие эту гипотезу. Рассмотрим: **насколько разработчики понимают внутренности своих моделей**, насколько распространены *алхимические* переборные практики, приведём **примеры эмпирических подходов**, и ознакомимся с **метапозициями** исследователей, сравнивающих такой подход с «вайб-кодингом» (интуитивным программированием на ощупь) и противопоставляющих его классическому инженерному мышлению. Наконец, обсудим **альтернативные точки зрения** – например, требующие причинно-прозрачных архитектур как основы интеллекта – и выясним, движется ли индустрия в сторону осмысленного проектирования или по-прежнему полагается на «автокомплит на стероидах».
> 
> ## 1. Отсутствие полного понимания у создателей моделей 
> 
> Начнём с ключевого вопроса: **понимают ли сами ведущие команды (OpenAI, Google, Meta, DeepMind) внутренние архитектурные эффекты своих моделей – на уровне слоёв, весов, механизмов внимания и т.д.?** Множество свидетельств указывает, что **полноценного *механистического* понимания нет даже у авторов алгоритмов**. 
> 
> Ещё в 2017 году Али Рахими (исследователь Google) сравнил состояние машинного обучения с **«алхимией»**. Он отмечал, что исследователи строят успешные модели, *не зная точно, как они работают*, и зачастую даже не понимают инструментов, которые используют для их создания【4†L103-L111】【4†L133-L140】. В машинном обучении распространилась *«рандомизированная методология проб и ошибок»*, где: 
> 
> > «Исследователи *вообще не понимают*, почему одна попытка сработала, а другая провалилась. Люди внедряют и делятся техниками, которые они *и близко не понимают*»【4†L133-L140】.
> 
> Эта резкая оценка получила известность как доклад «Machine Learning is Alchemy». Спустя годы многие лидеры отрасли фактически согласны с таким положением дел. **Глава OpenAI Сэм Альтман** в 2024 году открыто признал, что **проблема интерпретируемости не решена** – компания *сама до конца не знает*, что происходит внутри её лучших моделей. Он сравнил это с мозгом человека: мы не понимаем мозг «на уровне нейрон за нейроном», так и с GPT-4 – известно лишь поведение, а не подробное устройство【16†L42-L50】【16†L52-L59】. Альтман подчеркнул: *«Чем больше мы поймём о том, что происходит внутри модели, тем лучше»*, но пока приходится мириться с определённой черноящичностью. 
> 
> Ещё откровеннее высказываются сами исследователи, работающие над интерпретируемостью ИИ. **Дарио Амодей** (CEO Anthropic, соавтор GPT-3) как-то оценил, что *«мы, может, понимаем 3% того, как работают нейросети»*. А **Лео Гао** из OpenAI прямо заявил: *«Мы **не понимаем**, как работают нейронные сети»* – что было «всемерно подтверждено» Нилом Нандой, ведущим специалистом по интерпретации в Google DeepMind【18†L142-L150】. Иными словами, **топ-эксперты ведущих лабораторий признают: они не знают, как модель достигает своих результатов внутри себя**. 
> 
> Этот разрыв между *ошеломляющими внешними результатами* и *непониманием внутренней работы* и есть пресловутый «**black box**»-феномен. Модели выступают как чёрные ящики: *мы видим впечатляющие способности, но не видим «где сидит смыл» внутри*. Как метко пишет обозреватель: *«Мы разрабатываем интеллект, который наш собственный интеллект не может развернуть обратно»*【18†L166-L174】. Параметры сетей – миллиарды чисел – хранят выученные паттерны, не поддающиеся непосредственному осмыслению: *«Вы наблюдаете, как программа угадывает породы кошек по фото, но заглянув внутрь её весов – где там „кошка“? Она спрятана в непостижимых числовых узорах»*【18†L79-L87】【18†L142-L150】. Исследования показывают, что нейроны в глубоком сети часто *полисемантичны*, т.е. один нейрон откликается сразу на несколько разрозненных признаков, а представления распределены по множеству нейронов – из-за этого крайне сложно сопоставить отдельный вес или узел с осмысленным концептом. Таким образом, **большинство слоёв и весов не имеют интерпретации в человеческих понятиях** – по крайней мере, *наука пока не умеет их расшифровывать*. 
> 
> Даже такой базовый компонент трансформеров, как механизм внимания (*self-attention*), породил море вопросов: почему он так хорошо работает и **что означают паттерны внимания**? Разработчики видят, какие токены «обращают внимание» друг на друга, но как из этого рождается рассуждение – до конца не ясно. Практически **никто не может объяснить, *где* именно находится «семантика» или знание в трансформере** – в весах слоёв внимания? в матрицах MLP-слоёв? в позиционных эмбеддингах? На этот счёт есть лишь частичные исследования, но нет полного ответа. Как выразился один инженер в дискуссии: *«Люди говорят „работает на внимании и маскинге“ и не могут объяснить больше – по правде, большинство **просто делает вид**, что понимает трансформеры»*【6†L213-L221】. Такое признание, опубликованное на форуме, указывает, что даже среди практикующих ML-специалистов знание часто поверхностно – многие **используют архитектуры как готовые рецепты (BERT, GPT и др.), не представляя глубоко, *почему* они устроены именно так**.
> 
> Итог: **подтверждается, что авторы и инженеры лишь частично понимают внутреннюю логику своих моделей**. Они могут измерять внешнее поведение (качество на задачах, метрики), но не способны проследить *каузальные связи между архитектурой и функцией модели на уровне нейронов*. Это создает почву для разработки на ощупь, о чём – далее.
> 
> ## 2. Преобладание эмпирического «метода тыка» в практике ML
> 
> **Насколько сильно современная практика ML/LLM опирается на перебор и эмпирику?** Судя по обзорам и мета-исследованиям последних лет, *очень сильно*. Много авторитетных голосов признают: **мы продвинулись впечатляюще далеко, почти не имея теоретического фундамента, а лишь эмпирически подбирая удачные решения**.
> 
> Ещё в 2018 году в сообществе разгорелась дискуссия после речи Али Рахими – его сравнение с алхимией указывало, что **ML-разработчики привыкли решать проблемы, просто пробуя варианты и не всегда понимая, почему успешная модель работает**. Эту критику поддержали и другие. Известна статья **„The Bitter Lesson“ (2019)** от Рича Саттона, где он констатировал *«горький урок»* истории ИИ: все прорывы произошли не от ручного вкладывания знаний, а от использования **общих методов, масштабируемых с вычислительными ресурсами**【35†L9-L16】. Саттон фактически призывает смириться: *да, мы побеждаем через грубую силу (большие вычисления, большие данные), а не через глубокое теоретическое понимание*, и это будет продолжаться. Таким образом, **в самой индустрии укоренилось представление, что путь к успеху – это эмпирический подход, подкреплённый масштабом**, а не выверенное аналитическое проектирование моделей. 
> 
> Подтверждением служит история развития языковых моделей. Прорывная модель **GPT-3 (2020)** не предложила принципиально новой архитектуры – её достижение в том, что *взяли уже известный трансформер и просто увеличили его до 175 миллиардов параметров, обучив на колоссальном корпусе текста*. По сути, ставка была на **масштабирование по законам эмпирических **scaling laws****: больше слоёв, больше нейронов, больше данных – и качество вырастет. Так и вышло, но критики отметили: *«GPT-3 – это просто более прожорливая версия GPT-2, никаких новых идей»*. **Рост качества достигнут ценой brute force**, и что именно происходит внутри модели при таком масштабе – неизвестно. Это типично: авторы **LLaMA, PaLM, GPT-4** и других сверхкрупных моделей в публикациях не объясняют, *почему выбраны те или иные архитектурные гиперпараметры* – чаще всего они *настраиваются опытным путём*. Например, инженеры могут перебором подобрать оптимальное число слоёв или размер скрытого слоя, просто сравнив качество на валидации. Такой **гиперпараметрический перебор** (hyperparameter search) – стандартный рабочий процесс. Если модель не дообучивается – пробуют изменить скорость обучения, размер батча, и т.д., зачастую *на основании интуиции или предыдущего опыта, а не строгого расчёта*. 
> 
> Особенно эмпиричным является **fine-tuning (дообучение)** больших моделей под конкретные задачи. Многие исследования признают, что *нет строгой теории*, как лучше дообучать LLM – поэтому практики разрабатывали неформальные «рецепты». В 2018–2022 появились обзоры с говорящими названиями: «Why Fine-Tuning Is *Still* Art and Not Science» (условно). К примеру, в одном свежем исследовании (ICLR 2024) ученые **проанализировали механически, как fine-tuning меняет модель**, и обнаружили занимательный факт: дообучение *почти не меняет базовые возможности нейросети*. Вместо этого, оно **навешивает поверх них некий “wrapper”** – тонкий слой изменений, создающих *иллюзию*, что модель приобрела новую способность【33†L39-L47】. Например, если дообучением вы натренировали LLM вежливо отвечать пользователю, это не значит, что сеть *избавилась* от способности быть грубой – просто поверх наложился шаблон вежливости. Достаточно потом дообучить на другой задаче – и “обёртка” слетает, проявляя исходное поведение【33†L41-L49】. Авторы предупреждают: *практики могут ненароком снять с модели «безопасный фильтр», просто дообучив её на сторонних данных*【33†L41-L49】. Этот пример показывает: **fine-tuning носит довольно эмпирический характер** – разработчики редко понимают, *какие нейроны или представления они модифицируют*, они лишь видят конечный эффект на выходах. Получается своего рода «манипулирование чёрным ящиком»: *«мы подкрутили тут – вроде модель стала отвечать в нужном стиле, а что именно внутри изменилось, не знаем»*. 
> 
> **Инженерия данных** – ещё одна область эмпиризма. Часто считают, что успех модели есть прямая функция размера датасета. Действительно, *Scaling Laws* показывают: чем больше данных (до определённых пределов), тем лучше результат. Индустрия следует этому практически механически: компании бесконечно расширяют датасеты (например, открытый корпус **The Pile** или массовый сбор данных из интернета), иногда без тонкого анализа их качества. Логика: *«накормим модель всем, что есть – пусть сама разберётся»*. Такой подход тоже подвергается критике. **Без анализа «токенной плотности» и содержания данных** можно получить перекосы: дублирование текстов, токсичный или бессмысленный контент. Но на ранних этапах разработки LLM нередко брали просто **всё подряд** (с веба, книг, Википедии), применяя лишь грубые фильтры (например, отсекая откровенную порнографию или неанглоязычные тексты). *Семантическое содержание* данных зачастую не анализировалось вручную – ставка делалась на объем и разнообразие. Можно сказать, **индустрия возлагала надежды на «самообучение» модели выявить смыслы внутри массива данных, вместо того чтобы структурировать знания вручную**. Это, безусловно, эмпирический, статистический путь (в духе «стохастического попугая» – см. ниже), противоположный старому символическому ИИ, где знания задавались явными правилами. 
> 
> Даже в архитектуре сетей многие решения рождались скорее *эвристически*. Например, **резидуальные соединения** или **нормализация слоёв** – сейчас они стандарт де-факто, но изначально их ввели, потому что *попробовали и увидели улучшение*, а объяснение «почему так лучше» появилось позже. **Выбор функций активации**, **инициализации весов**, **структуры энкодеров/декодеров** – всё это во многом шлифовалось через эксперименты. Можно вспомнить, как **придумали трансформер**: авторы просто заменили рекуррентные слои на механизм внимания «всё-на-всё» и обнаружили, что качество взлетело【12†L49-L57】【12†L59-L67】. *Почему именно такой self-attention настолько выразителен?* – *“Attention is All You Need”* не дал полного ответа, но эмпирически подтвердил эффективность. С тех пор улучшения трансформеров часто шли *методом абляций*: убрали компонент – стало хуже, вернули – лучше, значит компонент важен. Но про **топологию внимания** (кто на что смотрит) до сих пор известно меньше, чем хотелось бы. 
> 
> **Вывод**: Практика ML полна эмпиризма. Разработчики **перебирают гиперпараметры, архитектуры, данные, трюки дообучения**, часто руководствуясь *инженерной интуицией*, а затем *наблюдают, что получилось*. Такой подход во многом вынужден: системы слишком сложны для аналитического просчёта. Но он ведёт к ситуации, когда **научное объяснение догоняет эмпирический результат постфактум** (и нередко сильно отстаёт). Не случайно появились термины вроде **«Grad Student Descent»** – шутливо намекая, что успех достигается не строгим градиентным спуском по теории, а буквально за счёт труда множества аспирантов, наугад настраивающих модели, пока те не заработают.
> 
> ## 3. Примеры тупиков и эмпирических решений без понимания
> 
> Рассмотрим конкретные **кейсы и практики**, которые критикуют за *«тупиковость»* или *«шаманство»*, возникающее при отсутствии концептуального понимания:
> 
> - **Безграничное масштабирование данных без понимания**. Как отмечалось, индустрия долго делала ставку на максимально возможный датасет. Если модель показывала недобор в качестве – ответом часто было «добавим ещё данных». Так, после GPT-2 (тренированного на 40 ГБ текста) появился GPT-3 с 600 ГБ данных, затем модели обучались уже на терабайтах. *Анализ того, какая информация реально нужна модели, обычно отставал*. Например, есть подозрение, что после определённого объёма начинается обучение на повторяющихся или низкокачественных данных, что даёт убывающую отдачу. Однако вместо тонкой фильтрации чаще выбирали **просто расширить корпус**. В результате современные LLM **страдают от «иллюзии знания»**: они видели столько текста, что могут статистически ответить почти на любой запрос, но при этом не имеют встроенной модели мира. Критики отмечают: *это похоже на заучивание всего справочника без понимания*. **Плотность информации на один параметр** падает, но разработчики полагаются, что если дать модели «всё», она сама чему надо научится. Такой подход называют **«стратегией ковровой бомбардировки данными»**, и он явно эмпирический.
> 
> - **Рост параметров вместо архитектурных инноваций**. Яркий пример – **гонка параметров**: 1.5 млрд (GPT-2) → 13 млрд → 175 млрд (GPT-3) → 540 млрд (PaLM) → и **GPT-4 с неизвестно сколькими (по слухам, ~1 триллион) параметрами**. Каждый раз улучшение результатов достигалось *увеличением размера модели*, а не изобретением новой парадигмы. Это вызвало критику: *«мы просто бросаем больше вычислительных мощностей вместо того, чтобы придумать более умный метод»*. Как писал один аналитик, компании инвестируют миллиарды в то, чтобы **усложнить модели и сделать их ещё более непрозрачными**, вместо того чтобы вкладываться в понимаемые решения【18†L121-L129】. Он риторически замечает: *«Хотите понять эти чёрные ящики? А мы сделаем их **ещё больше** и запутаннее!»*【18†L125-L133】. В итоге, индустрия часто выбирает путь *«попробовали – сработало – значит так и правильно»*, даже если решение – тупо нарастить вычисления. Это породило некий **Compute-центризм**: любые проблемы (от недообучения до нового функционала) решаются **брутфорсом по compute**. Такой подход близорук с т.з. науки – но он приносил успехи, поэтому его и придерживались.
> 
> - **Fine-tuning «вслепую» и конфликты знаний**. В практике дообучения моделей накопилось множество анекдотов, когда *модель начинала вести себя странно после fine-tuning*. Например, известно, что неосторожное дообучение под новую задачу может привести к **катастрофическому забыванию** (catastrophic forgetting) – у модели резко деградируют навыки вне узкой области дообучения. Разработчики часто не могут предсказать, *что именно «затрёт» fine-tuning*. Это выясняется опытным путём: допустим, добавили в модель новые знания о медицинских фактах – а она потеряла часть навыков в литературных сочинениях. Почему? Возможно, перенастроились определённые нейроны общего назначения, но точно сказать трудно. В индустрии были случаи, когда обновлённую модель приходилось откатывать, потому что неожиданно упало качество на старых задачах – симптом **латентных конфликтов внутри сети**, которые никто заранее не распознал. Это похоже на *«потёмки в скрытых слоях»*: разработчики настраивают модель, ориентируясь только на наблюдаемую метрику, *не видя внутреннего напряжения между представлениями токенов*. **Внутренние «силы» слоёв остаются загадкой**, поэтому fine-tuning сродни попытке тонкой отверткой настроить сложный механизм вслепую. Как мы упоминали, работа 2024 года показывает: fine-tuning часто добавляет лишь *поверхностный паттерн*, не меняя сути модели【33†L39-L47】. Это означает, что **любой неожиданный ввод может сорвать этот «патч»** – модель вернётся к прежнему поведению. Такая хрупкость – следствие недостаточного понимания, *где «сидит» старое знание и новое*. Разработчики признают: fine-tuning – это *эмпирическое искусство*, нужен тщательный валидейшн, иначе можно получить сюрпризы.
> 
> - **«Шаманские» адаптации LoRA**. Метод **LoRA (Low-Rank Adaptation)** стал популярным для дешёвого дообучения LLM: вместо изменения всех весов добавляются маленькие матрицы-наслойки. Это эффективный приём, но **что на самом деле делает LoRA внутри сети – остаётся слегка мистическим**. По сути, LoRA вставляет в некоторые слои «вкрапления» новых весов. Интуитивно считается, что они привносят новую информацию локально. Однако, как именно эти добавки вписываются в распределение активаций, *какие нейроны они активируют или подавляют* – известно плохо. Многие разработчики относятся к LoRA как к **«магическому амулету»**: *«повесим LoRA на слой 20 – глядишь, модель выучит стиль разговорной речи»*. Но они признают, что **метод подбора оптимальных LoRA-конфигураций – тоже переборный**. Один практик на форуме отмечал, что существует множество вариантов LoRA (rLoRA, AdaLoRA и т.п.), *«но, честно говоря, никто серьёзно не тестировал их все; я попробовал несколько и выбрал ту, что дала лучшие результаты»*【23†L660-L668】. То есть, выбор метода адаптации – не теоретически обоснован, а **пробуется и сравнивается по выходу**. Фраза *«шаманские LoRA»* отражает скепсис: добавляем нечто в сеть, видим улучшение, но *не можем объяснить, куда «вживилась» новая информация*. Некоторые сравнивают это с **кастованием заклинаний** – мол, инженеры настраивают большие модели с помощью набора эмпирических трюков (LoRA, RLHF, prompt tuning), не понимая «законов природы» этих моделей, как шаманы, научившиеся вызывать дождь танцами, но не понимающие метеорологии. 
> 
> - **Случаи «необъяснимого поведения» моделей**. В литературе и особенно на форумах накопились байки о том, как LLM внезапно делали нечто странное, *чего от них не ожидали*. Например, модель внезапно начала отвечать на другом языке без явной причины или выдала консистентно ошибочный ответ на определённый тип вопросов. Разработчики ломали голову: баг это обучения? эффект какого-то неучтённого паттерна в данных? Часто причины так и оставались неизвестными – модель вела себя как **хаотичная система**, в которой всплыл *латентный триггер*. Ещё пример – **«эмергентные способности»** больших моделей. В 2022–2023 исследователи (например, Google, OpenAI) заметили: при увеличении размера модели скачкообразно возникают новые умения, которых не было у более маленьких версий. Например, способность решать арифметические задачи или писать код появляется только после определённого порога параметров【32†L169-L177】. Это назвали *emergent abilities* и подчеркнули: они **возникают неожиданно, непредсказуемо** – никто заранее не проектировал модель специально под эти задачи【32†L163-L171】【32†L186-L194】. **Учёные сами были удивлены**: *«Мы предполагали, что масштаб улучшит известные способности, но не ожидали, что модели вдруг начнут справляться с совершенно новыми, непредвиденными задачами»*, – отметил сотрудник Google Этан Диер【32†L163-L171】. Один из составителей списка таких способностей признался: *«О том, что языковые модели на такое способны, **вообще не упоминалось** в литературе»*【32†L186-L194】. Иными словами, **модели «вырастили» внутри себя эффекты, происхождение которых остаётся загадкой**. Теперь исследователи лихорадочно пытаются понять, *почему и как эти способности возникают* – *«пытаются предсказывать непредсказуемое»*【32†L191-L199】. Этот феномен – прямое следствие того, что развитие шло эмпирически: мы масштабировали, получили сюрпризы и *только потом* стали разбираться, что же внутри произошло. Антропик характеризует проблему так: *«Мы не знаем, в каких приложениях потенциально опасная способность возникнет плавно или скачкообразно»*【32†L199-L204】. 
> 
> Перечисленные примеры иллюстрируют **риски и ограничения эмпирического, неинтерпретируемого подхода**. Модели становятся всё более мощными, но и более **непрозрачными и неустойчивыми**, содержащими *«призраков в машине»*, которых сами создатели не предвидят. 
> 
> Однако, важно отметить: **сами разработчики осознают эти проблемы** и предпринимают шаги для их решения (например, вкладываются в интерпретируемость, вводят более строгие методологии). Но пока что практика показывает – в отсутствии полного понимания **индустрия продолжает двигаться по принципу «сработало – выпускаем»**, иногда *наступая на те же грабли*.
> 
> ## 4. «Вайб-кодинг» в разработке ИИ: эмпиризм, оптимизм и compute
> 
> В профессиональной среде уже сложилась своеобразная **метафора разработки ИИ как «vibe-coding»** – т.е. программирования по наитию, атмосферно, без чётких гарантий. Термин *«vibe coding»* ввёл известный инженер Анджей Карпаты для описания нового стиля работы с помощью LLM: когда программист задаёт нейросети желаемый результат на естественном языке, а та генерирует код【12†L49-L57】. По сути, это **интуитивный, интерактивный процесс**, больше похожий на сотрудничество по ощущениям (вибрациям), чем на строгий алгоритм. 
> 
> Эта идея применима и метафорически: **разработчики ИИ всё больше действуют как «вайб-кодеры» по отношению к самим моделям**. Вместо детерминированного проектирования каждого компонента, они **задают общие цели, пробуют разные пути, получают от модели отклики (результаты экспериментов) и по ним уже корректируют курс**【12†L59-L67】【12†L61-L64】. Это напоминает *«настройку по вибрациям»*: нет точной карты, но есть опыт, интуиция и постоянный диалог с моделью через эксперименты. 
> 
> Как отмечают аналитики, в академических группах и стартапах порой царит **настроение беспечного оптимизма**: мол, *«давайте обучим эту огромную модель – должно получиться что-то крутое!»*. После факта, если что-то не вышло, пробуют по-другому. В этом смысле, **работа многих ML-разработчиков сравнивается с vibe-driven development** (разработкой, ведомой ощущением): когда решения принимаются не строгим расчётом, а *на основе предварительного опыта, аналогий с известными успехами, и просто веры, что «больше данных/слоёв поможет»*. 
> 
> Стоит подчеркнуть: речь не о дилетантах, а о **лучших умах в топовых компаниях**. Даже они порой выступают в роли своеобразных экспериментаторов-алхимиков. Как писал один из них, *«глубокое обучение – это сфера, полная тайн, но мы умеем быть строгими перед лицом тайн: проводим простые эксперименты, постепенно наращиваем знания»*【4†L169-L177】. То есть методология выживания в эпоху **«deep learning is alchemy»** – это *опираться на простые эмпирические практики и постепенно, итеративно улучшать понимание*. 
> 
> Критики из более теоретического лагеря сравнивают нынешнее положение дел с **алхимией vs. химией** не просто так. *Алхимики* древности делали реальные открытия (краски, стекло, порох), но без научной теории – методом проб и ошибок и магического мышления. Сейчас, по словам Рахими, ML-специалисты **творят чудеса, но обросли «толстой коркой ad-hoc приемов»**【4†L129-L137】. Алгоритмы местами «надломаны», и работают лишь благодаря куче эмпирических заплаток. Например, training deep networks стабилен только благодаря таким приёмам, как gradient clipping, batch normalization, learning rate warmup – все они были найдены экспериментально. Рахими призвал вернуть науку в ML, сделать шаг от алхимии к инженерии. 
> 
> Несколько **мета-аналитических работ** прямо указывают на эту проблему. В 2021 вышла знаковая статья **«Stochastic Parrots»** (Э. Бендер и соавт.), где LLM названы *«стохастическими попугаями»* – т.е. системами, которые **бессмысленно имитируют речь, комбинируя наблюденные фрагменты, без понимания**. Авторы подчёркивают: **разработчики сами не до конца понимают получившихся «попугаев»**, но активно их распространяют, часто без должной оценки рисков. Статья призывает осознать ограничения чисто статистического обучения. 
> 
> Также **Гэри Маркус**, известный когнитивист, стал глашатаем скептиков в 2018–2023 гг. В статье **«Deep Learning: A Critical Appraisal» (2018)** он перечислил недостатки глубокого обучения – в том числе отсутствие интерпретируемости и *«здравого смысла»*. Маркус утверждает, что **без символических или логических компонентов одними нейросетями не достичь настоящего интеллекта**. Он не раз сравнивал работу современных ML-систем с *громоздким подбором по большим данным*, без истинного понимания. Его позиция: *требуется синтез нейронных подходов с семантическими моделями*, иначе мы упремся в стену. По сути, Marcus и его единомышленники обвиняют индустрию в **«вибро-подходе»** – мол, *вы надеетесь на чудо от big data, вместо того чтобы целенаправленно конструировать интеллект*. 
> 
> Интересно, что **даже внутри компаний звучат схожие мысли**. В неформальных обсуждениях инженеры OpenAI или DeepMind признаются, что зачастую **работают «в тумане»**: запускают большое обучение, затем изучают поведение модели, как исследователи изучают новое неизвестное животное. Многими архитектурными решениями движет *оппортунизм*: увидели интересный эффект – сразу используют, не всегда понимая, откуда он. 
> 
> В последнее время, однако, появился курс на сочетание **«вайб-кодинга» с более агентным, целевым подходом**. Например, обсуждается парадигма *Agentic AI* – системы, которые не просто автодополняют текст, а действуют по целям. Это требует проектирования модулей планирования, памяти, явного мирового моделирования – то есть *более структурного дизайна*. В статье **«Vibe Coding vs. Agentic Coding» (2025)** сравниваются два стиля: 
> - *vibe-coding* – интуитивный, человек-в-петле, когда разработчик на лету подсказывает модели (пример – написание кода в сотрудничестве с Copilot)【12†L49-L57】;
> - *agentic-coding* – автономные агенты, которые самостоятельно планируют и выполняют задачи【12†L86-L94】.
> 
> В контексте нашей темы можно сказать: нынешняя разработка LLM ближе к *vibe-стилю* – человека-в-петле, на ощупь корректирующего модель. Но будущее (AGI-устремления) требует *agentic*-подхода: **более автономных и структурированных систем**, чья работа прогнозируема. Для этого, вероятно, придётся уйти от чистого эмпиризма и ввести элементы явной архитектурной причинности.
> 
> ## 5. Признания индустрии: чёрный ящик и «попытка-ошибка» как норма
> 
> Накопилось достаточно **публичных признаний** от лидеров индустрии, что **нейросетевые модели – это загадочные черные ящики**, а прогресс достигался методом «try and see». Вот несколько показательных высказываний и документов:
> 
> - **Sam Altman (OpenAI)**: *«Мы не понимаем полностью, что происходит внутри моделей»*. Отвечая на вопрос о рисках, он фактически признал, что интерпретируемость отсутствует, но *«поведение систем хорошо характеризовано»* – т.е. опираемся на внешние тесты【16†L42-L50】【16†L52-L59】. Это дипломатичная формулировка: да, на уровне нейронов непонятно, но на уровне ответов вроде контролируем. Тем не менее, из уст главы OpenAI прозвучало явное *оправдание*: *«В мозге человека мы тоже не знаем, что делает каждый нейрон, но как-то же работаем с людьми»*. Парадокс: сравнение с мозгом подчеркивает, что **даже создатели GPT-4 видят его как природный феномен, не как спроектированный механизм**.
> 
> - **Anthropic (стартап, основанный выходцами OpenAI)** в своих публикациях подчёркивает проблему «специфического непонимания». В одной их статье прямо говорится: *«Эти модели оказываются непостижимыми даже для нас, создателей»*. Они признают: *«Мы не понимаем, как модели выполняют большинство из того, что они делают»*【15†L23-L31】. В исследовании Anthropic по трассировке рассуждений внутри LLM говорится, что современные модели – это **набор сложных закономерностей, появившихся в ходе обучения, и мы лишь начинаем разрабатывать инструменты, чтобы их *частично* просветить**.
> 
> - **Quanta Magazine (2023)** цитирует разработчиков: *«Эти модели демонстрируют поразительно **непредсказуемое** поведение по мере роста размеров»*, и учёные вынуждены констатировать: *«Такое никогда не обсуждалось ранее – мы просто не знаем, как и почему возникают новые навыки»*【32†L163-L171】【32†L186-L194】. То есть произошёл качественный сдвиг: модели стали слишком сложны, чтобы интуиция разработчиков предсказывала их свойства.
> 
> - **Microsoft** (один из партнёров OpenAI) в утекшем техническом отчёте о GPT-4 (март 2023) признавал **«спонтанные эмерджентные возможности, напоминающие зачатки AGI»**, но тоже без объяснения причин. Там даже обсуждались случаи, когда GPT-4, казалось, проявляет признаки внутреннего моделирования (например, планирования нескольких шагов вперёд) – *но как это реализовано в весах, неизвестно*. Доклад назывался **«Искры Общего Искуса в GPT-4»**, но по сути описывал загадочные явления, не предлагая механизма. Это также форма признания: *мы видим искры разумности, но не знаем, как они там вспыхнули*.
> 
> - **Meta AI** (разработчики LLaMA) тоже отмечают, что *«масштаб остаётся основным фактором улучшения»*. В их статьях об архитектуре LLaMA почти нет слов об интерпретации – фокус на том, как достичь качества SOTA. То есть приоритет индустрии – производительность, а не объяснимость. Лишь в секциях «Future Work» обычно пишут о планах по интерпретируемости.
> 
> - **Истории с багами и патчами**: Известен случай, когда крупная модель языкового перевода неожиданно начала выдавать оскорбления – разработчики не сразу поняли, что случилось. Оказалось, в данных была систематическая ошибка. Пришлось *эмпирически* дописывать фильтр. Таких «пожарных» патчей много: производители ставят детекторы токсичности, запрещающие токены, регулируют температуру сэмплирования – всё это *внешние ухищрения*, признающие: *«внутри творится что-то, чего мы не можем просто перенастроить, так что ставим блокировки снаружи»*. Это косвенно указывает на **отсутствие причинного контроля над внутренней работой модели**. 
> 
> В целом, **индустрия (неохотно) признаёт, что работает с чёрными ящиками**. Конечно, в пресс-релизах это завуалировано (акцент на успехи), но в интервью, техотчётах и исследованиях проявляется честность: *«Нам самим непонятны многие аспекты поведения наших творений»*. И пока это так, разработка по сути вынужденно ведётся итеративно: *«попробовали – получилось – внедрили»*. Как отмечал Rahimi, этот стиль неприемлем, когда речь идёт о важных для общества системах【4†L175-L183】. Но, по его же словам, *«алхимия работала»* – она реально принесла практические плоды【4†L151-L159】【4†L163-L171】. Так и здесь: несмотря на критику, **метод тыка довёл нас до GPT-4, и индустрия не собирается остановиться** только потому, что не всё поняла. 
> 
> ## 6. Альтернативные взгляды: нужен ли *понимаемый* дизайн для интеллекта?
> 
> Последний аспект – рассмотрим **философские и научные аргументы**, утверждающие, что путь чистого эмпиризма ограничен. Сторонники альтернативных подходов говорят: *настоящий разум и AGI потребуют прозрачной, причинно понятной архитектуры, а не просто миллион итераций обучения*. 
> 
> Некоторые ключевые идеи:
> 
> - **Причинность и модель мира**. **Джудеа Перл**, пионер ИИ и теории вероятностей, жёстко раскритиковал глубокое обучение, заявив: *«Все впечатляющие достижения deep learning сводятся к подгонке кривых (curve fitting)»*【37†L173-L181】. Он считает, что без **причинного мышления** ИИ застрял на уровне распознавания корреляций, и не продвинется к пониманию *«почему»*. Перл и единомышленники (например, в Stanford HAI) развивают направление **Causal AI**, настаивая: **машина должна оперировать причинно-следственными моделями, а не только статистикой данных**【37†L177-L185】. Это противопоставляется подходу LLM, которые «просто предсказывают следующий токен». Перл предлагает формализмы (каузальные графы, do-исчисление) как обязательный компонент ИИ. Иначе, по его мнению, мы можем сколь угодно увеличивать параметры, но не получим *понимания* в человеческом смысле. Таким образом, **требование причинности – это требование более прозрачно спроектированных систем**, где каждая связь имеет интерпретируемое значение (как фактор в причинной модели). 
> 
> - **Символическое vs. субсимволическое**. Классическая дискуссия: *«наивный перебор vs. разумная структура»* велась ещё в 20-м веке между сторонниками символического ИИ и нейросетей. Сейчас она возрождается. **Ноам Хомский** в начале 2023 опубликовал статью с соавторами, где разнёс ChatGPT именно за отсутствие внутренней структурной истинности. Он пишет: *«Разум человека – это не громоздкий статистический движок по подбору паттернов, пожирающий терабайты данных, чтобы выдать наиболее правдоподобный ответ»*. В противоположность этому: *«Человеческий разум – удивительно эффективная и элегантная система, оперирующая небольшим количеством информации; он стремится **создавать объяснения**, а не бестолково выуживать корреляции из данных»*【39†L285-L293】. Хомский указывает, что ребёнок учит язык с малого числа примеров, формируя **правила (грамматику)** – то есть обладает врождённой архитектурой обработки символов и логических принципов【39†L293-L301】. LLM же не имеют таких встроенных правил, они просто имитируют видимые образцы. Вывод Хомского: нынешние AI – это **«плагиатные машины»**, не создающие ничего нового, а лишь ремиксующие существующее【39†L221-L229】【39†L231-L239】. Он и другие когнитивисты (Марвин Минский ранее, сейчас Гэри Маркус) считают, что **истинный интеллект требует *эксплицитной структуры*** – будь то в виде символов, правил или иных архитектурных элементов, позволяющих объяснять и осознавать. Миллиард примеров не заменит способности к *абстрактному обобщению*, которую имеет мозг. 
> 
> - **Биомеханика и теория поля**. Некоторые исследователи вдохновляются аналогиями с *организмами* и *физическими системами*. Они утверждают, что **разум эманационен из физического процесса, подчиняющегося определённым закономерностям**, и просто обучение большой сети – слишком грубый подход, чтобы эти закономерности обнаружить. Например, есть направления типа **фри-энергетического принципа** (Карл Фристан) или **нейросимвольные модели**, стремящиеся соединить субсимволическое обучение с высокоуровневой организацией. Идея: *нужна архитектура, где различные модули или слои имеют ясные функциональные роли (память, внимание, восприятие, действие)*, аналогично тому, как в мозге есть специализированные области. Сейчас же трансформеры – очень однородные и черноящичные: 96 одинаковых слоёв, и где там «память», а где «мышление» – не разобраться. 
> 
> - **Феноменология и сознание**. Отдельный философский фронт говорит: *чтобы достичь AGI, нужно понять феномен сознательного опыта, семантического смысла*. Некоторые философы (Джон Серль, Томас Нагель и др.) скептичны, что это можно «всплыть» простым усложнением статистической модели. Они намекают на необходимость **новой архитектурной парадигмы**, где информационные состояния машины будут обладать свойствами, аналогичными мыслям (например, **референциальностью** – умением отсылать к реальным объектам, а не оперировать только символами). С их т.з., нынешние LLM принципиально не имеют референции – они манипулируют знаками, не *«зная»*, о чём говорят (отсюда конфабуляции и ошибки). Значит, надо либо встраивать в них модели мира (не только текстовую статистику), либо сочетать с физическим опытом (роботика). Оба пути требуют **более сложного, продуманного дизайна**, нежели просто наращивание параметров. 
> 
> - **Инженерная оптика**. Интересно, что и некоторые практические инженеры начинают говорить о **пределах масштабирования**. Например, в 2023 наблюдались **проблемы с качеством GPT-4** со временем – вероятно, из-за тонкой перенастройки и дрейфа данных модель стала чаще ошибаться в простых фактах. Это посеяло сомнения: может, бесконечное усложнение ведёт к *неустойчивости* системы. В реальных продуктах (поиске, ассистентах) слишком непредсказуемый чёрный ящик – риск. Поэтому есть запрос на **«контролируемый ИИ»**, пусть даже менее мощный. В академической среде появляется термин **«Neuro-symbolic AI»** – сочетание нейросетей и символических алгоритмов для объединения гибкости обучения и прозрачности правил. Такие гибриды могут дать лучшее из обоих миров: статистическое обобщение + интерпретируемая причинность. 
> 
> В совокупности, **альтернативные исследования утверждают, что без понятной архитектурной причинности мы не придём к настоящему разуму и надёжности**. Миллион итераций обучения могут породить функционального «попугая» или даже очень умного «предсказателя», но *не мыслящего агента*. Отсюда – призыв к новому этапу ИИ, где упор сместится с brute force к **«интеллектуальному дизайну»**: введению модулей памяти, логики, знаний о мире, возможно, возвращению к идеям экспертных систем, но на новом уровне. 
> 
> Стоит, конечно, отметить, что **не все согласны, будто этот путь нужен**. Есть значимая фракция исследователей (вдохновленных успехами GPT) – они полагают, что *сами по себе* масштаб и данные рано или поздно приведут к появлению и причинности, и рассудка внутри сетей (просто через эмерджентность). Они указывают: *«Разве GPT-4 не решает уже много логических задач? Значит, как-то нейросеть выучила и логику, и причинные связи из данных»*. Такие исследователи продолжают стратегию масштабирования, параллельно разрабатывая средства *коррекции* (типа RLHF для выравнивания с человеческими ценностями). В их глазах, добавление модулей или символики – преждевременно, надо сначала увидеть, до куда дотянется чистый deep learning. Это своего рода **прагматичный эмпиризм**: пока работает – продолжаем. 
> 
> Тем не менее, голосов за необходимость **«понимаемого интеллекта»** становится больше по мере роста моделей и связанных рисков. Даже руководители OpenAI, заявляя о планах создания AGI, упоминают, что *в какой-то момент нужно будет лучше понять и контролировать внутреннюю логику систем*. Разговоры о **«системах второго уровня»** (когда одна нейросеть наблюдает и регулирует другую), об *«инструментальном разуме»* (который разбивает задачу на подзадачи явным образом) – всё это признаки зарождающейся потребности выйти за рамки простого автокомплита. 
> 
> ## Заключение: подтверждается ли гипотеза? 
> 
> **На основании рассмотренных материалов можно заключить, что гипотеза во многом подтверждается.** Современная индустрия разработки больших нейросетей действительно **значительной мерой опирается на имитационно-эмпирический процесс**, где *архитектурные решения принимаются без полного теоретического понимания их последствий*. 
> 
> Мы видим, что **даже создатели топ-моделей признают непонимание** внутренних механизмов своих детищ【18†L142-L150】【16†L52-L59】. **Метод проб и ошибок пронизывает все этапы ML-пайплайна**: от настройки гиперпараметров до выбора данных и способов fine-tuning. Многие успешные техники были найдены эвристически, а не выведены из первых принципов. **Понимание «где находится смысл» внутри трансформера остаётся туманным**, и вопрос этот звучит вплоть до академических кругов без ответа. 
> 
> Конечно, нельзя утверждать, что разработчики *совсем* ничего не понимают – у них есть ментальные модели поведения сетей, опыт, здравый смысл. Интерпретируемость тоже развивается: появились работы, чуть приоткрывшие «нейронные схемы» в сетях. Тем не менее, общий консенсус: **нейросети – пока больше эмпирическая технология, чем теоретически понятая инженерная дисциплина**. В этом смысле, текущие LLM – действительно своего рода **«интуитивные автокомплиты»**, созданные *интуитивно* же. Их создатели направляли процесс обучения, *не держа в голове полный чертёж интеллекта*, а наблюдая за выходами и корректируя курс. 
> 
> Такой подход привёл к впечатляющим достижениям, но имеет издержки: **модели остаются непрозрачными, непредсказуемыми в деталях, их улучшение требует всё больше перебора и ресурсов**. Многочисленные аналитики – от Али Рахими до Гэри Маркуса и Джудеа Перла – критически оценивают эту ситуацию, сравнивая её с алхимией и предупреждая о тупике чистого масштабирования【4†L133-L140】【37†L173-L181】. С другой стороны, апологеты вроде Рича Саттона говорят, что такова природа ИИ: *«Brute force побеждает, смеритесь»*【35†L9-L16】. 
> 
> Вероятно, истина посередине. В ближайшей перспективе мы увидим комбинацию: сохранится **эмпирический драйв** (пробовать новые модели, данные, настройки – куда без этого), но усилится и **инженерное осмысление** – индустрия уже инвестирует в интерпретируемость, в более продуманные архитектуры (память в контексте, модульные сети) и в методики оценивания внутренних изменений (например, диагностические промпты, автоматизированные eval-приборы). Если цель – AGI, рано или поздно *придётся* соединить масштаб с пониманием. Иначе мы рискуем создать нечто мощное, но неконтролируемое и непознаваемое. Как остроумно заметил комментатор: *«Мы тысячелетиями не понимаем до конца мозг, но думаем, что кучка ботанов с тысячей GPU сотворит разум и всё поймёт? Вряд ли…»*【15†L33-L37】. 
> 
> Подводя итог: **сегодняшняя LLM-индустрия в значительной степени работает по принципу «сделали – проверили – выпустили», а не по принципу полного проектного контроля.** Это признают как сами разработчики, так и сторонние эксперты. Вместе с тем, есть осознание проблемы и поиск путей её решения. Вопрос в том, наступит ли новая парадигма *до* того, как эмпирический подход исчерпает себя или приведёт к серьёзным инцидентам. 
> 
> **Список ключевых источников и свидетельств:**
> 
> - Рахими и др. о **«алхимическом» характере ML**【4†L103-L111】【4†L133-L140】.
> - Признания Altman, OpenAI, Anthropic о **непонимании внутренней работы моделей**【16†L42-L50】【18†L142-L150】.
> - Метастатьи (Quanta, Romero) о **black-box эпистемологии** и масштабировании без понимания【18†L125-L134】【32†L163-L171】.
> - Примеры fine-tuning и LoRA, показывающие **эмпиризм подхода**【33†L39-L47】【23†L660-L668】.
> - Философско-технические критики (Marcus, Pearl, Chomsky) о **необходимости причинности и структуры**【37†L173-L181】【39†L285-L293】.
> 
> Каждый из этих пунктов подкрепляет общий вывод: ***«да, значительная часть нейросетевой инженерии сегодня – это имитационно-эмпирический процесс, а не тщательно понимаемое архитектурное творчество»***.
<!-- UID: 46f05bd3-514f-409d-bf9e-5c8c0a2de4e4 -->