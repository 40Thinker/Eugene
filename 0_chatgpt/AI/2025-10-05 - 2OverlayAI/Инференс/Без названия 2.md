А как сделать так, чтобы для пользователя было просто API, и он не знал что это не обычный чат, а там внутри и память подключена, и агенты и куча всего еще. Как такое делают?

# From Black Box to Cognitive Architecture: Building AI APIs That Hide Complexity

## TL;DR Summary

This article explores how to create intelligent API endpoints that appear as simple chat interfaces but internally orchestrate complex cognitive systems including memory management, agent routing, and dynamic LLM selection. The key insight is that the user should only see a standard OpenAI-compatible interface while behind-the-scenes sophisticated AI architectures handle context, reasoning, and response generation.

## Bridging the Gap: From Standard API to Cognitive Infrastructure

Traditional AI development often treats large language models (LLMs) as simple text generators[^1]. However, advanced systems require more sophisticated handling of context, memory, and dynamic decision-making. The challenge is creating seamless user experiences while maintaining complex internal architectures that can scale and adapt.

The solution involves building what we might call an **Overlay AGI Architecture** – a cognitive framework that sits on top of standard LLM APIs but provides enhanced capabilities without exposing complexity to end users[^2]. This approach directly connects with concepts from the [[GPT Hosting with Preconfiguration]] note, which emphasizes embedding behavior through context injection rather than code modification.

## The User Experience: Simple Interface, Complex Functionality

The core principle is maintaining a clean public API that closely mimics OpenAI's specification. Users interact through familiar fields:
- `model`: Abstract identifier for the service
- `messages`: Standard chat format 
- `session_id`: Context management without user burden

This design directly reflects insights from [[AGI Beyond Docker Semantic Resonance]], where the distinction between "image" (code) and "spirit" (cognitive process) becomes crucial. The API acts as a semantic interface that allows users to engage with complex systems through familiar patterns, rather than forcing them into technical implementation details[^3].

## Internal Architecture: Hidden Complexity Behind the Curtain

The internal processing pipeline demonstrates sophisticated cognitive infrastructure:

### 1. Session and Memory Management
Using Redis for fast context retrieval and Chroma for vector storage creates persistent conversation memory without burdening users. This approach mirrors how biological brains maintain context through distributed memory systems[^4]. 

This pattern directly connects to concepts in [[LLM Поведение и Анализ]] about **latent memory** – the ability to maintain information beyond explicit token contexts, enabling long-term reasoning and coherent dialogue.

### 2. Dynamic Agent Selection
The routing system chooses between different agent pipelines (sports coach, code reviewer) based on content analysis rather than predefined rules[^5]. This mirrors cognitive architecture principles where different reasoning modes activate automatically based on context.

### 3. Flexible LLM Orchestration
The system can seamlessly switch between local vLLM deployments and cloud providers like OpenAI or Anthropic. The [[Cloud AGI via API Integration]] note emphasizes exactly this kind of flexible deployment that maintains cognitive richness while allowing resource optimization[^6].

This approach builds upon the modular reasoning chain concepts from [[Cloud AGI via API Integration]], where different LLM calls are orchestrated through well-defined pipelines rather than isolated function calls.

## Technical Implementation: Building the Cognitive Bridge

The FastAPI implementation demonstrates how to create this bridge between user expectations and complex internal systems:

```python
@app.post("/v1/chat/completions")
async def chat_endpoint(payload: dict, cred: HTTPAuthorizationCredentials = Depends(bearer)):
    # ... authentication ...
    
    # Load conversation history from memory storage 
    store = ConversationStore()
    history = await store.load_history(session_id)
    
    # Build full context including system prompts
    full_context = [
        {"role": "system", "content": f"You are {model_name}."},
        *history,
        *messages,
    ]
    
    # Select appropriate agent pipeline based on content
    pipeline_cfg = select_pipeline(full_context) 
    
    # Execute through LangChain chains (with potential RAG)
    chain = build_chain(pipeline_cfg)
    assistant_reply, usage = await llm.run(chain, full_context)
```

This implementation directly reflects the [[Overlay AGI in ChatGPT Interface]] concept of **overlay layers** that provide cognitive behavior without requiring modification of core model architecture[^7]. The system acts as a semantic shell around standard LLMs, embedding sophisticated reasoning capabilities through context manipulation.

## Hidden Complexity: What Users Don't See

The beauty of this approach lies in what remains invisible to users:

- **Persistent Context**: No need to send full conversation history
- **Dynamic Model Selection**: Different models chosen based on topic without user intervention  
- **Agent Pipelines**: Complex operations like RAG, tool calling, or domain-specific reasoning
- **Consistent Tone and Style**: System prompts automatically generated for appropriate voice
- **Usage Metrics**: Built-in tracking for billing and analytics

These hidden elements work together to create a system that behaves like an advanced cognitive agent rather than just a simple chat interface[^8].

## Scalability: From Single Instance to Multi-GPU Systems

The architecture is designed with scalability in mind, supporting:
- Multiple FastAPI workers
- Redis clustering for memory handling  
- Chroma sharding for vector storage
- GPU pooling for local LLMs
- Cloud provider load balancing

This mirrors the principles discussed in [[46 Overlay AGI]] about achieving O(1) or O(n) computational complexity while maintaining cognitive sophistication through external knowledge management[^9].

## The Cognitive Framework: Beyond Simple Prompt Engineering

What makes this approach fundamentally different from basic prompt engineering is that it creates a **cognitive architecture** rather than just an input/output system. Users interact with what feels like a sophisticated AI assistant, but the underlying system is built on principles of:

- Semantic resonance (as discussed in [[AGI Beyond Docker Semantic Resonance]])
- Frame-based reasoning (from [[Cloud AGI via API Integration]])  
- Distributed memory management
- Modular cognitive processing

This evolution from simple API endpoints to **cognitive infrastructure** directly addresses the limitations identified in traditional AI deployment approaches and provides a practical framework for building more sophisticated AI services[^10].

---

#### Sources

[^1]: [[GPT Hosting with Preconfiguration]]
[^2]: [[Overlay AGI in ChatGPT Interface]] 
[^3]: [[AGI Beyond Docker Semantic Resonance]]
[^4]: [[LLM Поведение и Анализ]]
[^5]: [[Cloud AGI via API Integration]]
[^6]: [[Cloud AGI via API Integration]]
[^7]: [[Overlay AGI in ChatGpt Interface]]
[^8]: [[GPT Hosting with Preconfiguration]]
[^9]: [[46 Overlay AGI]]
[^10]: [[AGI Beyond Docker Semantic Resonance]]

## Как превратить «мозговой центр» в обычный Chat‑API  
*(пользователь видит только простую точку входа, а внутри работают память, агенты, несколько LLM и всякое прочее)*  

Ниже – пошаговый рецепт + готовый пример кода. Всё построено на **FastAPI**, но принципы одинаковы для любой технологии (Node/Express, Go‑Gin, Java‑Spring Boot и т.п.).

---

## 1️⃣ Что именно видит клиент

| Поле | Описание (по аналогии с OpenAI) |
|------|-----------------------------------|
| **`model`** | строка‑идентификатор публичного «моделя» (например, `my‑assistant‑v1`). Пользователь не знает, какие под‑модели реально задействованы. |
| **`messages`** | массив объектов `{role: "system"|"user"|"assistant", content: string}` – обычный чат‑формат. |
| **`session_id`** *(опционально)* | UUID/токен, позволяющий серверу хранить контекст между запросами без необходимости передавать всю историю каждый раз. |
| **`max_tokens`**, **`temperature`**, … | Стандартные параметры генерации – их тоже можно скрыть в «default‑config», а клиент может переопределять при желании. |

**Ответ**:  

```json
{
  "id": "chatcmpl-01a2b3c4d5e6f7g8h9i",
  "object": "chat.completion",
  "created": 1726324001,
  "model": "my-assistant-v1",
  "choices": [
    {
      "index": 0,
      "message": {"role":"assistant","content":"…"},
      "finish_reason":"stop"
    }
  ],
  "usage": {"prompt_tokens":123,"completion_tokens":45,"total_tokens":168}
}
```

То есть **публичный API полностью копирует OpenAI‑спецификацию** – клиенту ничего не меняется, а у вас под капотом любые «агенты», «мемори‑системы» и «рекомендательные модели».

---

## 2️⃣ Схема внутренней обработки (что происходит за кулисами)

```
┌─────────────────────────────┐
│   HTTP → FastAPI endpoint     │
└───────▲───────────────────────┘
        │
        ▼
┌─────────────────────────────┐
│  Auth + Rate‑limit Middleware│
└───────▲───────────────────────┘
        │
        ▼
┌─────────────────────────────┐
│   Session / Memory Manager    │   ← получает session_id,
│   (Redis + Chroma)          │      достаёт прошлый контекст
└───────▲───────────────────────┘
        │
        ▼
┌─────────────────────────────┐
│  Agent Selector + Router     │  решает, какой «агент‑pipeline»
│  (LangChain / custom)       │  будет обслуживать запрос
└───────▲───────────────────────┘
        │
        ▼
┌─────────────────────────────┐
│  LLM‑Invoker                 │   – локальный vLLM, OpenAI,
│  (vLLM | OpenAI | Anthropic)│   – выбирается динамически
└───────▲───────────────────────┘
        │
        ▼
┌─────────────────────────────┐
│  Post‑process & Response      │
│  (добавить usage, logs…)    │
└─────────────────────────────┘
```

### Ключевые компоненты

| Компонент | Что делает | Как реализовать |
|-----------|------------|-----------------|
| **Auth + Rate‑limit** | Проверка JWT / API‑ключа, ограничение QPS/IP. | `fastapi.security.HTTPBearer`, `slowapi`/`starlette-rate-limit`. |
| **Session/Memory Manager** | По `session_id` достаёт последние *k* реплик из векторного хранилища (Chroma) + обычный буфер (Redis). При первом запросе создаёт новый `session_id`. | `class ConversationStore:` → `redis.lrange`, `chroma.search`. |
| **Agent Selector** | На основе содержания сообщения, пользовательского профиля и/или A/B‑теста выбирает один из предопределённых пайплайнов (например, «спортивный советник», «код‑ревьюер», «деловой планировщик»). | Таблица правил (`if "code" in text → pipeline_code`). |
| **LLM‑Invoker** | Универсальный клиент: `invoke(messages, model_cfg)`. Для локального – HTTP к vLLM; для облачного – SDK OpenAI/Anthropic. | `class LLMClient:` с методом `call()`. |
| **Post‑process** | Добавляет метрики (`usage`), сохраняет новый диалог в память, логирует запрос/ответ (для аудита). | `store.append(session_id, user_msg, assistant_reply)`. |

---

## 3️⃣ Пример кода (FastAPI + LangChain)

```python
# ── api/main.py ────────────────────────────────────────────────
from fastapi import FastAPI, Depends, HTTPException, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import uuid, json, os

from .auth import verify_token                # JWT проверка
from .store import ConversationStore           # Redis+Chroma wrapper
from .router import select_pipeline            # правила выбора агента
from .llm import LLMInvoker                   # vLLM / OpenAI клиент
from .pipeline import build_chain              # LangChain chain per pipeline

app = FastAPI()
bearer = HTTPBearer()

# ---------- единственная публичная точка -----------------
@app.post("/v1/chat/completions")
async def chat_endpoint(
    payload: dict,
    cred: HTTPAuthorizationCredentials = Depends(bearer)
):
    # 1️⃣ Аутентификация
    user_id = verify_token(cred.credentials)   # → str

    # 2️⃣ Разбор входа (совпадает с OpenAI‑спецификацией)
    model_name = payload.get("model", "my-assistant-v1")
    messages   = payload["messages"]
    session_id = payload.get("session_id") or str(uuid.uuid4())

    # 3️⃣ Получаем историю из памяти
    store = ConversationStore()
    history = await store.load_history(session_id)        # список Message

    # 4️⃣ Объединяем: system‑prompt → история → новые сообщения
    full_context = [
        {"role": "system", "content": f"You are {model_name}."},
        *history,
        *messages,
    ]

    # 5️⃣ Выбираем пайплайн/агента
    pipeline_cfg = select_pipeline(full_context)

    # 6️⃣ Собираем LangChain‑цепочку (может включать RAG, инструменты)
    chain = build_chain(pipeline_cfg)   # возвращает Callable[[list], str]

    # 7️⃣ Вызов LLM через универсальный клиент
    llm = LLMInvoker(pipeline_cfg["llm"])
    assistant_reply, usage = await llm.run(chain, full_context)

    # 8️⃣ Сохраняем новый диалог в память (только последние N)
    await store.append(session_id,
        {"role": "assistant", "content": assistant_reply})

    # 9️⃣ Формируем ответ по OpenAI‑спецификации
    resp = {
        "id": f"chatcmpl-{uuid.uuid4().hex[:24]}",
        "object": "chat.completion",
        "created": int(uuid.time_low),
        "model": model_name,
        "choices": [
            {"index": 0,
             "message": {"role": "assistant", "content": assistant_reply},
             "finish_reason": "stop"}
        ],
        "usage": usage,
        # optional: echo back session_id, чтобы клиент мог хранить его
        "session_id": session_id
    }
    return resp
```

#### Кратко о вспомогательных модулях

```python
# auth.py
import jwt, os
def verify_token(token: str) -> str:
    payload = jwt.decode(token, os.getenv("JWT_SECRET"), algorithms=["HS256"])
    return payload["sub"]          # user_id
```

```python
# store.py
import redis.asyncio as aioredis
from chromadb.api import Client as ChromaClient

class ConversationStore:
    def __init__(self):
        self.redis = aioredis.from_url("redis://redis:6379")
        self.chroma = ChromaClient()

    async def load_history(self, sid: str, limit=10):
        msgs = await self.redis.lrange(f"chat:{sid}", -limit, -1)
        # каждый элемент — json‑строка Message
        return [json.loads(m) for m in msgs]

    async def append(self, sid: str, msg: dict):
        await self.redis.rpush(f"chat:{sid}", json.dumps(msg))
        await self.redis.ltrim(f"chat:{sid}", -100, -1)   # храним последние 100
```

```python
# router.py
def select_pipeline(messages):
    txt = " ".join(m["content"].lower() for m in messages)
    if "football" in txt or "sport" in txt:
        return {"name": "sports_coach",
                "llm": {"type":"local", "model":"Meta-Llama-3.2-8B-Instruct"}}
    if "code" in txt or "bug" in txt:
        return {"name":"code_reviewer",
                "llm":{"type":"cloud","provider":"openai","model":"gpt‑4o-mini"}}
    # fallback
    return {"name":"general_assistant",
            "llm":{"type":"cloud","provider":"anthropic","model":"claude-3.5-sonnet"}}
```

```python
# llm.py
import httpx, openai, anthropic

class LLMInvoker:
    def __init__(self, cfg):
        self.cfg = cfg

    async def run(self, chain, messages):
        # 1️⃣ LangChain‑chain → список сообщений (можно добавить RAG)
        prompt = await chain(messages)          # возвращает list[Message]

        # 2️⃣ Выбираем клиент
        if self.cfg["type"] == "local":
            async with httpx.AsyncClient() as client:
                resp = await client.post(
                    f"http://vllm:80/v1/chat/completions",
                    json={"model":self.cfg["model"],"messages":prompt})
                data = resp.json()
        elif self.cfg["provider"] == "openai":
            data = openai.ChatCompletion.create(
                model=self.cfg["model"], messages=prompt)
        else:
            data = anthropic.messages.create(
                model=self.cfg["model"], messages=prompt)

        # 3️⃣ Выделяем ответ и usage
        reply = data["choices"][0]["message"]["content"]
        usage = data.get("usage", {"prompt_tokens":0,"completion_tokens":0})
        return reply, usage
```

```python
# pipeline.py (пример простого RAG‑pipeline)
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate

def build_chain(cfg):
    # Для большинства пайплайнов достаточно: Retrieval → LLM
    vector = Chroma(persist_directory="./chroma")
    def chain(messages):
        # 1️⃣ Поиск релевантных кусочков (если включено)
        query = messages[-1]["content"]
        docs = vector.similarity_search(query, k=3) if cfg["name"]!="general_assistant" else []
        # 2️⃣ Формируем системный prompt + retrieved chunks
        system = f"You are a {cfg['name']}.\n"
        if docs:
            system += "Relevant facts:\n" + "\n".join(d.page_content for d in docs)
        tmpl = ChatPromptTemplate.from_messages([
            ("system", system),
            *[(m["role"], m["content"]) for m in messages]
        ])
        return tmpl.format_messages()
    return chain
```

---

## 4️⃣ Что именно скрыто от пользователя

| Скрытая функция | Как выглядит в публичном ответе |
|------------------|---------------------------------|
| **Постоянная память** | Пользователь не передаёт всю историю; сервер подставляет её автоматически из `session_id`. |
| **Динамический роутинг LLM** | В запросе нет поля «model=local», но внутри выбираются разные модели в зависимости от темы. |
| **Агент‑pipeline (RAG, инструменты)** | Для обычного пользователя — просто «ответ». А под капотом могут вызываться поиск по базе, калькуляторы, генерация кода и т.д. |
| **Тон/стиль** | Системный промпт формируется автоматически (`You are a sports‑coach...`). Пользователь его не пишет. |
| **Метрики usage** | В ответе присутствует `usage`, но клиент может их игнорировать – они нужны только для биллинга/логов. |

---

## 5️⃣ Как «продать» такой API клиентам

1. **Документация в стиле OpenAI**  
   - Swagger/OpenAPI (`/docs`) показывает единственную схему `/v1/chat/completions`.  
   - Примеры `curl` и Python‑клиент (`openai.ChatCompletion.create(...)`), где только меняете `api_key` и `base_url`.

2. **Ключи доступа** – один API‑ключ → JWT/токен, клиент не видит внутренние сервисы.

3. **Версионирование**  
   - `/v1/chat/completions` – текущая версия.  
   - При больших изменениях (добавление новых системных полей) выпускаете `/v2/...`.

4. **Квоты и тарифы** – на уровне API‑ключа, а не на уровне LLM. Вы сами решаете, сколько будет стоить каждый запрос (по usage).

5. **SDK‑обёртка (необязательно)**  
   - Пакет `myassistant` в PyPI, где внутри вызывается ваш endpoint и автоматически сохраняет `session_id`. Пользователь опять же видит только обычный чат.

---

## 6️⃣ Как масштабировать «скрытый» слой

| Компонент | Что масштабировать | Как |
|-----------|-------------------|-----|
| **FastAPI** | Параллельные запросы → несколько воркеров | `uvicorn --workers 4` или Deploy в Kubernetes (`replicas: 3`). |
| **Redis (memory)** | Высокая пропускная способность чтения/записи | Redis Cluster / отдельный VM с большой RAM. |
| **Chroma** | Объёмный векторный индекс | Хранить на SSD, включать `persist_directory`, при росте – sharding. |
| **vLLM (локальная модель)** | GPU‑нагрузка | Пул из нескольких контейнеров, каждый со своим GPU; роутер распределяет запросы по «load». |
| **Облачные LLM** | Ограничения провайдера | Лимитировать количество токенов в минуту и переключать на более дешёвый провайдер при превышении. |

---

## 7️⃣ Полный набор файлов (структура проекта)

```
my_assistant_api/
├─ docker-compose.yml                # fastapi, redis, chroma, vllm, caddy
├─ Caddyfile                        # TLS‑терминатор → https://api.myassistant.dev
│
├─ api/
│   ├─ Dockerfile
│   ├─ main.py                     # ← единственная публичная точка
│   ├─ auth.py
│   ├─ store.py
│   ├─ router.py
│   ├─ llm.py
│   └─ pipeline.py
│
└─ requirements.txt
```

*`docker-compose.yml`* (коротко)

```yaml
version: "3.9"
services:
  api:
    build: ./api
    ports: ["8000:8000"]
    env_file: .env
    depends_on: [redis, chroma, vllm]

  redis:
    image: redis:7-alpine

  chroma:
    image: ghcr.io/chroma-core/chroma:latest
    volumes: ["./chroma_data:/chroma"]

  vllm:
    image: vllm/vllm:latest
    command: ["--model","Meta-Llama-3.2-8B-Instruct"]
    runtime: nvidia   # если есть GPU

  caddy:
    image: caddy:2-alpine
    ports: ["443:443", "80:80"]
    volumes: ["./Caddyfile:/etc/caddy/Caddyfile"]
```

Запуск:

```bash
docker compose up -d
# проверяем Swagger:
curl -s https://localhost/v1/chat/completions/docs | head
```

---

## 8️⃣ Итоги

| Что сделали | Как выглядит клиенту |
|-------------|----------------------|
| **Один эндпоинт** `/v1/chat/completions` | `POST {"messages":[...],"session_id":"…"}` → обычный JSON‑ответ. |
| **Скрытая память** (Redis + Chroma) | Пользователь получает «контекст», но не передаёт его в запросе. |
| **Динамический роутинг агентов/LLM** | Внутри системы выбираются *sports_coach*, *code_reviewer* и т.д., пользователь об этом ничего не знает. |
| **Масштабируемость** (мульти‑GPU, облачные модели) | Сервис выдерживает рост нагрузки без изменения публичного API. |
| **Стандартные метаданные** (`usage`, `model`) | Совместимо с клиентскими SDK OpenAI – легко интегрировать в уже существующие приложения. |

Таким образом вы получаете **«чёрный ящик» чат‑бота**, который выглядит и работает как обычный ChatGPT‑API, но внутри у вас целая оркестрация агентов, памяти, RAG‑поиска и гибкого выбора LLM. Всё, что требуется от клиента – отправить запрос и сохранить полученный `session_id` (или позволить серверу его сгенерировать).  

Если нужен готовый репозиторий – дайте знать, пришлю zip‑архив со всеми файлами и инструкциями по деплою в Docker/K8s. 🚀