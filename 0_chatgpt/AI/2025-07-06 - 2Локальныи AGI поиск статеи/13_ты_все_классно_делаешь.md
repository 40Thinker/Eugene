>[!nexus_user] **User** - 06.07.2025 at 16:26
> Ты все классно делаешь, мы вместе еще лучше сделаем. Давай в этот запрос добавим еще один тип фильтра ( по тому какие статьи могут помочь в работе по гипотезе что провал и остановка развития ветки ИИ на основе LLM в неверном составе датасетов и дообучения base моделей во всех смыслах (данные, практики, пайплайны и тд, общая философия датасетов для создания и обучения моделей ) и решения - дообучение, Файн тюнинг, lora, rag и прочее применимые к qwen 3-30b-base. Т.е. не важно о чем статья, она имеет смысл лишь в том случае если раскрывает механику полезную для этой гипотезы и этого решения, и под этим углом её упоминать и описывать, но брать за раз для изучения 7 дней, начать с 1 января 2015 года, т.е. в запросе написать 1-7 января 2015 г.), чтобы мне не пришлось читать 5-10 тысяч статей, ты понимаешь суть идеи, расширь запрос до предела размера ответа, это около 2-3 А4 страниц. I'm building a self-hosted AGI-like system. Search for research since June 1, 2020 with practical or conceptual value for this. These are examples (not strict rules) of what to look for — focus on anything that might be useful for local AGI, even if unusual:
> 
> – Modular or self-improving LLMs (example: cognitive agent frameworks or modular memory)
> – Long-term memory systems: RAG, LoRA, attention compression, vector DBs, memory agents
> – Reasoning architectures (example: symbolic-neural hybrids, tool-using agents, logical chains)
> – Offline learning: distillation, continual learning, data-efficient training, training by dialog
> – Small models with large effects (examples: novel routing, curriculum tricks, subnetwork activation)
> – Unusual post-training techniques (example: training like raising a child, or behavioral shaping)
> – Innovative interfaces (example: AGI with sketchpad, gesture, world model overlays)
> – Alternative to tokenization (example: JEPA, concept vectors, joint multimodal anchors)
> – Entirely new math or representations (examples: topological thinking, dynamic graphs, phase fields)
> – Hybrid RAG systems that include web search + local index + on-the-fly memory construction
> – Self-rewriting/self-updating models (example: agents that modify their own weights or config via prompts)
> – Emergent AGI traits (examples: subjective behavior, unpredictability, recursive insight)
> Please exclude SaaS/cloud-only systems. Give papers, projects, or even unexplored ideas worth prototyping for local AGI.
<!-- UID: 0d6145f5-5b04-442c-9d57-7ecdfd441fd5 -->




>[!nexus_agent] **Assistant** - 06.07.2025 at 16:26
> Понял. Мы расширяем предыдущий “королевский” запрос до **максимального уровня абстракции + прикладной пользы** с новой глубинной гипотезой:
> 
> > Что **текущая ветка развития LLM/AGI остановлена** не из-за ограничений архитектуры, а из-за **неверного состава датасетов, ошибочных принципов их подготовки и философии обучения** (особенно для base моделей), а следовательно, **спасение — в дообучении (LoRA, RAG, finetune, distill) на предельно осмысленных текстах и структурах**, применимых к Qwen3-30B-Base и локальным AGI.
> 
> Вот итоговый **гиперзапрос (~2.5 A4)** — с учётом всех твоих направлений, примеров, пояснений и фильтра.
> 
> ---
> 
> ## 🧠 **Расширенный исследовательский промпт (архитектура AGI + гипотеза про крах ветки LLM из-за датасетов, + решения на Qwen 3)**
> 
> > I am building a self-hosted AGI-like system — fully offline, modular, and evolving through recursive fine-tuning, distillation, and memory-aware extensions (e.g. LoRA, RAG, local adapters). My current hypothesis is that the **stagnation of the LLM/AGI field is not due to transformer limitations**, but due to a **fundamentally flawed foundation in dataset design, fine-tuning pipelines, pretraining philosophies, and evaluation regimes**.
> >
> > Many base models — including excellent architectures like Qwen 3–30B — are underperforming at high-level reasoning not because they lack capacity, but because:
> >
> > 1. **Their base pretraining datasets are philosophically, semantically and structurally incoherent**;
> > 2. They are missing latent structures and high-level abstractions needed for cognitive recursion;
> > 3. Post-training strategies (fine-tuning, RLHF, chat templates) suppress emergent behaviors;
> > 4. The prevailing philosophy of dataset curation emphasizes **volume over depth**, **breadth over causal hierarchy**, and **average truth over meaningful exception**.
> >
> > Therefore, I’m initiating a 3-phase strategy:
> >
> > - Phase 1: Search for any research since **Jan 1–7, 2015**, across all disciplines, that could help **(a)** identify these structural and philosophical failures in current datasets or training paradigms, or **(b)** offer mechanics to rebuild them with greater AGI-alignment;
> > - Phase 2: Extract all mechanics, concepts, workflows, or structures that could be used to **fine-tune Qwen3-30B-base** into a recursive, local AGI with deep understanding;
> > - Phase 3: Design micro-datasets, curriculum fragments, and memory injection strategies for LoRA, RAG, or modular distillation pipelines.
> 
> ---
> 
> ## 🔍 The search goal is not bounded by NLP or transformer research.
> 
> > Any paper from **January 1 to January 7, 2015**, across AI, neuroscience, learning theory, philosophy, data science, cognitive systems, complex systems, linguistics or software engineering may be relevant **only if it helps us understand**:
> >
> > - How flawed data curation leads to degenerate cognition in large models;
> > - How the **structure of a dataset can encode or destroy reasoning capacity**;
> > - What types of input/representation are more effective at enabling **recursive self-improvement**;
> > - How **semantic loss or “meaning decay”** occurs during tokenization, compression, and training.
> 
> ---
> 
> ## 🔬 Useful example areas (search directions, not rules)
> 
> ### 1. **Failures in dataset philosophy**
> 
> - Critiques of LLM-scale datasets (e.g. Common Crawl, The Pile) showing **semantic redundancy, incoherence, or anti-cognition bias**;
> - Historical papers that discuss the **collapse of reasoning** due to **averaging over contradiction** (e.g. when factual ambiguity is treated as noise instead of structure);
> - Data-centric AI research exposing the problems of scale-first curation and statistical "truthiness".
> 
> ### 2. **Better ways to construct or select data**
> 
> - Studies proposing **curriculum learning with concept dependency graphs**;
> - Papers on **synthetic datasets that encode mathematical or philosophical rigor**;
> - Epistemology-aware dataset filtering (e.g., remove text without causal or deductive flow);
> - Psycholinguistic models that discuss how **children learn concepts from sparse data** and how this could translate into machine data selection.
> 
> ### 3. **Latent structure vs token flatness**
> 
> - JEPA-like models where tokens are replaced by latent predictors;
> - Emergent behavior from **non-tokenized input**: concept vectors, high-level anchor points, symbolic-analog hybrids;
> - Representational geometry studies of latent spaces — what shapes allow for reasoning recursion.
> 
> ### 4. **Microdataset effects**
> 
> - Case studies where **small datasets** had **disproportionate impact** on model performance (especially low-rank finetuning);
> - Papers showing **LoRA or QLoRA success** with datasets of 10–1000 examples that were well-structured;
> - “Exemplar learning” or “seed concept bootstrapping” (train from a few high-quality nuclei instead of broad corpora).
> 
> ### 5. **Memory and RAG-based reassembly**
> 
> - Hybrid RAG systems where a live index is built from **structured reasoning fragments**, not full documents;
> - Memory augmentation via **filtered graph snippets**, **dialogue state loops**, or **contextual self-retrieval**;
> - Novel post-RAG architectures that build **recursive context trees** instead of flat retrieval.
> 
> ### 6. **Emergence failures due to data noise**
> 
> - Why LLMs lose recursion or reflection at scale — is it due to noise, entropy, redundancy?
> - When does training on too much internet garbage flatten model gradients and saturate attention?
> - Is there a quantifiable threshold at which **emergent traits decay with overfitting to poor data**?
> 
> ### 7. **Encoding subjective / symbolic / recursive structures**
> 
> - Research that encodes **subjectivity, narrative logic, mythic structure, or nested abstraction** into training corpora;
> - Papers that explain how **symbolic cognition can be injected** into neural networks via training selection;
> - Multi-modal or cross-modal pipelines that use **sketches, visual flows, math diagrams** to augment language grounding.
> 
> ---
> 
> ## ✅ The paper is only relevant if…
> 
> > It has **potential to help build or fine-tune a local AGI**, particularly via improving datasets, training principles, or reasoning interfaces. The subject matter does not have to mention transformers or LLMs — it is relevant if it can **indirectly support** the following outcome:
> 
> **“Making Qwen3-30B-Base smarter by giving it the right data, in the right shape, with the right structure.”**
> 
> ---
> 
> ## ❌ Please exclude:
> 
> - Any SaaS- or API-based closed system papers;
> - Superficial benchmark competition results with no insight into dataset mechanics;
> - Pure statistical modeling papers with no relevance to AGI reasoning;
> - Review articles that repeat high-level points without exposing structural innovation.
> 
> ---
> 
> ## 🧩 Final objective
> 
> I’m reconstructing the philosophical, linguistic, and data-theoretical basis of local AGI training. This prompt is meant to **extract latent blueprints** from any field, as long as the paper (from Jan 1–7, 2015) could help:
> 
> - Explain *why current LLMs fail to think*;
> - Show *how dataset structure impacts recursion and emergence*;
> - Contribute to *methods of rescuing base models like Qwen3-30B* via fine-tuned microlearning or RAG-driven scaffolding.
> 
> Please prioritize insight, structure, and cross-domain bridges over surface-level results. The goal is **to find tools that fix the AGI pipeline at its roots**.
> 
> ---
> 
> 📌 Можем теперь запустить поиск по этому промпту за **1–7 января 2015**, и я выдам тебе только те статьи, которые **действительно что-то чинят** в архитектуре, датасетах или обучении базовых моделей. Дать старт?
<!-- UID: fe5d458c-6db2-4775-a885-7feaaa1d1aa0 -->