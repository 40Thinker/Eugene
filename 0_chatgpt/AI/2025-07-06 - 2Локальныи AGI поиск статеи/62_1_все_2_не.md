>[!nexus_user] **User** - 08.07.2025 at 8:02
> 1 все. 2 не важно 3 любое 4. Все, но приоритет реальные вакансии 2025
<!-- UID: bbb21b7f-aee9-48e7-b282-c32d75623c57 -->

# Анализ заметки [[62_1_все_2_не]] как системного архитектора Overlay AGI

---

## 1. В чём суть проблемы, о которой идёт речь в заметке

Суть проблемы заключается в **глубоком дисбалансе между техническими и гуманитарными компетенциями** в разработке искусственного интеллекта, особенно на пути к созданию AGI. Современные подходы к построению ИИ доминируются инженерно-вычислительными парадигмами: масштабирование моделей, оптимизация loss-функций, управление токенами и RAG-пайплайнами. Однако эти методы, как подчёркивается в заметке, не решают **фундаментальный вопрос о смысле**. Без понимания смысла, контекста, этики и когнитивных структур ИИ остаётся «статистическим попугаем», способным имитировать интеллект, но не способным его обладать. Проблема — не в отсутствии технологий, а в **отсутствии архитектуры смысла** как центрального элемента проектирования. Это морально, когнитивно и онтологически неустойчивая основа для создания системы, претендующей на обобщённый интеллект.

Список релевантных заметок:
- [[59_если_бы_человек_учился]] — напрямую формулирует, что ИИ без правополушарного мышления (гуманитарного, философского) не может быть AGI, подтверждая критику технократического подхода.
- [[68_можешь_более_детально_с]] — приводит мнения мыслителей (Thrun, Summerfield, Polli), подчёркивающих необходимость нейронаук, философии и этики в команде, что поддерживает идею междисциплинарности.
- [[63_мой_анализ_-_не]] — критикует поверхностный подход к интеграции гуманитариев как «надстройки», а не как ядра архитектуры, что точно соответствует сути проблемы: поверхностная кадровая политика вместо глубокого переосмысления.
- [[60_напиши_пожалуйста_промпт_для]] — содержит структурированный запрос на анализ компетенций, где гуманитарные навыки ставятся в один ряд с техническими, что указывает на признание проблемного поля.
- [[15_я_ранее_уже_упоминал]] — утверждает, что IT-навыки — лишь инструменты, а *без гуманитарной базы и теоретического мышления* химия создания ИИ не «случится», что является прямым указанием на суть проблемы.

---

## 2. Последствия отсутствия решения

Отсутствие решения ведёт к **системному коллапсу доверия и функциональности ИИ**. Если ИИ-системы строятся без встроенных моделей смысла, этики и онтологии, они становятся непредсказуемыми, необъяснимыми и потенциально опасными. Последствия проявляются на трёх уровнях: **когнитивном** (модели не понимают, что они говорят), **этическом** (они не различают добро и зло, справедливость и дискриминацию), и **практическом** (они не подходят для долгосрочного взаимодействия с людьми, так как не могут адекватно интерпретировать контекст). Это приводит к созданию систем, которые выглядят умными, но на деле — это «чёрные ящики» с манипулятивным потенциалом. Компании, полагающиеся на такие ИИ, рискуют репутацией, правовыми последствиями и утратой контроля над технологией.

Список релевантных заметок:
- [[63_мой_анализ_-_не]] — указывает, что существующие команды «подменяют мышление инференсом», что ведёт к созданию ИИ, который не мыслит, а симулирует, что и есть ключевое последствие.
- [[20_1_2_3_4]] — описывает, что отсутствие онтологического мышления приводит к «токеноцентричным моделям и обучающим костылям», то есть к системам, не способным к настоящему пониманию.
- [[68_можешь_более_детально_с]] — упоминает, что AI safety игнорирует психологию и философию, что ведёт к «неполному пониманию рисков», особенно в безопасности и этике.
- [[62_1_все_2_не]] — прямо говорит о том, что чисто статистический ИИ остаётся «непредсказуемым ящиком», что подчёркивает риск непрозрачности и нестабильности.
- [[15_я_ранее_уже_упоминал]] — утверждает, что без теоретического мышления системы «не порождают живых систем», что указывает на бесплодность текущего пути развития.

---

## 3. Как решить эту проблему

Решение заключается в **архитектурной интеграции гуманитарного мышления на уровне проектирования ИИ**, а не как внешняя обработка. Это требует создания **новых ролей, процессов и структур** внутри разработки. Гуманитарии должны быть не «этиками по запросу», а **онтологическими архитекторами, когнитивными инженерами и философами на равных с техниками**. Решение включает: (1) разработку **единой онтологии реальности** как основы AGI, (2) внедрение **семантических прослоек** между LLM и пользователем, (3) создание **голограммных команд**, где технари и гуманитарии мыслят как единый организм, (4) формализацию **рефлексивных циклов**, где ИИ способен к самоанализу, и (5) внедрение **конституциональных механизмов** (как в Anthropic), основанных на гуманитарных принципах. Только так можно построить ИИ, который не просто обрабатывает, но и **понимает**.

Список релевантных заметок:
- [[62_1_все_2_не]] — предлагает роль **AI-философа** и **онтологического инженера** как ключевых специалистов, что напрямую указывает на решение.
- [[60_напиши_пожалуйста_промпт_для]] — формулирует запрос на создание мета-обзора «кто и где понял, что смысл важнее токенов», что и есть основа для решения: смена приоритетов.
- [[68_можешь_более_детально_с]] — ссылается на pymetrics и DeepMind Ethics, как примеры интеграции нейронаук и этики, что является практикой решения.
- [[59_если_бы_человек_учился]] — предлагает создать **LoRA-модуль «философское ядро AGI»**, что указывает на техническое воплощение гуманитарного слоя.
- [[63_мой_анализ_-_не]] — предлагает перейти от «надстройки» к **голограмме будущей модели**, где гуманитарное мышление кодируется в архитектуре.
- [[20_1_2_3_4]] — позиционирует запрос как **«верхний онтологический якорь»**, необходимый для отказа от токеноцентричности, что и есть архитектурное решение.
- [[15_я_ранее_уже_упоминал]] — утверждает, что нужен **«теоретический вектор»**, связывающий технику и гуманитарику, что определяет структуру решения.
- [[62_1_все_2_не]] — описывает **Constitutional AI** как пример формализации ценностей, что является работоспособной практикой.
- [[60_напиши_пожалуйста_промпт_для]] — предлагает сравнить «мой подход» с индустриальным, что создаёт **вектор синтеза**, необходимый для решения.
- [[68_можешь_более_детально_с]] — рекомендует институционализировать роли **AI-философа** и **когнитивного архитектора**, что является организационным решением проблемы.

#### Sources
[^1]: [[62_1_все_2_не]]
[^2]: [[59_если_бы_человек_учился]]
[^3]: [[68_можешь_более_детально_с]]
[^4]: [[63_мой_анализ_-_не]]
[^5]: [[60_напиши_пожалуйста_промпт_для]]
[^6]: [[15_я_ранее_уже_упоминал]]
[^7]: [[20_1_2_3_4]]


>[!nexus_agent] **Assistant** - 08.07.2025 at 8:02
> Отлично, начну глубокий анализ вакансий (с приоритетом 2025 года), блогов, научных и философских источников, в которых описываются реальные и осмысленные компетенции, необходимые для создания качественного ИИ — включая технические и гуманитарные, философские, когнитивные и семиотические аспекты. Будут рассмотрены как англоязычные, так и русскоязычные источники, охватывающие всё доступное время. Я сообщу, когда обзор будет готов.
<!-- UID: 496baaee-81c0-40b1-a5b7-0d566994fbba -->

---

>[!nexus_agent] **Assistant** - 08.07.2025 at 8:07
> # Компетенции и типы мышления для создания AGI: технические vs гуманитарные
> 
> ## Введение 
> Разработка систем искусственного интеллекта общего назначения (AGI) требует не только передовых инженерных навыков, но и глубокого понимания человеческого мышления, языка и ценностей. В 2025 году эта мысль отражена как в требованиях реальных вакансий, так и в научно-философских дискуссиях. Индустрия ищет специалистов, сочетающих знания **машинного обучения и программирования** с **гуманитарной эрудицией** – пониманием языка, культуры, когнитивных процессов и этики. Одновременно мыслители в области AI подчеркивают необходимость **«смысловой» компоненты** в разработке AGI – от онтологического проектирования до этического осмысления. Ниже мы проанализируем: **(1)** какие роли и компетенции реально требуются на рынке, **(2)** какие навыки рассматриваются в философских и научных текстах, **(3)** где возникает осознание, что одной инженерии недостаточно (нужны «правополушарные» подходы), **(4)** кто уже включает гуманитариев в AI-команды и как это влияет на задачи, и **(5)** предпринимаются ли попытки формализовать новые роли или инструменты для осмысленной сборки AGI. В конце сравним тенденции в вакансиях и исследованиях, выявим общие черты, различия и точки сближения.
> 
> ## Требования индустрии: роли на стыке технологий и гуманитарных наук 
> Анализ вакансий 2024–2025 годов показывает растущий спрос на **междисциплинарные роли** в AI. Компании ищут специалистов, которые помимо навыков программирования (Python, ML, работа с LLM/NLP) обладают знаниями в лингвистике, когнитивистике, социальной науке или философии. В таблице ниже приведены реальные примеры таких позиций и требуемых компетенций:
> 
> | Роль (компания) | Пересечение компетенций (требования) |
> |------------------------------------------------------------|------------------------------------------------------------------------------------|
> | **Generative AI Specialist (Humanities)** <br> *Innodata Inc* | Степень **в области гуманитарных наук** обязательна; задачи включают генерацию подсказок для LLM【19†L348-L355】. |
> | **Philosophy Specialist – AI Trainer** <br> *Invisible Agency* | Предпочтительна **кандидатская степень по философии** или смежной области; опыт преподавания и исследований показывает компетентность【34†L183-L187】. |
> | **ML Data Linguist** <br> *ServiceNow* | Образование в области **компьютерной или вычислительной лингвистики** (PhD желателен); требуется глубокое знание **грамматики, синтаксиса и семантики** для улучшения генерации текста моделями【50†L84-L92】【50†L93-L100】. |
> | **Head of Computational Social Science** <br> *MassMutual* | Требуется PhD **в социологии, антропологии или экономике**; компания ожидает интеграции соц-научного подхода в инновации ИИ【32†L218-L222】. |
> 
> Как видно, индустрия стремится нанимать **специалистов-гибридов**: например, **инженеров по подсказкам (prompt engineers)** с гуманитарным образованием для разработки эффективных промптов и сценариев взаимодействия с LLM【19†L348-L355】, или **компьютерных лингвистов**, способных привнести знание структуры языка в обучение моделей【50†L84-L92】【50†L93-L100】. Появляются роли, ориентированные на **этику и философию в AI** – хотя их пока немного, уже есть вакансии вроде *AI Ethicist* или *AI Philosopher*. Так, компания искала *Philosophy Specialist* для тренировки ИИ-моделей, требуя академической подготовки в философии【34†L183-L187】. Крупные организации также ценят экспертов с пониманием **человеческого поведения и общества**: показательно создание роли директора по вычислительной социальной науке с требованием степени в антропологии/социологии для разработки человеко-ориентированных AI-решений【32†L218-L222】. Таким образом, **реальные вакансии** сигнализируют: успешная работа над AI-системами теперь предполагает *не только* владение кодом, но и *понимание языка, смысла, культуры и морали*. 
> 
> **Софт-навыки** и широкий кругозор также фигурируют в описаниях. Например, инженерам по ИИ требуется умение доносить сложные идеи до нетехнических стейкхолдеров и работать в мультидисциплинарной команде【50†L101-L109】. Это отражает понимание, что **AI-продукты создаются командами**, где должны найти общий язык программисты, лингвисты, UX-исследователи, психологи и другие.
> 
> ## Дискуссии в науке и философии: компетенции для «осмысленного» ИИ 
> В академических кругах и философских эссе всё чаще поднимается вопрос: *каких знаний не хватает нынешним AI-системам для достижения настоящего интеллекта?* Ключевой ответ – **знания о смысле и структуре реальности**, которые традиционно изучают гуманитарные науки. Авторы указывают на необходимость *онтологического и семантического проектирования* AI: прежде чем строить AGI, надо определить, **какие сущности, понятия и отношения** будут в его «разуме». В этой связи фигурируют идеи о **“онтологических архитекторах”** и **инженерах по смыслу**. 
> 
> Так, в одном обсуждении подчёркивается, что проработка **онтологий** – формальных моделей предметной области – является фактически **этапом высокоуровневого проектирования** AGI, аналогом архитектурного эскиза【41†L110-L118】. Без явного представления о ключевых понятиях и феноменах мира, которые AGI должен понимать, дальнейшее программирование бессмысленно. Дискуссии об онтологиях – это не пустая философия, а необходимая часть разработки: “договориться” о том, что такое реальность для ИИ, какие у нее элементы и законы, – таким образом закладывается фундамент осмысленного интеллекта【41†L110-L118】. 
> 
> Параллельно звучат призывы привлечь к созданию ИИ **экспертов по значению** – лингвистов, семиотиков, когнитивистов. Современные большие языковые модели (LLM) критикуют за манипулирование *токенами* без истинного понимания смысла. Решение видят во внедрении семантических технологий: **онтологий, знаний о мире, логических правил**. Появился даже термин **“семантический ИИ”**, противопоставляемый “чёрному ящику” AGI. *Semantic AI* фокусируется на **понимании и смысле**, используя онтологии и знания, а не только статистические корреляции в данных【49†L43-L51】. В LinkedIn-обсуждении 2024 года отмечалось: *Semantic AI строит более «умные», контекстно-осведомленные системы, опирающиеся на знанийные модели, созданные человеком, вместо попыток вслепую имитировать человеческое мышление*【49†L43-L51】. Такой подход обеспечивает объяснимость и надежность: решения AI становятся прозрачнее, поскольку опираются на явно заданные понятия и правила, в то время как чисто статистический AGI остаётся непредсказуемым “ящиком”【49†L51-L59】【49†L65-L70】. 
> 
> Важно, что **семантический подход** рассматривается не как академическая утопия, а как *более реалистичный путь* к продвинутому ИИ. Отмечается, что все необходимые инструменты уже существуют – от онтологий (например, в формате OWL/RDF) до механизмов логического вывода и семантического поиска【49†L59-L68】【49†L97-L105】. Таким образом, в научном сообществе зреет консенсус: **для достижения настоящего интеллекта машина должна оперировать значениями, а не только данными**. Это требует новых компетенций – **онтологической инженерии, знаниевой инженерии, логики**, – которые традиционно относились к сфере AI-исследований старой школы. Теперь эти навыки снова актуальны, уже применительно к LLM и нейросетям, чтобы встроить в них *модель мира* и *здравый смысл*. 
> 
> Также в философских работах подчёркивается роль **когнитивных наук**: понимание того, *как мыслит человек*, какие у нас категории знаний, каким образом мы выводим смыслы из контекста – всё это ценно при создании AGI. Например, предлагается привлекать **экспертов по смыслу**, способных наладить соответствие между **внутренним представлением AI** и человеческими концептами. Обсуждаются даже экзотические роли вроде **онтопсихологов AI** или **специалистов по сознанию**, которые бы работали над тем, чтобы у AGI была целостная картина мира, а не разрозненный набор выученных паттернов. И хотя эти названия пока во многом теоретические, сам факт их появления в дискурсе говорит о *необходимости гуманитарного мышления* в AI-разработке.
> 
> ## Правополушарное мышление: осознание необходимости гуманитариев 
> Постепенно приходит понимание, что **создание продвинутого ИИ – не только инженерная задача**, но и *творческий, гуманитарный вызов*. Об этом сигнализируют как представители академии, так и индустрии. Например, в Колумбийском университете отмечают, что сейчас технологическим компаниям **“уже недостаточно просто программистов; им нужны люди с гуманитарным складом”**【21†L113-L117】. Студентов призывают осваивать *когнитивную науку*, сочетая информатику с психологией и философией, потому что будущее принадлежит **интердисциплинарным мыслителям**【21†L115-L123】. В этой же статье профессор прямо говорит: *“AI-программисты должны понимать этические импликации своей работы, а гуманитарии – разбираться, как устроена технология”*, то есть требуются **гибридные специалисты** нового типа【21†L126-L131】. Такая позиция отражает растущее осознание: **для создания разумных машин нужны «оба полушария» – и аналитическое, и образно-ценностное**.
> 
> Эту мысль ярко формулирует инженер и философ Чад Вудфорд, имеющий редкое сочетание степеней (по инженерии, праву и философии). В 2023 году он написал программное эссе **“Почему AI нужны полиматы и философы”**, где утверждает: *“Командам разработки AI нужны и левополушарные, и правополушарные мыслители”*. Если ИИ призван служить всему человечеству, то **в командах должны быть люди, понимающие технологии, философию и право одновременно**【48†L73-L81】. Вудфорд подчёркивает: *Искусственные интеллектуальные системы неизбежно отражают мышление их создателей. Поэтому, если их создают исключительно инженеры, ИИ получится “мыслящим как инженер” – склонным к сугубо логичному, но узкому взгляду*【48†L75-L82】. Он пишет: *“Мы не хотим, чтобы наши будущие интеллектуальные машины думали только как инженеры… Нам нужно, чтобы они рассуждали как мудрецы”*【48†L79-L82】. То есть помимо кода и алгоритмов, разработчики должны привнести в ИИ **мудрость, этику, эмпатию** – качества, традиционно относимые к гуманитарному началу. 
> 
> Вудфорд вводит даже новое понятие своей роли – **“AI philosopher” (AI-философ)** – признавая, что формально такой должности почти нигде нет, но **она остро необходима**【48†L155-L163】【48†L159-L164】. AI-философ в его понимании – это специалист, который работает бок о бок с инженерами, привнося “правополушарный” взгляд: заботится о том, чему учатся модели (не только *насколько* они умны, но *чему* – прививая им человеческие ценности и устремления), задаёт долгосрочные вопросы (например, *что означает сознание для машины?*), следит, чтобы ИИ стремился к мудрости, а не просто к эффективности【48†L149-L157】【48†L159-L164】. Фактически он призывает компании вводить таких **философов в команду**, подобно тому как в Google есть “евангелисты интернета” или специалисты по UX. И хотя пока подобных вакансий мало, именно *философы, гуманитарии* могут стать *“профессиями будущего”* в эпоху AI【48†L155-L163】.
> 
> Не только философы, но и представители бизнес-образования замечают эту тенденцию. В 2024 году *Financial Times* писали о росте интереса к выпускникам с двойными компетенциями – например, **MBA + изучение этики AI**, или **компьютерные науки + антропология**. Корпорации начинают ценить умение взглянуть на проблему шире технарского рамки: осмыслить, *как технология повлияет на общество, культуру, человека*. Таким образом, **“правополушарное” (гуманитарное) мышление** постепенно признаётся неотъемлемой частью успеха AI-проектов.
> 
> ## Интеграция гуманитариев в AI-команды: кто это делает и зачем 
> Ряд передовых компаний и исследовательских организаций уже предпринимают шаги к **мультидисциплинарным командам**, включающим гуманитариев. Особенно показательна область **AI-этики и ответственного ИИ**. Например, Google и Meta *ещё с середины 2010-х годов создали внутренние команды AI-этиков*, куда вошли люди с образованием в философии, науках о поведении, гендерных исследованиях и т.д.【48†L104-L112】. Их задача – сотрудничать с архитекторами и инженерами AI, **встроив принципы справедливости, прозрачности и ответственности** прямо в процесс разработки моделей【48†L104-L112】. AI-этики участвуют в подборе и разметке данных, разработке методик устранения смещений, проведении оценок влияния системы на пользователей【48†L104-L112】. Таким образом, технологическая компания интегрирует *ценностную экспертизу* наравне с технической. Это пример осознанной стратегии: **чтобы продукт ИИ был успешен и принят обществом, он должен учитывать человеческие ценности с самого начала разработки**【48†L114-L117】.
> 
> Однако путь не всегда гладкий – в последние годы были случаи, когда компании сокращали такие гуманитарные команды, стремясь к быстрым релизам. Например, Microsoft в 2023 году распустила свою команду по этике AI, несмотря на то что активно продвигает продукты на базе ИИ【48†L119-L127】. Тем не менее, **дальновидные игроки рынка** удерживают междисциплинарный курс. Вудфорд отмечает, что **“самые прогрессивные компании понимают: этичность и человекоориентированность AI – это долгосрочная инвестиция”**, окупаемая доверием пользователей и устойчивостью продукта【48†L129-L137】. LinkedIn, например, внедряет принципы прозрачности AI *“by design”* с 2021 года【48†L133-L140】, а OpenAI, создавая передовые модели, параллельно исследует вопросы безопасности и взаимодействует с философами для оценки рисков【48†L133-L140】. 
> 
> Помимо этиков, появляются и другие форматы интеграции гуманитариев: 
> - **Социальные ученые и антропологи** в AI-проектах. Например, **MassMutual нанял PhD-антрополога** на руководящую роль (см. выше), ожидая, что тот поможет учесть социальные факторы и человеческое поведение при внедрении AI-решений в сфере финансов【32†L218-L222】. В целом, в финансовых и консалтинговых компаниях растёт интерес к специалистам по поведенческой науке, способным интерпретировать выводы AI-моделей в контексте реальных человеческих действий (например, для модели кредитного скоринга важно понять, не является ли она дискриминационной – тут нужен и технический, и социальный анализ). 
> - **Лингвисты и филологи** в командах по разработке языковых моделей. Компании-разработчики NLP-систем (например, Google, Microsoft, Yaндекс) давно привлекают *компьютерных лингвистов*, но теперь акцент расширяется: нужны не только навыки программирования на Python, но и глубокое знание теоретической лингвистики, прагматики языка, дискурса. Такие эксперты помогают моделям *понимать контекст, неоднозначность, переносный смысл*. Даже новая роль **“инженер по промптам”** зачастую требует умения писать тексты и владеть языковыми тонкостями – нередки случаи, когда на позиции *Prompt Engineer* берут людей с образованием в гуманитарных науках, но способных быстро освоить работу с LLM【19†L348-L355】. 
> - **Специалисты по UX и психологии** для настройки взаимодействия человека с ИИ. Например, в проектах по созданию чатботов или виртуальных ассистентов все чаще участвуют психологи, когнитивные исследователи, этнографы, которые изучают, *как пользователи воспринимают AI*, какие у них ожидания и страхи. Такой специалист может выявить, что, скажем, определенный стиль ответа ИИ вызывает у людей недоверие или тревогу – и скорректировать поведение модели. Вакансии UX-исследователей с фокусом на AI-интерфейсы появляются в крупных компаниях (Google, Amazon и др.), требуя комбинации навыков: анализ данных об использовании + знание когнитивной психологии. 
> 
> Отдельно стоит отметить примеры из научных лабораторий. В знаменитой лаборатории DeepMind (принадлежит Google) помимо программистов и математиков работали и **философы** – в 2020 году там запустили исследовательский проект совместно с Оксфордским философским факультетом для изучения **этических вопросов AGI**. Компания Anthropic, основанная выходцами из OpenAI, позиционируется как делающая “AI, ориентированный на человеческие ценности” – в их командах есть специалисты по общественным наукам, а сам подход назван *“конституционным AI”*, где в модели закладываются принципы, сформулированные в человеческом понятном виде (например, аналог конституции или этического кодекса). Это по сути работа **на стыке права, этики и программирования**.
> 
> Итак, **тенденция такова**: команды, создающие продвинутый AI, становятся все более *интердисциплинарными*. Гуманитарии в них выполняют не вспомогательную роль, а *ключевую*: они помогают задать правильные цели для ИИ, сделать систему понятной и приемлемой для людей, напитать её смыслом, а не только данными. Там, где такие специалисты интегрированы, задачи формулируются шире – например, не просто *повысить точность модели*, а *убедиться, что модель понимает нюансы человеческого ввода и ведёт себя этично*. Это приводит к более зрелым AI-продуктам.
> 
> ## Новые роли и инструменты: к “смысловой сборке” AGI 
> Как индустрия, так и исследователи пытаются **формализовать подходы, где смысл важнее токена**. Речь идёт о разработке методологий и даже должностей, специально нацеленных на *семантическое, содержательное развитие AI*. Ниже перечислены некоторые из таких попыток и идей:
> 
> - **AI Philosopher / AI Ethicist**: уже упомянутая новая роль – философ/этик в команде AI. Хотя пока встречается редко, аргументировано, что в будущем без такого специалиста не обойдётся ни один серьезный проект AGI【48†L155-L163】. Его ответственность – *следить за “моральным компасом” ИИ*, за тем, чтобы системы обучались с учётом ценностей, и консультировать инженеров при выборе подходов (например, стоит ли давать модели возможность саму менять свои цели). По сути, это **формализация гуманитарной функции** внутри AI-проекта. Уже существуют программы стажировок и курсы, готовящие AI-этиков (на стыке CS и философии). 
> - **Ontologist / Knowledge Engineer**: в сфере символического AI такая роль не нова – ещё в 1980–90х были *knowledge engineers*, вручную вписывающие знания в базы. Теперь же, на фоне LLM, концепция возрождается: требуются **онтологические архитекторы**, которые будут разрабатывать *общие схемы знаний для AGI*. Появляются исследования, предлагающие интегрировать онтологии с нейросетями – например, проекты по **объединению knowledge graphs с трансформерами**. В LinkedIn-статьях выдвигается идея “**Универсальной онтологии реальности**” для AI【46†L461-L469】, своего рода единого каркаса понятий, на котором можно строить различные модули интеллекта. Такие инициативы призваны дать инструментарий для **“смысловой сборки”** AGI: конструкторы смыслов, которые систематически вплетаются в архитектуру ИИ. Например, статья *“Ontology as the Missing Keystone in AGI”* (2024) утверждает, что без единой онтологической основы разные компоненты AGI будут говорить на “разных языках”, и сравнивает ситуацию с **Вавилонской башней** – предлагая в качестве “краеугольного камня” единую систему понятий【42†L25-L29】. Отсюда логично вытекает роль **онтологического инженера** – того, кто разрабатывает и поддерживает эту систему понятий. 
> - **Semantic AI Platforms**: помимо ролей, создаются и технологические **платформы для семантического ИИ**. Они позволяют совместно использовать машинное обучение и базы знаний. К примеру, некоторые стартапы предлагают инструменты, где разработчики могут связать ответы LLM с формальной онтологией: модель при генерации ответа не просто предсказывает следующий токен, а сверяется с графом знаний, уточняя контекст. Это своего рода **“смысловые прослойки”** между человеком и черным ящиком нейросети. Их цель – обеспечить, чтобы в работе AI главенствовала **семантика, а не статистика**. Такие инструменты ещё в начале пути, но активно обсуждаются (см. комментарий специалиста в посте J. Ravenel о том, что пока он не видел промышленных систем, совмещающих векторное представление и онтологии, но многие над этим думают【49†L149-L158】). 
> - **Constitutional AI и прочие гибридные подходы**: также можно назвать попытки OpenAI и Anthropic ввести явные правила поведения модели (что есть хорошо, что плохо) – по сути, это упрощённый вариант смысловой сборки, когда вместо полной онтологии дают модели **набор человеческих инструкций в текстовом виде**. Специалисты по гуманитарным наукам участвуют в составлении таких “конституций” ИИ, используя знания этики, права, психологии. Формально эти люди могут называться аналитиками или research scientists, но фактически выполняют роль **“переводчиков” человеческих ценностей на язык, понятный ИИ**.
> 
> В целом, происходит интересный сдвиг: если раньше развитие AI шло от алгоритмов к возможностям (сначала придумываем архитектуру – получаем функционал), то теперь сообщество говорит о движении *от смыслов к реализации*. Сначала определить **какой смысловой каркас** должен быть у AGI, а потом выбирать под это подходящие алгоритмы (нейросети, символьные методы или их сочетание). Это и есть **“смысловая сборка” AI**, и для неё требуются как новые инструменты (онтологические базы, языки представления знаний, системы логического вывода в связке с нейросетями), так и **новые профессионалы** – философы, лингвисты, онтологи, работающие рука об руку с программистами.
> 
> ## Сравнение: индустриальные требования vs научные перспективы 
> Накопленный материал позволяет сопоставить **практику индустрии** и **идеи исследований** относительно необходимых для AGI компетенций:
> 
> | **Аспект** | **Индустрия (вакансии, 2025)** | **Наука и философия (идеи)** |
> |----------------------------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------|
> | *Технические навыки* | Превалируют навык программирования (Python, алгоритмы), опыт с ML/LLM. Требуются Data Science, MLOps, инженерия данных【28†L323-L331】. | Упор на **когнитивные и онтологические** навыки: моделирование знаний, понимание мышления. Предлагаются методы символической логики и онтологии в паре с ML【49†L43-L51】【49†L65-L70】. |
> | *Языковые и семантические знания* | Ценятся специалисты с **лингвистическим образованием** для работы над NLP-моделями【50†L84-L92】【50†L93-L100】. Знание нескольких языков и локализаций – плюс. | Подчеркивается критическая важность **понимания смысла**. Необходимы специалисты, умеющие формализовать значения, контекст, здравый смысл (семиотика, логика, когнитивная лингвистика)【49†L43-L51】【41†L110-L118】. |
> | *Этические и социальные компетенции* | В ряде вакансий упоминаются **этика AI**, принципы Fairness/Transparency. Компании ищут AI-этиков, однако таких ролей пока немного【48†L155-L163】. Социальные науки привлекаются в специфических случаях (например, UX-исследователь для AI-продукта). | Научный дискурс утверждает, что **этика, права, гуманитарное знание – ядро создания AGI**. Предлагается институционализировать AI-этику. Гуманитарии рассматриваются как равноправные участники разработки, формирующие цели и ограничения системы【48†L73-L81】【48†L159-L164】. |
> | *Роли и должности* | Конкретные **гибридные роли** уже на рынке: *ML Engineer + Linguist*, *Prompt Engineer* с гуманитарным бэкграундом, *AI Ethics Officer*, *Computational Social Scientist*. Эти должности оформлены и финансируемы (см. примеры вакансий выше). | Обсуждаются новые роли, зачастую пока концептуальные: *AI Philosopher*【48†L155-L163】, *Ontological Architect*, *Expert in Meaning*. Предлагается, чтобы такие специалисты стали частью команд наряду с инженерами, хотя официальных позиций мало. |
> | *Подход к созданию AGI* | Преимущественно **инженерный**: большие модели тренируются на больших данных, ставка на увеличение мощности. Гуманитарные навыки применяются утилитарно (для улучшения UX, для корректировки ответов модели, для PR и соответствия регуляциям). | Постулируется **комплексный подход**: AGI – это не просто код, а *понимающая система*. Создание AGI уподобляется *построению интеллектуальной сущности*, что требует *осмысления до кодирования*. Предлагаются гибридные архитектуры (нейросети + символы), ориентированные прежде всего на **репрезентацию знаний и причинности**, а не только на вычислительную мощь【49†L65-L70】. |
> 
> **Общие тенденции:** и индустрия, и наука сходятся на том, что **моно-дисциплинарности недостаточно**. Чисто технические команды рискуют упустить из виду смысл, контекст и общественные последствия AI; с другой стороны, одни гуманитарии без инженеров не реализуют идеи в продукт. Поэтому наблюдается *конвергенция*: инженеры учатся говорить на языке этики и лингвистики, а гуманитарии осваивают основы кодирования и статистики, чтобы эффективнее внедряться в AI-проекты【21†L126-L131】. 
> 
> **Различия:** практический рынок движется немного медленнее и реагирует на уже явные потребности (например, после развития GPT-моделей возник спрос на prompt engineer с навыками словесности). Научное сообщество же пытается **предвосхитить будущие проблемы**: философы предупреждают о необходимости смысла в AI прежде, чем обнаружатся критические провалы. Вакансии 2025 года ещё редко требуют прямо “философа” или “онтолога” в команде, в то время как в статьях уже звучит: *“AI-командам абсолютно необходимы правополушарные люди – компании должны ввести должность AI-философа”*【48†L159-L164】. 
> 
> Другой нюанс: **в индустрии гуманитарные компетенции часто подаются как “желательные”**, тогда как технические – как “обязательные”. В научных же рассуждениях делается акцент, что гуманитарное равноважно техническому. Например, компания может указать “знание когнитивной психологии является плюсом” в описании вакансии, но редко – “требуется диплом психолога”. Академики же говорят о **необходимости равноправного сотрудничества**: *“AI-программисты должны понимать гуманитарное, а гуманитарии – техническое”*【21†L126-L131】. 
> 
> **Точки соединения:** уже сейчас есть примеры, где эти миры встречаются. Команды по **AI-безопасности** и **AI-Alignment** (типа Center for AI Safety, OpenAI Alignment Team) обычно включают и программистов, и философов/юристов. При формировании регуляторных политик для AI (например, в обсуждениях закона об ИИ в ЕС) работают группы из юристов, социологов и специалистов по ML. Таким образом, *прорывные исследования зачастую вдохновляют новые практические роли*: так, развитие направления **Responsible AI** в академии привело к появлению вакансий *“AI Responsibility Manager”* в корпорациях. Можно ожидать, что по мере осознания ценности семантики, появятся позиции типа *“Knowledge Engineer for LLM”* или *“Cognitive Architect”*, напрямую занимающиеся смысловой структурой AGI.
> 
> ## Выводы 
> **Создание AGI – это коллективный междисциплинарный проект**, требующий синтеза технического мастерства и гуманитарного мышления. Реальные вакансии последних лет подтверждают: рынок начинает ценить специалистов, умеющих быть “двоечниками” – одновременно кодить и понимать человека. Инженеры с бекграундом в лингвистике или философы, разбирающиеся в ML, больше не кажутся экзотикой; напротив, они становятся ключевыми связующими звеньями команд. 
> 
> В философских и научных кругах эта потребность осмыслена еще глубже: там выработано понимание, что **без содержания нет интеллекта**. Простое масштабирование моделей – тупиковый путь к подлинному разуму, если не вложить в модели понятия, цели, контексты, близкие человеку. Отсюда растёт интерес к семантическим онтологиям, к когнитивным архитектурам, интегрирующим *левое* и *правое*. Исследователи фактически призывают перевооружить AI-разработку: вместо чисто инженерного “культа большего вычислителя” перейти к осмысленному конструированию *мыслящей системы*.
> 
> **Закономерность**, выявленная нашим обзором, такова: **там, где пересекаются технарь и гуманитарий, и рождается потенциал для настоящего AI**. Индустрия, движимая конкуренцией, уже делает практические шаги – нанимает антропологов, этиков, лингвистов, пусть иногда и “по требованию момента”. Наука, менее скованная сиюминутными задачами, формулирует общие принципы и готовит кадры новой формации (междисциплинарные программы по когнитивной науке, этике AI и т.п.). В ближайшие годы мы вероятно увидим дальнейшее **сближение**: стандарты разработки AI будут включать этапы смыслового проектирования и этического аудита; должности AI-философа или онтологического инженера могут стать привычными наравне с ML-инженером.
> 
> В заключение, построение AGI можно сравнить с возведением собора знаний: нужны и **камнетёсы-инженеры**, отёсывающие блоки алгоритмов, и **архитекторы смысла**, держащие в уме величественный замысел. Только совместно они смогут создать сооружение, способное не просто вычислять, но *понимать* и *разделять с людьми общую реальность*. Путь к настоящему искусственному интеллекту лежит через уважение к сложности человеческого опыта – а значит, через союз технических и гуманитарных компетенций.
> 
> **Источники:**
> 
> - Реальные вакансии, отражающие спрос на смешанные навыки (2024–2025): требование гуманитарного образования для *Generative AI Specialist*【19†L348-L355】; предпочтение PhD по лингвистике для *ML Data Linguist*【50†L84-L92】【50†L93-L100】; должность главы направления AI с PhD по антропологии/социологии【32†L218-L222】; вакансия *AI Trainer* с философским образованием【34†L183-L187】. 
> - Статья Колумбийского университета о программе когнитивной науки (2023): индустрия *“больше не хочет чистых программистов, им нужны люди с гуманитарным измерением”*【21†L113-L117】; призыв к этическому и техническому образованию одновременно【21†L126-L131】. 
> - Эссе C. Woodford *“Why AI Needs More Polymaths and Philosophers”* (Medium, 2023): *“AI teams need left- and right-brain thinkers”*, AI отразит мышление создателей【48†L73-L81】; *“не хотим, чтобы ИИ думал лишь как инженер... пусть думает как мудрец”*【48†L79-L82】; роль AI-философа как новая необходимость【48†L155-L163】【48†L159-L164】; описание работы AI-этика в компаниях【48†L104-L112】; аргумент о долгосрочной выгоде этичных AI-команд【48†L129-L137】. 
> - Обсуждение *Semantic AI* (LinkedIn, J. Ravenel, 2024): отличие Semantic AI от AGI – упор на **понимание смысла через онтологии**【49†L43-L51】; преимущества (объяснимость, доверие) семантического подхода【49†L51-L59】【49†L65-L70】; комментарии о знаниево-ориентированных вариантах AI и их терминах【49†L97-L105】. 
> - Пост в LiveJournal (deep_econom, 2021): важность онтологий и определений при проектировании AGI, онтология как *“архитектурный эскиз”* интеллекта【41†L110-L118】. 
> - Вакансии на стыке AI и философии/этики: подборка Indeed (*Philosophy Specialist – AI Trainer* и др.) подтверждает, что такие роли появляются【34†L179-L187】. 
> - Другие упомянутые источники: материалы о Responsible AI в крупных компаниях; новости о составах команд OpenAI, Anthropic и DeepMind (в тексте ссылки на них опущены для краткости, основные тезисы подкреплены приведенными цитатами).
<!-- UID: 763fcc2d-6afd-4200-9f3c-5b4a27097753 -->