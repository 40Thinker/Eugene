>[!nexus_user] **User** - 24.07.2025 at 23:40
> 1. 3. Русский
<!-- UID: c9fad70a-18b5-47c1-8a7f-4c4406bda1df -->




>[!nexus_agent] **Assistant** - 24.07.2025 at 23:40
> Отлично. Я подготовлю подробное исследование на русском языке по всем аспектам выбора следующего токена в больших языковых моделях — от embedding и attention до философских оснований и инженерных стратегий генерации.
> 
> Это будет структурировано в виде 20 тематических разделов с формулами, метафорами, кодом, визуализациями и указанием на границы интерпретации.
> 
> Я сообщу, как только оно будет готово к просмотру.
<!-- UID: 0990dcf6-da34-41c4-a4ba-440d46df580e -->

---

>[!nexus_agent] **Assistant** - 24.07.2025 at 23:56
> # Глубокий разбор механизма выбора токенов в LLM
> 
> ## 1. **Философия токена: что такое «следующее слово» в языковой модели?**
> 
> **Объяснение:** В больших языковых моделях (LLM) выбор следующего слова – это не осознанное понимание, а статистический процесс. Модель не «знает» значения слов так, как человек, – она обучена продолжать текст, выбирая следующий токен на основе вероятностей, выученных из огромного корпуса данных. В ходе обучения модель настраивает миллиарды параметров, чтобы максимально правдоподобно предсказать, какое слово обычно идет вслед за предыдущими в текстах. Отсюда появляется ощущение смыслового потока, хотя на самом деле модель просто **вычисляет наиболее вероятное продолжение** текущей последовательности【9†L127-L133】. Например, если контекст – «Не откладывай на завтра то, что можно», LLM вероятно продолжит «сделать сегодня». Однако это не потому, что модель понимает пословицу, а потому что такая фраза часто встречалась и имеет высокую вероятность.
> 
> **Формализация:** Технически LLM оптимизирует функцию следующего токена: \( P(w_{t+1} | w_1, w_2, \dots, w_t) \), где \(w_i\) – последовательность предыдущих токенов. Она вычисляет **логиты** – сырые оценки правдоподобия для каждого возможного токена – и применяет к ним softmax для получения вероятностей【4†L119-L125】. Таким образом, модель предсказывает тот токен, который максимизирует эту вероятность. Нет внутренней символической структуры «значения» – есть только веktorное представление и вероятностная модель. Ключевой философский момент: **значение слова у модели определяется контекстами, в которых оно встречается**. Это реализует принцип распределенной семантики: *«Ты узнаешь слово по компании, в которой оно находится»* (Дж. Фёрс) – модель схватывает статистические связи между словами, и из них рождается функциональный эквивалент значения.
> 
> **Пример:** Рассмотрим упрощенный случай: модель видела предложения «кошка сидит на коврике», «кошка спит на коврике». Получив ввод «кошка ___ на коврике», она высоко оценит токены «сидит» и «спит». Она не знает, что значат эти слова, но знает, что в подобных контекстах они часто появлялись. Если обучить модель только на фразе «кошка [MASK] на коврике» с правильным ответом «сидит», она научится выдавать «сидит» как наиболее вероятный токен. Но она не *осознает*, что кошка действительно сидит – она лишь воспроизводит шаблон.
> 
> **Ограничения и эффекты:** Такая *«статистическая семантика»* приводит к тому, что модель убедительно генерирует текст, оставаясь при этом *«стохастическим попугаем»*, не имеющим встроенного мира или фактов【9†L127-L133】. LLM не различает истину от лжи – она лишь продолжает вероятностный паттерн. Поэтому возможны *галлюцинации*: если в обучающих данных часто встречалось что-то неправдивое или выдуманное продолжение, модель может его воспроизвести. Она не проверяет факты – у нее нет внешней реальности, только **корпус текста** как источник всех «знаний». Это накладывает философское ограничение: **значение в LLM – это тень использования слова в тексте**. Модель имитирует смысл, но не переживает его. Как отметил один обзор, LLM *«рассчитывает, какое слово наиболее вероятно должно быть следующим, и потому заслужила прозвище ‘стохастический попугай’ – чрезвычайно хорошо угадывая последовательности, но не имея внутреннего представления, что они значат»*【9†L127-L133】.
> 
> **Философское замечание:** Возникает вопрос – если модель просто предсказывает токены, откуда берётся **осмысленность** её ответов? Здесь граница интерпретации: некоторые исследователи (например, Илья Суцкевер) предполагают, что *очень хорошее предсказание следующего слова требует формирования модели реальности*, то есть при достаточно мощном «угадывании» LLM невольно начинает отражать закономерности мира. Другие (например, Э. Бендер) возражают, что без **связи с внешним миром** (так называемой проблемы *«семантической привязки»*) модель лишь манипулирует знаками. Мы видим правдоподобные тексты, но не можем уверенно сказать, есть ли за ними *понимание*. Таким образом, «следующее слово» для LLM – это статистический выбор, рождающий иллюзию осмысленности, и этот процесс демонстрирует как силу (модель производит правильные ответы), так и слабость (она может уверенно говорить бессмыслицу) таких моделей【9†L129-L137】. Граница интерпретируемости проходит там, где нам хотелось бы приписать модели намерения или знания: на самом деле за каждым шагом стоит только математика вероятностей, хотя совокупный эффект напоминает осмысленный диалог.
> 
> ## 2. **Embedding-пространство: как представляется слово и почему оно не дискретное**
> 
> **Объяснение:** Компьютеру неудобно работать напрямую со словами в текстовом виде – поэтому перед обработкой слова преобразуются в **токены**, а затем в векторы. *Embedding*-пространство – это высокомерное название многомерного векторного пространства, где каждому токену соответствует точка (вектор). Вместо дискретного one-hot-кода (где «кошка» = [0,0,...,1,...,0] с единицей на позиции слова в словаре) используется плотное **embedding-представление** – набор, скажем, из 300 чис, генерируемых обучаемой матрицей. Эти числа подбираются так, чтобы токены со схожим контекстным употреблением имели близкие векторы【6†L200-L208】. В результате **семантически похожие слова находятся близко в embedding-пространстве**, а различные – далеко. Таким образом, модель превращает символическое слово в **латентный вектор**, который уже не дискретен, а живет в непрерывном пространстве смыслов.
> 
> **Формализация:** Пусть размер словаря – \(V\), а размер пространства эмбеддингов – \(d\). Тогда embedding-матрица \(W_E\) – это матрица размером \(V \times d\). Каждому токену \(w_i\) соответствует строка (или столбец) этой матрицы – вектор \(\mathbf{e}_{w_i} \in \mathbb{R}^d\). Проекция one-hot представления через \(W_E\) дает этот вектор: если \(e_{w}^{(one-hot)}\) – вектор с единицей на позиции \(w\), то \( \mathbf{h} = W_E^T \cdot e_{w}^{(one-hot)} = \mathbf{e}_{w}\). Например, при \(V=50{,}000\) и \(d=768\), как у GPT-2, \(W_E\) – 50к x 768 матрица【4†L85-L93】【4†L95-L102】. Эти эмбеддинги обучаются на задаче предсказания слов: градиент подстраивает координаты так, чтобы модель лучше угадывала следующий токен. В итоге **embedding-матрица впитывает статистику языка**. Близость можно измерять, например, косинусом: если \(\cos(\mathbf{e}_{w1}, \mathbf{e}_{w2})\) высок, токены \(w1, w2\) имеют схожие контексты.
> 
> **Пример:** Представим toy-модель, где словарь: {«кот», «кошка», «собака», «корабль»}. В one-hot кодировке каждая – ортогональный базисный вектор, никакой близости между «котом» и «кошкой» нет (скалярное произведение 0). В embedding-варианте можем получить например: \(\mathbf{e}_{\text{«кот»}} = (0.2, -1.3, 0.7)\), \(\mathbf{e}_{\text{«кошка»}} = (0.1, -1.2, 0.6)\), \(\mathbf{e}_{\text{«собака»}} = (1.0, 0.5, -0.2)\), \(\mathbf{e}_{\text{«корабль»}} = (-0.9, 0.4, 0.0)\). Скалярное произведение «кот»·«кошка» будет близко к 1 (большое), а «кот»·«корабль» – малое или отрицательное. Это отражает, что в текстах «кот» и «кошка» употребляются в схожих контекстах (домашние животные), а «корабль» – совсем о другом. Таким образом, *embedding* фактически захватывает **латентную «метафизику» слова** – его статистическую роль. Это не просто кодировка, а позиция в пространстве, где направления могут соответствовать понятиям: например, в классических word2vec-эмбеддингах обнаружилось направление «сделать слово множественным» или «сменить род». Аналогично, LLM может иметь направление от «кот» к «кошка», указывающее на женский род.
> 
> **Ограничения и эффекты:** Embedding-пространство – фундамент семантики модели, но оно **не явное и неразмеченное**. Векторы нелегко интерпретировать человеку напрямую. Кроме того, модель может сталкиваться с *полисемией*: одно слово («bank») имеет несколько значений («банк» или «берег реки»). Без учета контекста базовый embedding будет какой-то усредненный. LLM частично решает это тем, что **контекстные эмбеддинги** меняются под влиянием соседних слов: например, после self-attention слово «bank» скорректируется разными слоями под фразу «river bank» или «central bank». Ограничение также в том, что если токен – нецелостное слово (кусочек слова), то его embedding может нести лишь часть смысла. Например, BPE-разбиение «pineapple» = «pine» + «apple» (ананас -> «сосна» + «яблоко») может привести к тому, что модельного представления целого «pineapple» явно нет, оно получается композицией двух частичных эмбеддингов. Это **потенциально искажает семантику**: токен «apple» внутри «pineapple» – не фрукт, но вектор «apple» все равно будет внесён, и модель должна через последующие слои «понять», что тут речь не о яблоке【44†L27-L30】. Несмотря на эти сложности, embedding-слой удивительно эффективен: он переносит нас из дискретного мира слов в непрерывное пространство смысловых признаков, где алгоритму проще обнаруживать закономерности.
> 
> **Философское замечание:** *Латентное пространство* эмбеддингов – своего рода *«Платонов мир идей»* для LLM. Каждое слово проецируется в него не однозначно, а как набор признаков. С одной стороны, это устраняет жёсткую дискретность языка: в мозге модели нет отдельной ячейки «КОТ», а есть сочетание активностей, которое может частично перекрываться с «кошка» или «тигр». Возникает **непрерывность смыслов** – можно плавно перейти от вектора «радость» к «печаль» и получить точки, соответствующие, скажем, «ностальгии» посередине. С другой стороны, embedding – это *«метафизика слова»* без привязки к реальному миру. Он отражает лишь статистическую метрику: слова, используемые похоже, сближаются. Это и хорошо (модель «чувствует» семантические поля), и содержит риски (она унаследует стереотипы и ассоциации корпуса). Тем не менее, embedding-пространство – фундамент, на котором покоится всё остальное: именно поэтому LLM не оперирует словами как символами, а манипулирует **тенями значений** в многомерном пространстве.
> 
> ## 3. **Self-Attention: что именно «внимание» делает на уровне выбора токена**
> 
> **Объяснение:** Self-attention – центральный механизм трансформеров, позволяющий модели **учитывать все предыдущие токены при обработке каждого нового**. Идея в том, что модель распределяет «внимание» между разными частями контекста: важные слова получают больший вес, менее релевантные – меньший. При генерации каждого следующего токена self-attention помогает собрать информацию из всего пройденного текста, чтобы учесть нужные зависимости. Фактически, **каждый токен рассматривает все другие токены перед ним и черпает оттуда нужные сведения**, взвешивая их по значимости【16†L51-L59】. Это решает проблему, с которой сталкивались RNN: сохранение долгосрочных зависимостей. В трансформере любой токен может напрямую взаимодействовать с другим, даже если они далеко друг от друга в тексте, – достаточно, что механизм внимания обнаружит между ними связь.
> 
> **Формализация:** Self-attention оперирует понятиями **Query (запрос)**, **Key (ключ)** и **Value (значение)**. Для каждого токена \(i\) на входе обучаемые матрицы генерируют три вектора: \(\mathbf{q}_i, \mathbf{k}_i, \mathbf{v}_i\). Затем вычисляются *скалярные произведения* между запросом текущего токена и ключами всех предыдущих токенов: \( score_{i,j} = \mathbf{q}_i \cdot \mathbf{k}_j / \sqrt{d} \) (здесь \(d\) – размерность головы внимания, деление на \(\sqrt{d}\) – для стабилизации градиентов)【16†L63-L71】. Эти *scores* нормируются через softmax, превращаясь в веса \(\alpha_{i,j}\), которые показывают, насколько токен \(j\) важен для токена \(i\). Далее строится **взвешанная сумма значений**: \(\mathbf{z}_i = \sum_{j \le i} \alpha_{i,j} \mathbf{v}_j\). В контексте генерации токена \(N+1\), мы берём последний скрытый вектор (предыдущего токена) как «запрос» и аналогично получаем взвешенную комбинацию всех предыдущих скрытых состояний. Эта комбинация несёт в себе агрегированную информацию из всего контекста. 
> 
> Проще говоря, self-attention реализует формулу: 
> \[ \text{Attn}(i) = \sum_{j=1}^{i} \mathrm{softmax}_j(\mathbf{q}_i \cdot \mathbf{k}_j) \cdot \mathbf{v}_j .\]
> 
> Каждый слой может иметь несколько *голов внимания* (multi-head), которые параллельно ищут разные типы связей. Результаты голов конкатенируются и проходят через линейный слой, что даёт обновлённое представление токенов.
> 
> **Пример:** Возьмём предложение: «Вчера Алиса увидела Боба, и она помахала ему рукой». Когда модель генерирует слово «помахала» или следующий токен «ему», важно понять, к кому относится «она». Self-attention на слове «она» (или вблизи него) может сопоставить «она» (query) с «Алиса» (key) – скалярное произведение будет большим, так как контекст подсказывает связь (Алиса – женское имя). Веса attention сместятся к «Алиса». В итоге, представление токена «она» обогатится информацией: *«она» = Алиса*. При генерации «помахала ему» модель через attention также выявит, что «ему» коррелирует с «Боб» (мужское имя). Таким образом, *attentional weights* разрешают местоименные ссылки. Если бы не self-attention, модель могла бы забыть, кому «она» помахала. Каждый attention-head может заниматься своей задачей: одна отслеживает субъект-местоимение, другая – временные связи (например, связывая «вчера» с глаголом прошедшего времени), третья – прямую речь и т.д. В итоге, на каждом шаге *модель «спрашивает» весь контекст: «Какие слова важны для текущего?»* – и переливает нужную информацию в скрытое состояние.
> 
> **Ограничения и эффекты:** Self-attention – мощный механизм, но не магия. Он учитывает **лишь те зависимости, которые модель смогла выучить**. Если в тренировочных данных не было достаточно случаев, чтобы понять длинную зависимость (например, связь из начала романа к концу), модель может её упустить. Кроме того, хотя self-attention в теории видит любой дальний токен, на практике есть *смещение к более близким токенам*: модель часто сильнее фокусируется на недавних словах, особенно в нижних слоях. Это похоже на человеческую краткосрочную память: последние реплики диалога сильнее влияют на ответ. Также, **стоимость внимания** возрастает квадратично с длиной: чем больше токенов, тем больше пар для сравнения. Поэтому очень длинные контексты могут приводить к *размыванию внимания* – модель может затрудняться сфокусироваться, важные детали «утонут» в массе. Исследования показывают эффект *«lost in the middle»*, когда информация, расположенная в середине длинного контекста, получает меньше внимания, чем начало или конец【49†L152-L157】. Это значит, что токены дальнего контекста как бы «конкурируют» с ближними: последние слова (особенно инструкция в конце) могут существенно перевесить то, что было сотни токенов назад. Такое поведение заложено либо обучением (модель видела, что концовки текстов часто содержат важные выводы), либо архитектурно (при ограниченной точности вычислений далекий контекст даёт меньшие градиенты). 
> 
> **Философское замечание:** Self-attention часто сравнивают с человеческим **вниманием**, отсюда и термин. Однако не следует антропоморфизировать: модель не «читает текст и решает, что важнее», как человек; она выполняет матричные умножения. Тем не менее, метафора частично оправдана: механизм обеспечивает *динамическое контекстуальное взвешивание*. Можно сказать, у модели есть *«мета-состояние»* всей предыдущей последовательности – векторы keys и values хранят закодированную историю, а query текущего токена решает, что из этой истории взять. Это придаёт системе некую гибкость: **значение токена становится функцией от всего контекста**. В этом смысле attention – своего рода взаимодействие «всего со всем», схожее с ассоциативным мышлением: модель может ассоциировать далёкие части текста напрямую. Философская грань: мы можем отчасти интерпретировать attention-матрицы – они показывают, какие слова связались – но *понимает ли модель?* Она улавливает статистические соотношения, например, *«она»→«Алиса»* – что можно трактовать как зародыш моделирования смысла (понимание, что «она» – это «Алиса»). Однако все эти связи лишены *ультра-лингвистического* понимания: модель не знает, кто такие Алиса и Боб на самом деле, у нее нет ментальной модели людей – только связи слов. Внимание – блестящий инструмент, делающий токеновые предсказания осмысленными в широком контексте, но граница интерпретируемости пролегает дальше: мы не всегда можем разложить, **почему** конкретная матрица внимания сформировалась так, это распределено по многим головам и слоям. Тем не менее, исследования (например, работы по *«Transformer Circuits»*) показывают, что иногда удаётся приписать определённым головам конкретные функции (например, *индукционные головы* для копирования последовательностей). Это даёт надежду, что уровни выбора токена через attention можно частично визуализировать и понять, хотя и не полностью.
> 
> ## 4. **Механизм контекста: как вся история влияет на текущий шаг**
> 
> **Объяснение:** LLM генерирует текст *авто-регрессионно*: токен за токеном, каждый раз учитывая **всю историю ранее**. Под «контекстным окном» понимают максимальную длину токенов, которую модель способна принять на вход. В современном GPT-4, например, это 32k или даже 128k токенов, в Claude 2 – до 100k+. В пределах этого окна любая часть истории *может* влиять на текущее предсказание благодаря механизмам вроде self-attention. Важное отличие: **это не память в традиционном смысле**, где модель хранит прошлые состояния – у трансформера нет «ячейки памяти» как у LSTM. Вместо этого *каждый новый вывод пересчитывается с нуля* на основе предыдущих токенов: модель получает всю последовательность (обрезанную до окна) и пропускает через слои, получая распределение следующего токена. Таким образом, вся история взаимодействует с текущее словом через attention, как описано выше.
> 
> **Что значит, что контекстное окно – не просто память?** Во-первых, модель не добавляет токены в какое-то хранилище – **она каждый раз получает заново весь текст (или буфер)**. В машинном представлении, она не помнит за пределами того, что ей передано в input. Во-вторых, контекстное окно ограничено по размеру: например, 2048 токенов у старых моделей. Если диалог превышает этот лимит, самые давние токены «вытесняются» – обычно их просто перестают включать в prompt (например, в чатах старые сообщения обрезаются). В-третьих, влияние дальнего контекста **может ослабляться**. Хотя архитектурно нет экспоненциального затухания (как было в RNN), есть эмпирический факт: *эффективная* память модели меньше формальной. Исследования показывают, что открытые LLM часто начинают игнорировать информацию, находящуюся дальше, чем половина максимального контекста, если она не повторяется【17†L23-L27】【17†L33-L40】. Это возможно из-за того, что при обучении средняя длина текстов была меньше максимума, и модель не привыкла регулярно учитывать очень дальние токены.
> 
> **Как токены дальнего контекста конкурируют с ближними?** Через self-attention: каждый новый слой переоценивает важность слов. Если последние 100 токенов содержат прямой вопрос или команду, а тысячи токенов до этого – описание, модель часто **сфокусируется на последней инструкции**. Например, если в начале контекста написано «Задача: расскажи про кошек», а в самом конце дописано «Теперь вместо этого выдай список чисел», то финальное поведение модели подчинится последнему – просто потому, что attention в верхних слоях при генерации ответа придаст решающий вес именно последней фразе (она ближе по позициям и может явно указывать формат вывода). Так, *ближний контекст по практике влияет сильнее на стиль и непосредственный вывод*, а дальний контекст несёт вспомогательные детали и факты, которые модель извлечёт только если сочтёт нужным (например, через созданные связи attention-голов). 
> 
> **Пример:** Представим чат с моделью, состоящий из 15 сообщений, обсуждающих разные факты, и в конце пользователь задаёт конкретный вопрос о том, что было упомянуто. Модель при формировании ответа: 
> - учтёт последние формулировки вопроса (она определяют, в каком формате отвечать, например, «списком» или «подробно»),
> - по ключевым словам вопроса попытается *вытащить* нужную часть контекста из памяти: механизмы внимания будут искать соответствия между словами вопроса и ранними сообщениями. Например, если вопрос: «Какова была статистика роста населения, о которой мы говорили?», attention-сеть вероятно найдёт сообщение, где упоминалась «статистика роста населения», даже если оно было 2000 токенов назад, и подтянет эти цифры в скрытое представление. 
> Если контекстное окно покрывает всю историю, модель **в теории** может вспомнить любые детали. Однако на практике может случиться *конфликт*: предположим, в истории были противоречивые утверждения (сначала сказали, что X=5, потом исправились на X=7). Если оба упоминания ещё в окне, какой выберет модель? Обычно, у трансформеров наблюдается *эффект рецентности*: модель склонна брать более позднее упоминание как актуальное, особенно если формат диалога это подразумевает. Attention тут сыграет свою роль: последняя фраза «Извините, правильное значение X=7» получит высокий вес (ведь она очень релевантна вопросу). Так конфликт сглаживается – модель выдаст X=7. Если бы противоречивые сведения были на равных позициях, модель могла бы запутаться и дать что-то неконсистентное.
> 
> **Ограничения и эффекты:** **Контекстное окно ограничено по длине** – всё, что выходит за его пределы, попросту невидимо модели. Это значит, что у LLM отсутствует долговременная память: например, она не помнит ваши чаты с прошлой недели, если вы явно не загрузите их в prompt. Кроме того, стоимость обработки растёт с длиной: увеличение окна в 2 раза примерно учетверяет время и память вычислений (квадратичная сложность self-attention). Поэтому инженерно приходится балансировать. Уже упомянутый эффект *«lost in the middle»* – важный практический нюанс: если снабдить модель слишком большим объёмом информации, *важные детали могут потеряться посередине*【49†L152-L157】. Разработчики стратегий prompt’ов отмечают, что если, например, подсунуть LLM длинный документ и в конце задать вопрос, стоит дублировать или резюмировать ключевые части ближе к вопросу, иначе модель может их игнорировать. Есть также феномен *контекстного дрейфа*: при очень длинных входах модель иногда начинает генерировать менее связный или более шаблонный ответ, возможно, потому что **внимание «распылилось»** на слишком много данных. 
> 
> **Философское замечание:** Можно спросить: *«а помнит ли модель контекст ‘осмысленно’?»* – ведь она не имеет состояния, но её скрытые представления накапливают информацию о прошлом. В каждом новом скрытом векторе присутствует *суперпозиция* всего необходимого, что было ранее. Это что-то вроде сжатого *«смысла предшествующего текста»* в активациях сети. Такой **латентный контекст** позволяет говорить о *«мета-состоянии»* модели: не памяти в привычном смысле, но внутреннего состояния, несущего информацию о прошлом. Отличие от памяти человека: у человека есть внеязыковые долгосрочные воспоминания, у модели – только то, что зафиксировано в тексте. Но подобно тому, как человек, читая роман, держит в уме образы персонажей и сюжет, LLM в своих активациях «держит» представление прочитанного сюжета. По мере увеличения контекста эти представления могут становиться всё более обобщёнными – детализация снижается (как мы замечаем, читая длинную книгу, помним суть, но можем забыть точные фразы глав назад). В LLM происходит нечто похожее автоматически. Граница интерпретации здесь: мы часто не знаем, *какой именно «образ» контекста сложился у модели*. Мы можем только доверять, что если нужная информация была, модель её использует, но если она где-то «забыла» (то есть не придала внимания в нужном месте), то увы. Появляются идеи внешней памяти (цеплять базы знаний или расширять окно до миллионов токенов), но у них свои технические сложности. Пока же внутренний контекст LLM – вещь эфемерная: он **есть в процессоре на текущем ходу генерации и исчезает после ответа**, и каждый запрос начинает формироваться заново с тем контекстом, что вы дали. Это отличие от человеческого мышления, но придает моделям предсказуемость: *нет скрытых состояний между запросами* (в обычном API-режиме), только то, что явно указано текстом.
> 
> ## 5. **Position Encoding: как модель узнаёт о порядке слов**
> 
> **Объяснение:** Трансформер по своей структуре не имеет встроенного понятия порядка: self-attention оперирует множеством токенов, и без дополнительной информации перестановка слов не изменила бы результатов – модель бы считала фразу «собака укусила человека» и «человека укусила собака» тождественными, если просто дать набор токенов. Чтобы этого не случилось, каждому токену добавляют или внедряют **позиционный код** – вектор, уникальный для позиции 1, 2, 3,… и т.д. **Синусоидальные позиционные кодировки** – один популярный подход (из оригинальной статьи «Attention is All You Need»): они задают для каждой позиции набор значений sin и cos разных частот, что превращает позицию в колебательный сигнал. Идея в том, что относительное смещение между токенами можно вычислить из разности их фаз этих синусоид. В современной практике часто используется **Rotary Positional Embedding (RoPE)** – поворот эмбеддингов на угол, пропорциональный позиции, – по сути, схожая идея с фазами, только реализованная внутри скалярного произведения в attention. Суть: модель получает информацию о том, что токен №5 находится раньше токена №6, а расстояние между токенами – 1. 
> 
> **Зачем нужны синусоиды или rotary?** Они обладают приятными свойствами: *1)* любая позиция кодируется уникально (сами значения sin/cos дают паттерн)【21†L25-L33】, *2)* кодировки плавно меняются от позиции к позиции (sin/cos – плавные функции) – это важно для обобщения, *3)* зашито представление **относительного порядка**: если взять скалярное произведение кодов двух позиций, получится функция от разницы позиций (для синусоид – cos разности фаз). Значит, attention может легко учесть расстояние между двумя токенами. Rotary-кодировка идёт ещё дальше: она *встраивает относительную позицию прямо в вычисление внимания*, так что \(\mathbf{q}_i \cdot \mathbf{k}_j\) автоматически умножается на функцию от (i-j). 
> 
> **Почему порядок – это не просто «место», а фаза?** Можно думать так: позиция – число, но если просто добавить число, возникают проблемы с масштабами (позиция 1000 даст очень большое добавление, сбивая обучение). Синусоиды решают: они ограничены по амплитуде (от -1 до 1) и дают осмысленные повторяющиеся шаблоны – *фазы*. Фаза в данном контексте – это угол на окружности. Для каждой размерности позиционного вектора используется своя частота: например, первая координата меняется как sin(позиция / 10000^(0/две_коорд)), вторая – cos(…) с той же частотой, третья – sin(…) с более высокой частотой и т.д. В результате каждая координата колеблется с определённой частотой при росте позиции【19†L15-L23】. Две позиции, отличающиеся на d, будут иметь предсказуемое отличие фаз: модель может интерпретировать это как относительное смещение. *Проще говоря, порядок кодируется не линейно (1,2,3,…), а как совокупность волн разных периодов.* Поэтому можно сказать, что позиция – **это набор фаз сигналов**, а не просто число.
> 
> **Пример:** Возьмём упрощённый пример с 4-битным позиционным кодированием (аналогия синусоид – просто проще). Для позиций 0,1,2,3,… можно задать коды в бинарном виде: 0 -> 0000, 1 -> 0001, 2 -> 0010, 3 -> 0011, 4 -> 0100, … Это уникальные коды, но заметим, что в двоичном представлении *относительная близость позиций не очевидна* – код 7 («0111») и 8 («1000») совершенно разные, хотя позиции рядом. Синусоидальная кодировка избегает таких скачков: она даст, грубо, для позиций 7 и 8 значения sin/cos, которые отличаются плавно (фаза сдвинулась немного)【21†L9-L17】【21†L19-L27】. Поэтому модель легче учится *интерполировать* на позиции, которых не видела – например, GPT-2 обучен на контексте 1024, но синусоиды позволили ему по существу обобщаться и на большие длины (правда, не идеально). Rotary еще улучшил: модели с RoPE (например, LLaMA) могут extrapolate на длинные контексты лучше, если подкрутить базовую частоту (как показали в работах, увеличив параметр θ). 
> 
> **Ограничения и эффекты:** Абсолютные позиционные коды (как синусоиды фиксированной частоты) имеют периодичность – например, у оригинальных sin/cos период 10000 токенов, после чего они начнут повторяться, что ограничивает максимальную длину. В практике это не проблема, обычно окно меньше. Более важно другое: *модель не знает, когда она близка к концу окна*. Если спросить её с контекстом ровно 2048 токенов, она ничем не сигнализируется, что следующий токен выйдет за границы (кроме косвенно – позиции стали очень большие, sin/cos возможно начинают повторяться). В новых реализациях (RoPE) попытки расширения окна включают *масштабирование θ*: растягивают синусоиды на большую длину. Но если зайти слишком далеко за обученный контекст, LLM начинает генерировать мусор – *контекстная размытия*. 
> 
> **Философское замечание:** Позиционные кодировки – пример того, как модель **вносит понятие порядка пространственно**, а не через изменение архитектуры. Интересно, что позиционный сигнал внедряется *аддитивно* к эмбеддингам или через модификацию формулы внимания (как в RoPE). Это означает: значение слова и его позиция неразрывно связаны в последующих вычислениях. Если убрать позиционность, LLM ломается – она не отличит перестановок слов. Значит, *структура предложения* целиком зависит от этих кодов. Можно сказать, модель знает, что *«первое слово, второе слово, …»* через эти коды. Например, синтаксис (кто подлежащее, кто сказуемое) частично закодирован порядком: в английском «cat chases mouse» vs «mouse chases cat» – разные смыслы. Модель, получив разные позиционные паттерны, выработает разные представления. Граница интерпретации: сами по себе позиционные векторы – абстрактны, но оказалось, что на них тоже можно взглянуть. Синусоиды – понятная структура (их, кстати, не учат, а задают фиксировано), RoPE – тоже задан алгоритмически. Были опыты с *learned positional embeddings* (каждой позиции свой обучаемый вектор) – они тоже работают, но не обобщаются за пределы виденных позиций. Философски любопытно, что **порядок сделан в виде периодических функций**: это напоминает гармонический анализ, разложение последовательности на частоты. Это не случайно: так модель может замечать ритмические структуры языка (рифмы, метр), но главное – различать относительное расположение. *Фаза* – более глубокое понятие, чем индекс: она позволяет сравнивать позиции через интерференцию. Мы видим здесь намёк на физику – информация о последовательности хранится не линейно, а как комбинация колебаний. Эта элегантность – одна из причин, почему синусоидальные/ротари-коды стали стандартом: они **элегантны, эффективны и хорошо масштабируются**【21†L37-L45】, сохраняя порядок без явных дискретных меток.
> 
> ## 6. **Логика выбора токена: как модель «голосует» за слово**
> 
> **Объяснение:** На финальном этапе генерации LLM превращает своё текущее скрытое состояние (тензор, содержащий всю собранную информацию о контексте) в **набор оценок (logits)** – по одному для каждого токена словаря. Эти логиты – сырые числа, не нормированные вероятности. Они показывают, насколько модель склоняется к тому или иному следующему токену. Затем применяется функция softmax: она экспоненцирует и нормирует логиты, превращая их в распределение вероятностей, сумма которых = 1. Таким образом, модель **«голосует»** за все слова сразу, но с разной «громкостью»: слово с самым высоким логитом имеет наибольший вес, с вторым – поменьше и т.д. Softmax как бы отбирает *кандидатов* и говорит: «самый вероятный – этот». Если бы мы всегда брали максимальный, получали бы *жадную генерацию*. Но чаще применяют случайный выбор пропорционально вероятностям (с модификациями типа top-k/p) – об этом далее. Однако сами вероятности – ключевой выход модели, то, что она рассчитала на основе внутренней «логики».
> 
> **Формализация:** Пусть \(\mathbf{h}_N\) – скрытый вектор после последнего слоя для позиции \(N\) (т.е. текущее состояние, от которого хотим предсказать \(w_{N+1}\)). У модели есть **выходная матрица (unembedding)** размером \(d \times V\) – её часто обозначают как \(W_U\) или считают транспонированной embedding-матрицей (в некоторых моделях они раздельные, но идея та же). Умножая \(\mathbf{h}_N\) на \(W_U\), получаем вектор \(\mathbf{z} = \mathbf{h}_N W_U\) размерности \(V\). Компонента \(z_i\) – это логит для i-го токена словаря【4†L119-L125】. Затем: \(P(w_i | \text{контекст}) = \frac{\exp(z_i)}{\sum_{j=1}^{V} \exp(z_j)}\). На практике, из соображений численной стабильности логиты часто вычитают максимум или используют лог-softmax, но концептуально именно так: softmax превращает логиты в вероятность. Отметим, что логит – это не прямая вероятность, а **логарифм несмещенного весового коэффициента**. Разница между логитами важна: если один на 2 больше другого, то вероятность первого ~ e^2 ≈ 7.4 раз выше второго (до нормировки).
> 
> **Что значит «вероятность слова, если модель не знает слов»?** – Модель и не знает *слов как смысл*, но она знает их статистические отношения. Вероятность здесь – модельное доверие, что данный токен уместно продолжит последовательность. Например, после «В небе светит яркое» логиты для «солнце» будут очень высоки, а для «лаконично» – низки. Softmax преобразует эти относительные оценки в удобную шкалу [0,1]. Это *не* калиброванная вероятность реального мира, а скорее *внутренняя степень уверенности модели*. Тем не менее, в задачах моделирования языка принято считать, что хорошо обученная модель оценивает распределение токенов приближенно к их эмпирическим частотам в данных – иначе говоря, если модель говорит 80% на «солнце» после «яркое», то в корпусе действительно в 80% случаев встречалось «солнце» в таких контекстах. 
> 
> **Пример:** Представим контекст: «Париж – столица». Когда модель находит скрытое состояние после «столица», она должна предсказать следующий токен. Допустим, кандидаты: «Франции», «Испании», «город». Хорошая LLM даст логиты: \(z_{\text{«Франции»}} \gg z_{\text{«Испании»}}, z_{\text{«город»}}\). Пусть условно \(z_{\text{Ф}} = 5.0, z_{\text{И}} = 2.0, z_{\text{г}} = 1.0\). Тогда softmax: \(P(\text{Ф}) \approx 0.95\), \(P(\text{И}) \approx 0.04\), \(P(\text{г}) \approx 0.01\). Модель очень уверена, что правильный токен – «Франции». После нормировки это становится распределением. Если мы семплируем, почти наверняка выпадет «Франции». Если же задать температуру = 0 (максимально детерминированно), то выберется просто аргмакс – «Франции». Можно сказать, что модель *«проголосовала»* за слово «Франции» с весом 5, остальные варианты получили гораздо меньшие голоса, поэтому итог практически единогласен.
> 
> **Ограничения и эффекты:** *Логит-ленс* – интересный метод интерпретации: если взять скрытое состояние после какого-то промежуточного слоя и применить к нему выходную матрицу, можно посмотреть, какое распределение токенов модель «думала» на этом этапе【4†L121-L128】. Выясняется, что по мере прохождения слоёв распределение *резко меняется*, сужаясь к итоговому выбору. В ранних слоях может быть много вариантов, а последний слой делает решающее «голосование». Это показывает, что **логика выбора токена распределена по слоям**: каждый вносит вклад. Остаточные (residual) связи помогают суммировать эти вклады. Например, модель может сначала сузить часть речи (предположим, решает, что следующим будет существительное), потом конкретизировать (название страны). Ограничение: softmax имеет тенденцию отдавать очень высокие вероятности топ-1 (особенно у крупных моделей). Это делает вывод детерминированным и может приводить к *избыточной уверенности*. При температуре 1 (стандарт) модели всё равно часто отдают >90% одной опции. Чтобы разнообразить, вводят температурное сглаживание (temperature > 1 делает распределение более равномерным, <1 – более резким). 
> 
> Также, **модель не разбивает “значения” по токенам явно** – она может распределить вероятность между синонимами или разными написаниями. Например, если ответ «в 1990 году», а можно написать цифрами или словами, логиты распределятся между «1990» и «тысяча девятьсот девяностом» токенами. Обычно модель выберет один представленный вариант, не умея осознанно сказать «оба одинаково подходят». Это свойство softmax: он вынужден *дать какой-то выбор*. Потому LLM порой генеративно придумывает что-то, даже если в реальности нет данных – у неё нет механизма выдать «не знаю», кроме как текстом (и то нужно специально обучать на это). 
> 
> **Философское замечание:** Процесс «softmax от логитов» – это и есть конечный **акт «волеизъявления» модели**. До этого у неё были непрерывные активации, неопределённые состояния, но softmax вынуждает точечный исход – конкретный токен. В некотором смысле, здесь решается *«судьба»* генерируемого текста: до softmax это была суперпозиция вероятных слов, после – одно слово. Это напоминает коллапс волновой функции в квантовой механике – но, конечно, чисто математически: стохастический выбор конкретного исхода из распределения. Интересно, что с точки зрения информации softmax – *необратимая операция*: мы теряем неопределённость, фиксируем выбор. 
> 
> Люди тоже, думая, перебирают слова, а потом произносят одно – можно параллель провести. Однако модель не *решает*, она просто следствие алгоритма. Тем не менее, на этом уровне можно задать вопрос: **что значат модельные «вероятности»?** Если у модели 95% на «Франции», это как понимать? Можно трактовать *«как часто такой контекст оканчивается словом Франции»*. Но в реальности контекст «Париж – столица ___» практически всегда заканчивается «Франции». Значит, 0.95 – даже занижено, модель допускает 5% погрешности (возможно, потому что в данных встречались шуточные варианты или ошибки). Другими словами, вероятности LLM отражают опыт корпуса, а не онтологическую истину. Модель может дать высокую вероятность утверждению, которое часто повторялось, даже если оно ложное. Например, может высоко оценить продолжение «на Земле 6 млрд людей» – потому что так писали, хотя сейчас >7 млрд. Она не проверяет реальность – просто выдаёт вероятное по текстам. В теории, *идеальная LLM* при бесконечном корпусе совпала бы с истинным распределением языка, но не с истинным знанием. Поэтому сгенерированную вероятность нельзя принимать как степень истинности – это лишь *статистическая уверенность модели, рожденная её обучением*. 
> 
> На уровне архитектуры последнее линейное преобразование + softmax – наиболее интерпретируемый шаг (можно смотреть на топ-логиты, анализировать альтернативы). Всё, что было раньше – скрыто и сложно для прямого понимания. Многие усилия по интерпретации (логит-линзы, проекции остаточных потоков) нацелены осмыслить, **как ранние слои влияют на финальные вероятности**. Например, обнаружено, что feed-forward слои часто *«проталкивают»* конкретные концепты, увеличивая логиты связанного слова【24†L97-L100】【24†L145-L153】. Но итог – всегда softmax. Он довольно *«плоский»* с точки зрения смысла: это просто математический аппарат, без которого, однако, LLM не могла бы решать задачу моделирования языка. Именно благодаря softmax мы может спросить: *«какое слово следующее?»* – и получить валидное распределение ответов, которым можно оперировать (например, считать перплексию текста, ранжировать продолжения). 
> 
> В завершение: **«вероятность слова» в LLM – это не загадка, а инженерный артефакт**. Модель не думает вероятностями – она выдаёт их как побочный продукт линейной регрессии на большие веса. Но эти веса обучены, и поэтому получаются *осмысленные вероятности*. Они – интерфейс между непрерывным внутренним миром модели и дискретным языком, и на этом интерфейсе мы можем наблюдать, как модель колебалась, прежде чем «сказать» слово. С философской точки зрения, это место, где решается *судьба дискурса* алгоритма: случай бросает кости, и очередной токен появляется на свет.
> 
> ## 7. **Residual и LayerNorm: механизмы стабильности и интерференции**
> 
> **Объяснение:** Архитектура трансформера устроена слоёно, и чтобы эффективно обучать очень глубокую сеть, используются два технических приёма: **резидуальные связи (residual connections)** и **слойная нормализация (LayerNorm)**. *Residual*-связь означает, что выход подблока (например, self-attention или MLP) складывается с его же входом (пропускается «в обход») перед подачей дальше. То есть на каждом шаге модель не перезаписывает полностью представление, а *прибавляет поправку* к уже имеющемуся. Это решает проблему исчезающих градиентов и вообще упрощает обучение глубоких сетей: сигнал ошибки может протекать назад по прямому пути, минуя неоднозначности внутри слоя【25†L41-L47】. *Layer Normalization* – это операция, которая выравнивает распределение активаций, поддерживая нулевое среднее и единичную дисперсию (после линейного преобразования и сложения с резидуалом). Она стабилизирует сеть: значения нейронов остаются в разумных пределах, не взрываются и не затухают. Вместе, Residual + LayerNorm создают *каждый блок трансформера* как: вход -> LayerNorm -> Self-Attention -> суммирование с исходным входом (residual) -> LayerNorm -> MLP -> суммирование с предыдущим результатом (ещё один residual). Благодаря этому выход каждого блока – сумма: **старое состояние + изменение**, а потом нормировка.
> 
> **Почему это важно для выбора токена?** Residual-цепочки делают так, что **финальное представление (в последнем слое) накапливается как сумма вкладов всех слоёв**【24†L61-L69】【24†L139-L147】. Каждый слой может «добавить что-то своё» в поток, не разрушая то, что уже было. Это словно модель пишет промежуточные подсказки на общий «резидуальный поток», который несёт информацию вперёд по слоям. Например, один из ранних слоёв может добавить информацию «это вопросительное предложение», другой – «тематика: география», третий – «спросили про столицу Франции». В итоге, в последнем слое все эти подсказки суммируются, и выходной слой преобразует их в высокую вероятность слова «Париж». Без residual связи каждый слой получал бы только вывод предыдущего и мог бы забывать часть информации. С residual – информация протекает через всю сеть: если какой-то слой не сочтёт нужным изменить компонент скрытого вектора, он просто прибавит 0, и сигнал из прошлого сохранится.
> 
> LayerNorm, со своей стороны, обеспечивает, что резидуальное сложение не разгоняет активации. Без нормализации добавление многих слоёв могло бы накапливать большие значения или приводить к доминированию некоторых признаков над другими. LayerNorm сдвигает и масштабирует вектор активаций, чтобы на выходе каждый канал имел примерно одинаковую статистику. Это упорядочивает взаимодействие между residual-складками: *интерференция сигналов становится контролируемой*. 
> 
> **Формализация:** Если обозначить через \(x^{(l)}\) вход \(l\)-го слоя, через \(A^{(l)}(x)\) функцию внимания (или MLP) этого слоя, то схема: 
> \[ y^{(l)} = x^{(l)} + A^{(l)}(\mathrm{LayerNorm}(x^{(l)})) , \] 
> \[ x^{(l+1)} = \mathrm{LayerNorm}(y^{(l)}) . \]
> Здесь первая LayerNorm нормирует вход перед нелинейностью, вторая – нормирует после суммирования. Итог: \(x^{(L)}\) (после последнего слоя L) ≈ \(x^{(0)} + \sum_{l=1}^{L} \text{something}_l\). Первоначальный вход \(x^{(0)}\) – это эмбеддинги + позиционные коды. Каждый слой дописывает \(\text{something}_l\) – он может быть положительным или отрицательным «выражением» каких-то признаков, которые слой распознал. Например, слой может добавить +3.2 на компонент, связанный с понятием «вопрос», или -1.0 на компонент «утвердительность». Благодаря нормализации, каждое такое добавление проходит в примерно сопоставимых масштабах – сеть не захлёстывается ростом.
> 
> **Пример (интерпретация):** Вопрос: «Париж – столица какой страны?» Эмбеддинги токенов и позиционные коды формируют начальный вектор. Допустим, слой 1 (attention) находит, что слово «Париж» связано с «столица» – он может записать в residual: «концепт(столица->город)». Слой 2 (MLP) может распознать шаблон вопроса – и добавить активацию, сигнализирующую «спросить о стране». Слой 3 (attention) может подтянуть информацию о том, что «Париж -> Франция» (в внутренних весах может храниться такой факт, или найдёт связь через training), добавив в поток вектор, коррелирующий с «Франция». Каждый из них работает параллельно (головы) и последовательно (слои). В конечном \(x^{(final)}\) все эти части сложатся: сигнал, соответствующий «Франция», будет сильным благодаря сумме нескольких вкладов (шаблон вопроса и факт о Париже). Если убрать residual, слой 3 мог бы потерять контекст вопроса. Если убрать LayerNorm, один из сигналов мог бы перегрузить сеть (например, слишком большой выброс какого-то одного признака мог бы заблокировать корректное обучение других). Вместе же они дали *согласованное сложение сигналов*. Из-за этого анализаторы трансформеров говорят о **«residual stream»**: эта сумма накапливается и несёт в себе интерпретируемые части【24†L61-L69】. Были исследования, где пытались проектировать residual поток на пространство слов (через «логит-линзу») после каждого слоя – и действительно видели, как меняется распределение по мере добавления новых компонент.
> 
> **Ограничения и эффекты:** Residual-суммирование имеет и побочный эффект: возможна **интерференция признаков** (superposition). Так как все добавления складываются в один вектор фиксированной размерности, сеть может вынужденно *упаковывать несколько независимых признаков в одну компоненту вектора*. Это похоже на наложение сигналов: разные аспекты смысла могут быть закодированы в одних и тех же активациях и «вычитаться» друг с друга слоями, что затрудняет интерпретацию. Однако с инженерной точки зрения residual – благо: без него глубокая сеть была бы нестабильной и труднообучаемой【25†L41-L47】. LayerNorm тоже имеет тонкие моменты: она делает поведение сети более гомогенным, но может усложнять теоретический анализ (её эффект нелинеен). Некоторые последние работы (например, NormFormer) чуть меняют порядок нормализации или добавляют bias'ы, но суть сохраняется: **нормализация + остаток = стабильность**. 
> 
> **Философское замечание:** Residual connection – простая идея (добавить кратчайший путь для градиента), но её роль в осмысленности модели значительна. Можно представить, что **каждый слой «говорит» что-то, не стирая сказанное до него**. Это напоминает диалог частей модели: одна часть обнаружила одно, другая – другое, и все их открытия складываются в общий «поток сознания» модели (residual stream). Это потокообразное представление и передаётся вплоть до выхода, где интерпретируется. Выходит, *смысл* формируется кумулятивно. LayerNorm выступает модератором этого «разговора», не давая одному голосу перекричать других и приводя все высказывания к общей шкале. Такая перспектива делает трансформер более понятным: не как загадочный ящик, а как иерархия слоёв, где каждый **вносит вклад в выбор токена** (например, экспериментально установлено, что в трансформерах средние и верхние слои больше влияют на конкретный контент ответа, а нижние – на грамматическую связность и стиль). Вся эта конструкция – пример *инженерной элегантности*: Residual связи обеспечивают, что информация может течь свободно сквозь 100+ слоёв, а нормировки – что каждый шаг не испортит статистику. Граница интерпретации: хотя можно в принципе попытаться отследить, что именно суммируется (через методы вроде *басеинового вычитания* или поиска направлений в residual-пространстве), часто признаки суперпозированы, и разделить их сложно. Известно, что можно *принудительно вмешаться* в residual: например, метод ROME (Rank-One Model Editing) позволяет найти в residual пространстве направление «Париж -> столица ???» и отредактировать его, заставив модель, скажем, отвечать, что Париж – столица другой страны. Это говорит о том, что знания хранятся распределённо в residual потоках, но при желании их можно найти и модифицировать【24†L145-L153】. 
> 
> В целом, residual + LayerNorm – скелет, державший «разум» модели на ногах во время тренировки, а теперь держащий его при генерации. Они добавляют модели стабильности, а нам – ещё один уровень для понимания: мы можем думать о процессе генерации как о **сумме смысловых компонент**, накапливающихся от эмбеддингов до выхода. Возможно, в будущем удастся эти компоненты явно идентифицировать (как сейчас делают для простых моделей в интерпретационных исследованиях). Тогда мы приблизимся к объяснению, *почему* модель выбирает тот или иной токен, прослеживая вклад каждого слоя в эти весы.
> 
> ## 8. **MLP Feedforward: зачем нужны нелинейные головы**
> 
> **Объяснение:** Помимо блока самовнимания, в каждом трансформер-слое есть второй важный подблок – **Feed-Forward Network (FFN)**, обычно двухслойный перцептрон с нелинейностью (например, GeLU). Его часто называют «MLP-голова». Назначение FFN – **преобразовать информацию по каждому токену индивидуально**, в отличие от attention, который смешивает информацию между токенами【16†L41-L49】. То есть после того, как self-attention позволил токену учесть контекст, feedforward-слой на этом токене выполняет локальное нелинейное преобразование, извлекая сложные признаки. Эти FFN обладают большей частью параметров модели (примерно 2/3 в GPT). Они увеличивают размерность внутри себя (как правило, скрытый размер FFN = 4 * размер трансформера, например 3072 vs 768) – **каждый такой слой может рассматриваться как набор нейронов-детекторов признаков**, которые реагируют на определённые паттерны в токене и его контексте【28†L287-L295】【28†L323-L331】.
> 
> **Что именно они делают в процессе генерации?** Исследования (Geva et al., 2021【28†L327-L331】) показали, что **FFN-слои действуют как память «ключ-значение»**: первый линейный слой извлекает из токена некоторый «ключ» – паттерн, второй линейный слой при срабатывании этого паттерна добавляет определённое «значение» в residual-поток. Грубо говоря, если self-attention собрал контекст (например, понял, что речь идёт о столице Франции), то FFN может «сказать»: *если контекст = [вопрос о столице, упоминание Парижа], то добавь активацию, соответствующую слову «Франция»*. То есть FFN хранит **ассоциативные правила**: на шаблон X выдай дополнение Y【24†L87-L95】【24†L97-L100】. Поэтому без FFN модель была бы линейна (как вниманием ни смешивай, без нелинейности она – просто комбинация линейных операций). FFN вносит **нелинейность и возможность многозначных соответствий**. Например, один нейрон FFN может срабатывать на конструкцию «столица [города]» и выдавать сигнал «ожидается страна», другой – срабатывать на конструкцию «что такое [слово]» и выдавать определение.
> 
> **Формализация:** В упрощённом виде FFN на токен \(i\) делает: 
> \[ \mathbf{u}_i = \phi( \mathbf{h}_i W_1 + b_1 ),\] 
> \[ \text{FFN}(\mathbf{h}_i) = \mathbf{u}_i W_2 + b_2, \] 
> где \(W_1\) расширяет размерность (например, 768 -> 3072), \(\phi\) – нелинейность (GeLU), \(W_2\) сжимает обратно (3072 -> 768). Матрицы \(W_1\) содержат «ключи», \(W_2\) – «значения». Конкретнее, \(W_1\) можно мыслить как набор строк-векторов: каждая строка – веса нейрона, умножаясь на \(\mathbf{h}_i\), даёт скаляр – активацию нейрона до GeLU. GeLU пропускает положительные активации (усиливает их), подавляет отрицательные – так происходит отбор признаков. После нелинейности только «сработавшие» нейроны остаются существенно !=0. Затем \(W_2\) берёт линейную комбинацию активных нейронов – фактически складывает их вклады. Если нейрон k активировался и \(W_2\) имеет значимые веса в его колонке, то во выходной вектор добавится компонент, соответствующий этому нейрону. Этот механизм позволяет FFN хранить много разных правил. Ключевое отличие от attention: FFN не смотрит на другие токены, но благодаря attention вход \( \mathbf{h}_i \) уже содержит контекст, значит, FFN реализует **контекстно-зависимую трансформацию токена**.
> 
> **Пример:** Рассмотрим задачу простого арифметического рассуждения: «Если у тебя было 3 яблока и дали ещё 2, сколько стало?» Self-attention может связать «3» и «2» с вопросом «сколько стало?», понять, что это сложение. Но *кто выполнит сложение 3+2=5?* Вероятно, какой-то FFN-нейрон обучен реагировать на паттерн [число, ещё, число, сколько стало] – активируется, и через \(W_2\) добавляет компонент, соответствующий числу «5», в скрытое состояние. Конкретный нейрон может хранить такую операцию как правило: if (pattern) then add (ответ). Или, например, FFN-нейрон может хранить факт: [Париж – столица ???] -> (Франция). То есть attention мог не явно «знать» про Францию, он просто подтянул нужный контекст – например, держит в vectors само слово «Париж». А FFN выступает как память: «Париж -> Франция». В известных исследованиях *Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space* было показано, что некоторые активации FFN можно интерпретировать в словарном пространстве: прибавление выходного вектора FFN с определённым индексом повышает логит конкретного слова【24†L97-L100】. Это прямо указывает: **FFN «голосуют» за слова**. Они делают это не прямо (через residual), но влияние прослеживается. К примеру, авторы нашли, что определённые нейроны в GPT-2 отвечали за грамматическое число: если модель определяла подлежащее как множественное, FFN-нейрон добавлял активацию, которая в дальнейшем повышала вероятность соответствующей множественной формы глагола.
> 
> **Ограничения и эффекты:** Так как FFN – основной нелинейный компонент, большая часть параметров и вычислений приходится на них. Это значит, что **большая часть знаний и способностей модели закодирована именно в весах FFN** (в сочетании с attention, конечно). Выяснилось, что многие знания типа фактов хранятся именно в весах FFN второго слоя (в матрице \(W_2\)), то есть они скрыты в этих огромных матрицах в виде *распределённых ключей-значений*【28†L327-L331】. Это затрудняет точечную интерпретацию: в attention мы можем хотя бы визуализировать матрицы внимания, а FFN – это непрозрачные весовые матрицы. Но известные случаи (как нейрон *«Santa = Claus»* в GPT-2, который при активации выдаёт слова про Санта-Клауса) показывают: иногда нейрон FFN несёт *очень конкретный концепт*. В больших моделях таких нейронов сотни тысяч, и каждый – суперпозиция многих узких правил.
> 
> Ещё эффект: из-за residual FFN-выход суммируется со вниманием. Их *совместное действие* и приводит к решению. Иногда FFN могут исправлять или компенсировать ошибки внимания. Например, если attention по какому-то странному контексту неправильно привязал местоимение, FFN на следующих слоях могут обработать грамматические несоответствия и подправить (возможно, за счёт того, что attention выход «не согласуется» в некой компоненте, FFN заметит и сгладит). Однако FFN не может общаться между токенами – он индивидуален для каждого, так что глобальную переоценку контекстных связей делает опять следующий attention. В сумме они чередуются: attention «собирает», FFN «преобразует». И так слой за слоем.
> 
> **Философское замечание:** Feed-forward слои – можно сказать, *«подсознание»* модели. Если attention – это «фокус внимания» на разных частях предложения, то FFN – **внутренние раздумья**, протекающие параллельно по каждому токену. Они вносят элементы рассуждений, память и знаний, которые не явно присутствуют в тексте. Например, человеческий аналог: читая «Париж – столица …», у нас всплывает знание «Франции» – не потому что мы его видим, а потому что мозг хранил факт. Аналогично, LLM FFN-слой может «вспомнить» требуемое слово. Этим FFN напоминают нейронную память, как выяснили исследователи: *«feed-forward слои в трансформере работают как key-value память, где каждый ключ коррелирует с текстовым паттерном, а каждый value индуцирует распределение на выходном словаре»*【28†L327-L331】. Поразительно, как эта память формируется: просто градиентом, без явного хранения знаний, а выходит, что хранит! Граница понимания здесь проходит через суперпозицию: одно и то же пространство FFN может хранить тысячи фактов, наложенных друг на друга. В последнее время возникли идеи *посреднических токенов* или *external knowledge networks*, но классический трансформер справляется встроенными FFN.
> 
> В контексте «уровней» принятия решения, FFN – **уровень архитектурного контекста**: они не завязаны на непосредственные слова, а работают в латентном пространстве, *внося новые признаки, отсутствующие явно в тексте*. Это их философская роль – обогащать внутреннее представление смыслом, который не был выражен входными токенами, но следует из них. Без этого LLM была бы как марковская цепь: взяла последние слова – выдала по ним следующий. С FFN она делает скачки: может логически комбинировать сведения (хотя логика – отдельный разговор, но простые шаги она делает). В FFN кроются и «нелогичности» – там могут возникнуть ложные ассоциации или biases: например, паттерн «она инженер» может ассоциироваться у модели с чем-то (из-за стереотипов данных) и FFN добавит лишнее. Это сложнее отслеживать, чем bias в attention.
> 
> Но в целом, **MLP-головы – мотор семантической обработки**. Если представить трансформер как аналитический механизм, attention – это чтение «данных» из памяти (контекста), а FFN – вычисления над ними. Без них LLM не сочиняла бы, не делала бы логических связок – она была бы линейным сплавом входных слов. Нелинейные головы придают модели *«искру мысли»*. Конечно, это метафора: фактически они – тысячи матричных умножений. Но именно там, в массивах весов, лежат, к примеру, факты из Википедии или грамматические правила. Расслоение знаний в FFN – активная область исследований: её хотят использовать для редактирования моделей (редактируя веса FFN, можно «переучить» факт) или для извлечения интерпретаций. Для нас же важно: **FFN-слои значительно влияют на выбор токена, добавляя нужные компоненты к активациям**, и тем самым поднимая вероятности определённых слов (как обнаружено экспериментально【24†L97-L100】). Они – невидимый режиссёр за кулисами, который подсказывает attention-блокам, что сказать дальше.
> 
> ## 9. **Rotary Embedding и альтернатива позиционным кодировкам**
> 
> **Объяснение:** *Rotary Position Embedding (RoPE)* – это усовершенствование абсолютных позиционных кодировок синусоидами. В RoPE вместо простого сложения позиционного вектора к эмбеддингу, реализуется **вращение подпространств эмбеддинга** на угол, зависящий от позиции токена. Проще говоря, для пары координат в эмбеддинге (например, 0-я и 1-я) задаётся угол поворота θ*pos, и векторы Query и Key поворачиваются на этот угол. В результате, скалярное произведение Query-Key двух токенов будет содержать множитель cos(Δpos) – то есть *зависеть только от разницы их позиций*【29†L23-L30】. Rotary-кодировка сохранила преимущества синусоидальной (относительная позиция явно представлена), но оказалось более *устойчивой к экстраполяции*: модель может обрабатывать более длинные последовательности, чем обучена, особенно если скорректировать базовый масштаб θ. 
> 
> **Как RoPE меняет представление позиций?** Он делает позиционирование **мультипликативной операцией на векторах** вместо аддитивной. Если синусоиды добавляются к эмбеддингу, то в RoPE позиционное смещение реализуется как \( \text{RoPE}_\theta(\mathbf{x}) = R(\theta) \mathbf{x}\), где \(R(\theta)\) – блок-диагональная матрица поворота, состоящая из 2x2 матриц 
> \[ \begin{pmatrix}\cos \theta & -\sin \theta \\ \sin \theta & \cos \theta\end{pmatrix} \]
> для каждой пары координат (всего d/2 таких блоков на вектор). Каждая пара координат как бы образует комплексное число, умножаемое на \(e^{i\theta \text{pos}}\). Так Query и Key токенов i и j получают фазы θ*i и θ*j, их скалярное произведение включает фактор \(e^{i \theta (i-j)}\), который даёт cos и sin зависимости от разницы. Это означает, что *модель лучше понимает относительное смещение*. В абсолютных синусоидах тоже была относительность, но RoPE встроил её непосредственно в формулу внимания.
> 
> **Почему он работает лучше при больших контекстах?** Отчасти потому, что можно менять базовую частоту. У синусоид базовая частота (большой период) жёстко зафиксирована ~10000, а у RoPE – можно задать параметр θ. Эксперименты показали, что увеличив θ (делая вращение медленнее), можно расширять контекст без переобучения – просто модель начинает воспринимать разницу позиций и на больших расстояниях тоже (это называют *Scaling RoPE for long context*【32†L31-L34】). Также RoPE проще комбинируется с механизмами *сдвига позиции* (например, ALiBi – attention с учетом расстояния) – правда, ALiBi и RoPE – разные подходы. 
> 
> **Пример:** Взглянем на LLaMA, Mistral – они используют RoPE. Скажем, LLaMA-2 70B обучена на 4k контекста. Но с помощью трюка *NTK scaling* (θ уменьшают), её контекст растягивали до 16k с приемлемой качеством. Это работает, потому что RoPE-коды при меньшем θ изменяются медленнее, и модель может обобщить на большее число позиций. Если бы были просто синусоиды, такой прямолинейный трюк мог бы не помочь, потому что вне обученного диапазона синусоиды все равно периодичны и модель бы путалась.
> 
> **Ограничения и эффекты:** RoPE – это альтернативный *способ вшивания порядков*; для самой логики выбора токена он прозрачен – он влияет на то, **как attention видит далекие токены**. В практике отмечено, что RoPE даёт более плавное снижение внимания с расстоянием, что улучшает *долгосрочную связность ответа*. Однако, никакая позиционная кодировка не избавляет от квадратичной сложности – они лишь улучшают представление порядка. Для очень длинных контекстов (миллионы токенов) требуются другие архитектуры (например, Reformer, или линейные внимания). Но RoPE – хорошая компромиссная: она сохраняет абсолютную позицию (модель всё же знает, кто первый токен, а кто сотый – через свою фазу), но главное – обеспечивает легко извлекаемую относительную позицию. 
> 
> **Философское замечание:** Если провести аналогию, синусоиды – как если бы модель воспринимала позицию как набор **фиксированных волн**, а RoPE – как будто модель видит **фазовые смещения** между словами. В человеческом языке порядок принципиален (перестановка слов меняет смысл), и трансформер научился учитываться порядок через такую *«волновую механику»*. RoPE занимает интересное философское место: он **«закручивает» представление**, вводя элемент геометрии (вращения) во взаимодействие токенов. Это изящно: относительное положение – ключевой фактор смысла (разница между «кот съел рыбу» и «рыба съела кота» – порядок). Через ротации LLM не хранит номер позиции явно, она **чувствует сдвиги**. Это ближе к тому, как люди воспринимают речь: мы не сохраняем индекс каждого слова, но понимаем, кто за кем был. 
> 
> Наличие разных подходов (RoPE, ALiBi, PER, RPE и пр.) показывает: порядок можно кодировать по-разному, и от этого зависит, насколько хорошо модель *обобщает* за пределами тренировочных длин. RoPE – удачный компромисс, поэтому он повсеместно принят (Meta в LLaMA, OpenAI в GPT-4 тоже, вероятно, используют что-то подобное, хотя точно неизвестно). 
> 
> В контексте «уровней влияния»: позиционные кодировки – **альтернативный механизм**, но он сильно связан с attention. Rotary – просто вариант, который улучшил limitations синусоиды. Это напоминает эволюцию идей: сначала фиксированный синус (как in Vaswani et al. 2017), потом замечают – можно лучше (про Relative Position Encoding заговорили примерно в 2018, потом RoPE в 2021). Эти улучшения – *инженерные доливки* к основному механизму внимания, но они важны: LLM с RoPE лучше понимает длинные документы, *держит весь контекст под контролем*. А значит, при выборе токена может учитывать дальние зависимости: например, если в начале текста был персонаж, а в конце снова про него – модель не забудет, кто это, лучше сохраняя когерентность. *Философски*, это делает модель более похожей на человека, способного помнить начало истории, хоть детали могут подзабыться («lost in the middle» правда всё еще бывает). 
> 
> Подытожим: **Rotary embedding – усовершенствование, которое меняет архитектурный уровень кодирования позиций**. Оно влияет на выбор токена опосредованно: улучшая работу внимания, особенно на больших дистанциях. Как слой влияния, RoPE – пример, как можно *альтернативно заложить фундамент порядка*. Но принцип остаётся: порядок – фазовый, относительный. А RoPE – просто более гибкий способ внедрить эту фазу, сохраняя преимущества синусоид и добавляя возможное масштабирование. В конечном итоге, использование RoPE сделало LLM более способными обрабатывать большой контекст без дополнительных хитростей – и это сильно расширило границы интерпретируемости (мы теперь можем вести длинный диалог с моделью, зная, что она помнит ранние реплики достаточно хорошо).
> 
> ## 10. **Активации, энтропия и temperature: как регулируется «решительность» модели**
> 
> **Объяснение:** *Temperature* в сэмплировании LLM – коэффициент, регулирующий **степень случайности** в выборе следующего токена. Формально, при temperature T логиты делятся на T перед softmax: если T > 1, деление на больше 1 делает распределение ближе к равномерному (повышает энтропию), если T < 1 – наоборот, усиливает разницу, делая модель более решительной (понижает энтропию). В пределе T=0 – модель всегда берёт наиболее вероятный токен (детерминированный вывод), T->∞ – все токены почти равновероятны (полный рандом). Таким образом, temperature – главный параметр, управляющий **«творческостью» vs «точностью»**: низкая T даёт предсказуемые, часто правильные, но возможно скучные ответы; высокая T даёт разнообразие и неожиданность, но рискует уйти в бессмыслицу. 
> 
> **Как температура влияет на выбор?** Она *математически масштабирует энтропию* распределения. Энтропия – мера неопределённости: \(H = -\sum p_i \log p_i\). Когда T увеличена, вероятности выравниваются, энтропия растёт – модель менее уверена, выбор шире. При T снижается, энтропия падает – распределение концентрируется на одном-двух токенах. Из-за экспоненты softmax даже небольшое изменение T может сильно изменить shape распределения. Например, если логиты [5,4,3] (экспоненты ~ [148,54,20], вероятности ~ [0.65,0.24,0.11] при T=1), то при T=0.5 логиты станут [10,8,6] – экспоненты ~ [22026, 2981, 403], вероятности ~ [0.85, 0.11, 0.02] (более решительно на первом), а при T=2 логиты [2.5,2,1.5] – экспоненты ~ [12,7.4,4.5], вероятности ~ [0.40,0.25,0.15] (более размазано; суммы не до 1 из-за примера, но тренд виден). То есть **temperature – ручка, управляющая уверенностью модели**【34†L90-L98】【34†L99-L107】.
> 
> **Почему слишком высокая T вызывает «галлюцинации»?** Потому что модель начинает выбирать токены, которым при нормальном softmax она бы дала низкую вероятность (то есть малообоснованные продолжения). Высокая энтропия = модель фактически *не очень «думает», что говорит*, она как бы выбирает случайно из множества возможных слов. Тогда последовательность может легко уйти от смыслового русла, особенно если одно-два случайно выбранных слова задали неверный тон. На практике, если поставить T=2 или 3, у GPT-like моделей текст быстро распадается на бессвязный поток. Например, при слишком высокой температуре на запрос про столицу Франции модель может с некоторой вероятностью ответить «Испании» или даже «42» – потому что её встроенные вероятности уже не доминируют, и даже нелепые варианты получают шанс.
> 
> **Почему слишком низкая T = скучный/детерминированный текст?** При T→0 модель всегда берёт самый вероятный токен. Это часто приводит к повторяемости (например, модель может зациклиться, если наиболее вероятный следующий токен – то же слово, а затем опять, и т.д.). Кроме того, без случайности модель не генерирует вариантов выражений: она будет формулировать ответы всегда одинаково. Например, на множество похожих вопросов ChatGPT при T≈0 ответил бы почти одинаковыми фразами – никакой креативности. RLHF обычно настраивает ответы, так что наиболее вероятный – безопасно-вежливый; при нулевой температуре это становится единственным ответом.
> 
> **Пример:** Попросим модель придумать продолжение истории. Если T=0.8 (умеренно), она может выдать интересный, связный сюжет с парой неожиданных деталей. Если T=2, она, возможно, начнёт абсурд: «...И вдруг кошка полетела в космос, потому что сахарный персик забыл выключить утюг.» – что-нибудь бессвязное, т.к. случайность позволила несовместимым идеям просочиться. Если T=0, она может продолжать самыми ожидаемыми клише: «Жили-были... и жили долго и счастливо.» – ничего особо нового. 
> 
> **Ограничения и эффекты:** Стоит понимать, что temperature не делает модель умнее или глупее – она только влияет на выбор из *того распределения*, что модель уже придумала. Если модель имеет неправильные склонности (например, галлюцинировать факты), то при высоком T это усугубится (она чаще выберет ошибочный токен), а при очень низком T – возможно, уменьшится (будет цепляться за наиболее вероятный, который если повезло верный). Но увы, часто наиболее вероятный для нее токен тоже может быть галлюцинацией, если она уверена в неверном. Исследования показали, что *просто ставить T=0 не гарантирует отсутствие галлюцинаций* – модель может очень уверенно галлюцинировать【34†L121-L129】. Temperature – инструмент, а не панацея. 
> 
> **Связь с энтропией:** В ML говорят про *«confidence vs diversity»*. Temperature – регулятор diversity. Энтропия распределения на выходе – количественная мера diversity. Если задать temperature, можно замерять среднюю энтропию ответов модели. Чем выше энтропия – тем менее повторимо и более широко распределены ответы. Это полезно в творческих задачах: стихи, идеи – лучше при T около 1 или чуть выше. Для фактических задач, где важна точность – T ниже 1, чтобы модель не отклонялась от наиболее вероятного (обычно корректного) ответа【33†L23-L31】【33†L39-L46】.
> 
> **Философское замечание:** Temperature – интересный параметр, потому что он как бы *вводит волю случая* в детерминированный алгоритм. Можно видеть это так: при обучении модель оценивает вероятности, а при генерации *мы решаем*, как трактовать эти вероятности – как почти детерминированный выбор (низкая T) или как приглашение к случайному творчеству (высокая T). Человеческая речь тоже обладает вариативностью – мы не всегда говорим одно и то же одной фразой, у нас есть *«температура»* мышления, можно сказать. Когда мы говорим официально – мы сдержаны (низкая энтропия речи), когда фантазируем – позволяем любым мыслям приходить (высокая энтропия). В LLM мы получили ручку управления этим аспектом.
> 
> Граница интерпретации: непосредственно temperature не часть модели, а *параметр сэмплирования* – внешняя по отношению к основному механизму. Поэтому его просто понимать (математически) и легко контролировать. Но он влияет на финальный выбор существенным образом, становясь одним из **«инструментов смысла»** в руках пользователя. Хотим прямолинейного ответчика – ставим T=0 (и может top-1 отбор), хотим музу – ставим T=1.2 + nucleus sampling. В каком-то смысле, temperature – это кусочек философии о **свободе воли** модели: при T=0 у неё почти нет свободы – она говорит самое закономерное, при T>1 – у неё *слишком много свободы*, что скатывается в хаос. Так же и человек: слишком строго следуя шаблонам – скучен, слишком хаотично – бессмыслен. Лучшее – где-то посередине.
> 
> Для понимания внутренностей: иногда анализируют **активации нейронов и энтропию**. Если модель очень уверена, некоторые выходные нейроны сильно больше других. Temperature сдвигает эти активации (по сути делит все на T). Это можно представить как *«охлаждение» или «нагрев»* выходного распределения – отсюда и название. Физическая аналогия: высокая температура – система (модель) «возбуждена», беспорядок велик, низкая – «охлаждена», стремится к порядку (энергетически минимальному состоянию). Интересно, что эти понятия заимствованы неслучайно: softmax с T напоминает Больцмановское распределение, где T – физическая температура, логит – отрицательная энергия состояния, а вероятность – распределение Гиббса. Так что у LLM даже *выбор слов можно трактовать физически*: токен – состояние системы, logit – энергия, softmax – вероятность в тепловом равновесии при T. Когда мы «нагрели» модель, она готова прыгнуть в состояние с вышеэнергетическое (менее вероятное) чаще, когда «охладили» – падает в минимумы энергии (самые вероятные слова). 
> 
> В конечном счёте, **регулируя temperature, мы внешне влияем на стиль генерации**, не меняя внутренних весов. Это удобный рычаг, но, конечно, он не улучшает понимание: если вопрос сложный, слишком высокая T лишь выдаст бред с большей вероятностью. Поэтому обычно рекомендуют: *для правильных ответов – T пониже, для творчества – повыше*. Модель же сама не меняется – меняется процесс сэмплинга. Энтропия выхода – мерило ее неопределенности: если вопрос модель знала точно (например «2+2=?»), энтропия низкая; если вопрос двусмысленный или моральный – распределение может быть широким (весы между несколькими вариантами), энтропия выше. Мы, наблюдая вероятности, можем судить, насколько модель «решительно» склоняется к чему-то. Temperature же решает, нужно ли нам это её колебание, или мы хотим генерировать иногда другой вариант. 
> 
> Итак, temperature – завершающий штрих в механизме выбора токена, позволяющий ввести случайность контролируемо. Внутри модели уже все решено (логиты), а вот *как превратить их в слово*, решает softmax и температура. И если весь остальной сложный трансформер – мозг, то temperature – *«настроение»* модели: можно сделать её более осторожной или более дерзкой в речи, просто крутя эту ручку.
> 
> ## 11. **Кросс-токеновая интерференция: конфликты и согласования**
> 
> **Объяснение:** В длинном тексте или диалоге разные части могут давать противоречивую информацию. Например, сначала сказали «Боб – доктор», потом «Боб – художник». Как LLM решает, что сказать, если вопрос: «Кто такой Боб?» Такой *конфликт контекста* разрешается не через какое-то осознанное логическое рассуждение, а через **механизмы внимания и резидуала**, которые мы обсудили. Предыдущие токены *конкурируют* за влияние на скрытое состояние текущего токена. Если информация противоречит, модель даёт взвешенный компромисс или выберет ту, что сильнее представлена. Обычно поздние токены имеют больше веса (*эффект рецентности*), но есть и *эффект авторитетности*: если часть контекста явно более релевантна (например, вопрос непосредственно ссылается на первый факт), attention может вернуться к раннему факту. **Self-attention действует как арбитр**, распределяя коэффициенты. Если предыдущие токены несут конфликт, модель может *усреднить их представления*, сглаживая противоречие, либо – что случается чаще – обратить внимание на один из конфликтующих фрагментов, игнорируя другой. Благодаря residual-складу, *обе версии могут существовать в скрытом представлении параллельно*, пока какой-то слой (или выход) не «сделает выбор».
> 
> **Что происходит при противоречии?** Возьмём пример: в тексте сначала: «Вася любит кошек», потом: «Вася ненавидит кошек». Если спросить: «Любит ли Вася кошек?», модель видит конфликт. В скрытых состояниях слово «любит» и «ненавидит» – противоположные признаки. Возможно, разные attention-heads сослались на разные предложения. В итоге, hidden state может содержать *смешанные сигналы* (часть нейронов предполагает «любит», часть – «не любит»). Финальное логит-распределение может оказаться неуверенным: вероятности «да» vs «нет» ближе к паритету. Если forced to answer, она выберет что-то, возможно последнее упоминание («ненавидит»), т.к. recency. 
> 
> **Как attention сглаживает или усиливает конфликты?** В системе обучения LLM нет специальной процедуры для логического согласования – она полагается на то, что человеческий текст обычно самосогласован, либо содержит явные разрешения (например: «раньше думали A, но потом оказалось B» – модель поймёт, что B актуально). Если таких подсказок нет, attention чаще отдаёт предпочтение ближнему контексту, как отмечалось: *«подверженность последней информации»*. Например, было исследование: модель, увидев два противоречащих источника в retrieval, правильно выбирает нужный 74.7% случаев【38†L1-L4】 – т.е. не всегда правильно, но часто. Это означает, LLM склонна либо *рационализировать*: возможно, output скажет что-то вроде «Есть разные сведения: сначала сказано то, потом это…», или – если не обучена на сомнения – просто выберет одно. RLHF-инструктированные модели часто **обходят противоречие**, указывая на конфликт («Извините, я вижу противоречие…»), *если* задать им соответствующий контекст. Это не гарантируется, но заложено в fine-tuning: быть честным о неуверенности. В base-модели без инструкций она бы просто выбрала наиболее вероятный ответ (на основе biases корпуса: если последний сказал «Вася ненавидит», она скажет «ненавидит» без комментариев).
> 
> **Пример:** Предположим контекст: «Учёные заявили X. Позже другие исследователи опровергли X.» Вопрос: «Правда ли X?» Хорошо настроенная модель (e.g. GPT-4) может ответить: «Ранее утверждалось, что X, но позже это опровергли, так что нет, X – неправда.» Это демонстрация, что модель *разрешила конфликт* – она уловила, что второе предложение отменяет первое. Почему так получилось? Вероятно, внимание обратило сильный вес на слова «опровергли X» – может, ключевые токены «опровергли» trigger-нули FFN нейрон, понимающий отрицание. Тогда hidden state воплощает «не X». И ответ следует «нет, X ложен». Если бы контекст был менее явным, модель могла замешкаться. Например, без слова «опровергли», просто «... заявили X. ... другие исследователи сказали Y (противоположное X)», модель может колебаться. Она взвесит: какая часть текста ближе к вопросу? Последняя, скорее. AttentionHeads, обученные на аргументацию, могут посмотреть на оба предложения (две разные heads) – и intermediate layers FFN могли бы генерировать meta-признак «конфликт». Если RLHF был, финальный ответ может дипломатично упомянуть обе стороны. Без RLHF – могла бы сказать прямо вторую версию.
> 
> **Ограничения и эффекты:** У LLM нет гарантий согласованности: она может легко противоречить сама себе, особенно без инструкций. Например, в отсутствие специального штрафа, модель может в одном абзаце утверждать X, а в следующем – Not X, если так сложились вероятности фраз (часто случается в халлуцинациях). Это отражает ограничение: *у модели нет глобального контроллера правды*. Кросс-токеновая интерференция решается локально на уровне attention слоя за слоем, но не единым «умом». Поэтому LLM склонны **галлюцинировать противоречивые факты**: один head, видя подсказку, тянет какой-то суждение, другой – другое, а final output может соединить их некорректно.
> 
> Тем не менее, большие модели демонстрируют улучшенную способность разрешать противоречия, вероятно благодаря обучению на диалогах и QA, где им показывали примеры с конфликтами. Как отмечалось, *LLM может правильно решить конфликт в ~74%*, но не 100%【38†L1-L4】. Это значит, иногда она выберет неверный источник. 
> 
> **Как attention «сглаживает»?** – Бывает, что модель выдаёт размытый ответ: «В некоторых источниках говорится X, в других – Y». Это попытка сочетать конфликтующие сигналы: residual поток несет признаки обоих утверждений, а выходная логит-линза показывает примерно равный вес. Модель тогда выбирает слова, описывающие ситуацию (если обучена так: GPT-4 обычно именно так поступает по сложным вопросам – перечисляет обе стороны). Можно сказать, attention *распределил почти поровну вес*, FFN *не добавили решающего факта*, и модель не уверена – тогда output генерируется, отражая эту неуверенность (в «умных» моделях; более простые могут просто случайно выбрать одно).
> 
> **Философское замечание:** Конфликты в тексте ставят границу интерпретируемости LLM: мы видим, что модель не имеет *единого мировоззрения*, она – продукт контекста. **«Мнение» модели – это функция последних токенов**. Это похоже на отсутствие «я»-центра: LLM не может сама решить, что истинно, она просто перерабатывает вход. Это отличие от человека, у которого есть устойчивые убеждения и критическое мышление. LLM лишь симулирует их, когда обучена: если в data был pattern «когда два источника спорят, AI отвечает дипломатично», она так и сделает.
> 
> Граница смысла здесь: можем ли мы сказать, что LLM «понимает» противоречие? Частично да: она обнаруживает несоответствие (в hidden state появляются противоположные признаки). Но *понимает ли, что они не могут одновременно быть истинными?* – это выходит за просто сопоставление токенов, требует элементарной логики. Большие модели способны (в рамках векторной магии) решить простые противоречия, но сложные – нет. Например, GPT может поймать противоречие «Вася кошку любит – Вася кошку ненавидит», но сложное косвенное – может и пропустить.
> 
> В случае примешивания несовместимых токенов residual stream – уникальное место: **там лежат все версии сразу до разрешения**. Некоторые исследования интерпретируемости пытаются вычленить «сырые идеи» в residual. В идеале, можно было бы даже **редактировать противоречие**: убрать компонент, отвечающий за ложный факт, усилить за истинный – и модель ответит правильно. Пока это сложно, но точечные правки (как ROME) подтверждают: можно найти в пространстве признаков конкретный факт и поменять.
> 
> Из этой темы вытекает важное: LLM нельзя считать эпистемически надёжной без внешней валидации – она не умеет гарантировать несамо-противоречивость. Ее рассуждения могут «дрейфовать» в тексте. ChatGPT иногда видно: начинает отвечать одно, потом исправляется – это буквально «слои attention/FFN переиграли решение». 
> 
> **Внутри-слойная интерференция:** Eще аспект: *head mixing*. Если два attention head тянут разные места, их выходы складываются (residual). Они могут частично погасить друг друга (если добавили противоположные активации). Или LayerNorm сгладит экстремы, чтобы конфликт не вызывал взрыва. Это можно назвать **«сглаживанием через нормировку»**. 
> 
> В итоге, *кросс-токеновая интерференция* – нормальное явление, потому что текст часто содержит либо повторяющуюся, либо разностороннюю информацию. LLM научена среднестатистически обходиться: или синтезировать (когда возможно – т.е. сделать вывод, как из нескольких подсказок), или при невозможности синтеза – либо выбрать последнее, либо выразить неопределенность. Это отражает ограниченную «когнитивную» способность: нет сознательного критического мышления, только шаблоны.
> 
> **Вывод:** Self-attention не гарантирует согласования всего со всем, но чаще ставит *локальную связность выше глобальной*. И хотя модель иногда заметно противоречит себе, более новые – с явным fine-tuning – имеют механизм избегать уверенных ложных утверждений («стохастический папагал» учится говорить «я не уверен...»). Философски, это показывает, что LLM – *существо контекста, а не убеждений*. Каждое новое слово определяется предыдущими: если там хаос, будет хаос на выходе; если там упорядочено – выход упорядочен. Никакого «единого Я», следящего за непротиворечивостью, нет. 
> 
> Граница интерпретации: мы пока не умеем точно предсказать, как модель решит конфликт – можно лишь примерно, наблюдая за training. Но снаружи, **видя ответы, мы можем оценивать энтропию и структуру её неопределенности**, что даёт нам сигнал: она *сама не уверена*. Хорошим подходом стало бы: если модель «сомневается», она должна говорить: «существует противоречие». Это даже внедряется – chain-of-thought побуждает её рассуждать, и если она выявит конфликт – она придёт к отказу от однозначного ответа. 
> 
> Таким образом, **кросс-токеновая интерференция – это уровень проявления ограничений модели**: где кончается плавное предсказание и начинаются либо ошибки, либо отказ. Это напоминает, что под капотом LLM нет цельного знания, а лоскутное одеяло статистик, сшитое attention. И на швах этого одеяла иногда торчат нитки противоречий.
> 
> ## 12. **Top-k и Top-p sampling, Beam Search: инженерные выборки**
> 
> **Объяснение:** Помимо temperature, существуют и другие стратегии постобработки вероятностного выхода – чтобы улучшить качество или разнообразие генерации. **Top-k sampling**: вместо выбора из всего словаря, модель берёт *только k наиболее вероятных токенов* и нормирует вероятности среди них (остальным присваивает 0)【34†L109-L117】. Это ограничивает выбор наиболее правдоподобными продолжениями, отсеивая длинный хвост маловероятных (часто бессмысленных) токенов. **Top-p (nucleus) sampling**: выбирается наименьшее множество токенов, чья суммарная вероятность ≥ p, и сэмплируется из них【34†L111-L119】. Т.е. модель на лету определяет, сколько токенов включить – если распределение «острое», возьмет только 1-2 (сумма быстро ≥0.95 например), если «плоское», возьмет больше. Это адаптивный вариант top-k. **Beam Search**: совсем другой подход, часто применяемый в детерминистских задачах (перевод, резюме) – модель поддерживает несколько кандидатов *последовательности* (beams) параллельно, расширяя каждый, и всегда продолжает наиболее вероятные комбинации. В конце beam search выбирает наиболее вероятную полную последовательность. Это помогает избежать локально оптимального, но глобально плохого продолжения. Однако beam search обычно ведёт к более однообразным, излишне «средним» текстам, а также был замечен за склонностью к повторениям (модель выбирает однотипные high-prob слова, что зацикливает)【40†L280-L289】【40†L290-L299】.
> 
> **Почему не всегда берётся самый вероятный токен?** Выбор тупо argmax (называемый *greedy search*) может приводить к очень предсказуемым и часто скучным выводам, а также *застреванию* в повторениях или шаблонах【39†L9-L17】. Например, модель может выбрать «самое вероятное слово» – если текст генерируется в шаблоне (скажем, она описывает утро героя – «он проснулся, потом он сделал то, потом он...») – greedy может зациклить «он» постоянно, т.к. это наиболее вероятный токен в начале многих предложений. Сэмплирование же позволяет иногда выбирать синонимы, новые конструкции, обходить повторения. 
> 
> **Как эти стратегии искажают смысл?** Они не *искажают* его как таковой, скорее *могут его корректировать или разнообразить*. **Top-k/p** отсекает редкие слова – иногда редкое слово было бы к месту (с точки зрения сюрприза или точности), но не попало в топ-k. Это немного «отрезает хвост распределения», убирая возможность неожиданного, но возможно верного ответа. Зато устраняет очень неуместные. Nucleus sampling (top-p) более мягко делает похожее: гарантирует, что мы охватываем ~95% массы (например при p=0.9 или 0.95). Тем самым мы почти никогда не берём токен из «длинного хвоста» <5% вероятности – где часто опечатки, случайности. Результат: ответ более связный. Но и оригинальность чуть снижается: модель может не использовать редкие слова или внезапные повороты, которые всё же были маловероятны.
> 
> **Beam search**: он стремится максимизировать общую вероятность предложения. В задачах, где необходимо идеально перевод или логически связное резюме, beam search давал выигрыш. Однако в открытом тексте (story) beam search, как обнаружилось, **делает текст скучным и повторяющимся**【40†L280-L289】. Это потому, что наиболее вероятное продолжение часто – шаблонное, предсказуемое. Beam search вылавливает именно такой – средний по корпусу – вариант. Пример: *для истории horror beam search может приводить к клишированному финалу, тогда как сэмплирование могло дать неожиданный твист*. 
> 
> **Пример:** Скажем, задача: сгенерировать продолжение предложения «Жил-был дракон, который...». Greedy (или beam search 1 луч) возможно даст: «...жил в пещере и охранял сокровища.» (очевидно, максимально вероятное клише). Top-5 sampling может иногда вместо «охранял сокровища» выбрать «...скучал в одиночестве» или «...терзал деревни по ночам». Это разнообразие. Beam search с 3 лучами, к примеру, рассмотрит: (1) «охранял сокровища», (2) «жил в замке», (3) «ел рыцарей». Затем оценит, что «жил в пещере и охранял сокровища» – самый вероятный полностью, и выдаст его. То есть beams не выбрал более интересные, потому что у них суммарная вероятность фразы меньше. 
> 
> **Почему sampling стратегии иногда «искажают смысл»?** – Если перестараться: например, top-k с k=2 фактически превращает модель почти в детерминированную – она будет метаться между 2 высокочастотными продолжениями, возможно, цикля. Или nucleus p=0.5 – очень агрессивно, может отрезать нужные слова (если модель имела распределение, где нужное слово было 4-м, но суммарно первые 3 уже дали 0.5). Поэтому параметры надо подбирать: обычно k=50, p=0.9 – эмпирический баланс для текстов. 
> 
> **Beam search недостатки:** Как сказано, *репетиция*. Известно, что beam search без специальных штрафов склонен генерировать повторяющиеся фразы – потому что повторение часто моделью оценивается вероятным (она уже сказала «он пошёл домой. Он пошёл домой.» – второе «он пошёл домой» иногда имеет высокий условный prob из-за недочётов модели). Beam search, максимизируя вероятность, выбирает повтор. Поэтому вводят *no_repeat_ngram_size* – эвристика, запрещающая повтор 2-3-словных фраз【40†L189-L197】【40†L284-L293】. С ней beam search лучше, но всё равно склонен к более монотонной форме (без неожиданных слов). 
> 
> **Top-k vs Top-p:** Top-p считается обычно более умной: она адаптивна. Например, если модель сильно уверена (один токен имеет 90% вес), nucleus p=0.9 возьмет только его – не будет зря шуметь. Top-k=50 в такой ситуации всё равно возьмёт 50 токенов, включая очевидно неуместные, и может случайно выбрать из них, что плохо. То есть *nucleus автоматически подстраивается*. В другом случае, если распределение равномерно (модель не уверена), p=0.9 может включить много вариантов (может ~100 слов), а фиксированный k=50 всегда 50. Поэтому nucleus, как правило, даёт более «уверенные» ответы, не лазя в заведомо маловероятное, сохраняя при этом вариативность, когда модель действительно не знает. 
> 
> **Философское замечание:** Все эти стратегии – *инженерное управление стохастичностью*. Поскольку LLM *не оптимизирована* на качество текста в глазах человека (она оптимизирована на логп-наивысшую вероятность), её самые вероятные продолжения не всегда совпадают с «наилучшими» по стилю или содержанию. Это парадокс: наиболее вероятный текст – часто перегружен клише или бесконечно подробен (например, language model может удлинять предложение, добавляя высоко вероятные в целом слова типа «очень», «действительно» – что делает текст унылым). Сэмплирование *искусственно делает текст более «человеческим»*, потому что люди не всегда выбирают самое предсказуемое слово – есть элемент вариативности. Исследование Holtzman et al. (2019) про *«The Curious Case of Neural Text Degeneration»* показало, что **human-like text имеет высокую перплексию (много удивляющих токенов)**, а текст от greedy/beam – слишком низкую (слишком предсказуемый)【40†L290-L299】. Поэтому nucleus sampling (который они предложили) – способ поднять перплексию текста ближе к человеческой, избегая «вырождения» (repetition loops). 
> 
> С философской точки зрения, *кто контролирует смысл*: модель предложила вероятностный ландшафт, но *человек решает, как извлечь из него ответ*. Это интересный момент: меняя decoding, можно из одной модели получить разные «персональности». Например, с высоким T – она более творческая, с низким – строгая. С nucleus p маленьким – она будет прямолинейна, с большим – более размыта. Beam search, можно сказать, отражает *«договороспособность»* – он пытается сделать самый «корпусно-вероятный» текст, это ближе к общему стилю. Sampling – добавляет *свободы воли* модели. 
> 
> **Почему sampling стратегии иногда считают «искажают смысл»?** – Потому что могут вводить токены, которых модель бы не выбрала, будь она рациональна. Например, top-k с низким k может *исключить правду*. Представим, модель раздаёт вероятности: «Правильный ответ – 40% A, 30% B, 30% C». Greedy выберет A (хотя B и C тоже близки). Nucleus p=0.9 возьмет все три (потому что суммарно нужно 0.9, возьмет A,B,C) – и может выбрать B, хотя B немного менее вероятно, чем A. Это *нарушило ранжирование модели*: возможно, A был правдой. Поэтому sampling иногда выдаёт альтернативный, менее вероятный вариант ответа – если повезет, это будет равноценный вариант (синоним, другой стиль), а если нет – может быть хуже. 
> 
> В итоге: *«Почему не всегда самое вероятное?»* – потому что это делает текст однообразным и иногда скучным. *«Как sampling и beam search искажают смысл?»* – beam search делает его более шаблонным (и потенциально повторяющим), sampling – может увести в сторону, если модель была не уверена. 
> 
> **Вывод:** Это всё – решения *инженерные*, надстройки над вероятностным выводом, призванные **компромиссно управлять риском и разнообразием**. Опять параллель с людьми: у одного речь более шаблонна (низкая энтропия), у другого – экспрессивна, с неожиданными словечками (высокая энтропия). В LLM у нас есть рычаги, чтобы отрегулировать. *Смысл* как таковой, заложенный моделью, sampling не улучшит – он лишь влияет, как этот смысл выражается или насколько он точно следует вероятностным оценкам. 
> 
> Граница интерпретации: при оценке качества вывода нужно учитывать decoding. Один и тот же вопрос, заданный модели, может получить разные ответы под разными стратегиями, даже противоречивые – но это не потому, что «модель решила иначе», а потому что *мы вытащили другую реализацию стохастического процесса*. Чтобы правильно судить способности модели, иногда запускают **«self-consistency»**: многократно сэмплируют её chain-of-thought, а потом берут наиболее частый результат. Это пользуется тем, что случайность даёт много возможных решений, но правильный (если она его знает) будет часто появляться. *Self-consistency* – яркий пример: мы осознанно используем sampling (разные CoT) + beam search (выбираем наиболее согласованный ответ). Это улучшает точность решающих вопросов. Опять-таки показывает: *добавив случай, мы парадоксально улучшаем итог, усредняя потом*. 
> 
> В целом, **декодирующие стратегии – последний уровень принятия решения**: модель дала распределение, а дальше – 
> - greedy (всегда пик),
> - beam (глобальный пик последовательности),
> - stochastic (образно, бросаем кость согласно вероятностям). 
> 
> Выбор стратегии – *внешний контроль смысла*, позволяющий немного отклоняться от заложенного. Искажение или улучшение – зависит от цели. Если цель – строгость (математический ответ) – лучше без случайности, если цель – интересный рассказ – сэмплирование. Поэтому, когда пользователь говорит: «Ответь кратко точно» – внутренне можно снизить температуру и пр. Если «Придумай сказку» – поднять.
> 
> Итак, **эти инженерные выборки существуют, потому что «наиболее вероятное» ≠ «наиболее качественное/интересное»**. А LLM, будучи просто вероятностной, нуждается в таких надстройках для разных применений. Это иллюстрирует отличия между просто статистикой языка и желаемым выходом.
> 
> ## 13. **Hyperparameter-induced bias: влияние архитектурных решений**
> 
> **Объяснение:** Под *гиперпараметрами архитектуры* мы подразумеваем такие параметры, как **число attention-heads, число слоёв, ширина эмбеддинга, размер feed-forward слоя, тип активации, и т.д.** Эти решения, заложенные разработчиками модели, влияют на её поведение. Например, **количество attention-heads** задаёт, сколько различных отношений модель может *параллельно* учитывать. Больше голов – потенциально модель может улавливать больше разных типов связей одновременно (синтаксис, семантика, дальние совпадения имен и т.п.). Меньше голов – она может *пропускать* некоторые зависимости. **Размер hidden state** (например, 4096 vs 512) определяет *ёмкость представления*: больше размер – модель может кодировать более тонкие различия в контексте и хранить больше информации. **Глубина (число слоёв)** влияет на способность к сложным многошаговым выводам: больше слоёв = больше нелинейных преобразований = более сложные функции, которые модель может выучить (в пределе – более глубокие рассуждения). Таким образом, архитектура как бы задаёт *потолок возможностей*. 
> 
> **Как они меняют поведение модели на уровне выбора токена?** Часто – количественно, не качественно: все трансформеры работают похоже, но мелочи отличаются. Например, **увеличив число голов**, можно заметить, что модель лучше справляется с многозначными референциями (одна head занимается одной ссылкой, другая – другой) – то есть выбор токенов становится более точным при сложных контекстах【41†L5-L12】【41†L27-L34】. Но чрезмерное число голов без увеличения размера эмбеддинга может быть лишним: если embed=192, а heads=32 (каждая голова ~6-дим), эксперимент показал – производительность даже упала【42†L123-L131】, потому что *головы мешали друг другу*, не хватало размерности (это назвали «attention bottleneck»). То есть, просто увеличить heads – не всегда лучше: у модели может возникнуть *переизбыточность*, некоторые головы будут нефункциональны или ловить шум, что может ухудшить вывод (например, она может начать лишние токены приплетать). **Размер feed-forward**: больше – больше «нейронной памяти». Скажем, GPT-3 175B с большим FFN (нейрон=12288 dims) явно хранит много фактов. Если FFN меньше – модель могла бы забывать или не улавливать редкие паттерны. 
> 
> **Пример влияния:** Сравним две модели: 6 слоёв vs 24 слоя (другие параметры равны). На сложный вопрос «Сколько ног у паука?» – обе могут ответить «8». Но если вопрос: «Сколько ног у осьминога плюс паука?» – 6-слойная может запутаться (неправильно суммировать), а 24-слойная – правильно (8+8=16) благодаря более глубокой цепочке, которая может реализовать рассуждение (на самом деле, без chain-of-thought обученной, не факт, но глубина помогает). Или: маленькая модель (N=6, d=256, heads=4) в диалоге может забывать детали упомянутые 100 токенов назад (не хватает емкости хранения, attention-глав), а большая (N=32, d=2048, heads=16) – запомнит и вернётся к ним. Значит, на уровне выбора токена маленькая модель может *делать ошибки по памяти или контексту*, а большая – правильно учитывать всё сказанное. 
> 
> **Bias, вызванный архитектурой:** Например, **модели с малым числом attention-heads склонны к более «локальной» генерации** – каждый attention «видит» ограниченные типы зависимостей, может преобладать n-граммная связность, а дальние взаимоотношения проигнорируются. Это *bias к более простым, шаблонным продолжениям*. Модели с большим числом слоёв могут иметь *bias к многословности*: было замечено, что очень глубокие модели (GPT-3) без регулировки склонны генерировать более подробные, длинные ответы – возможно, потому что на каждом слое добавляется какая-то деталь. Меньшие модели – наоборот, иногда слишком кратки, потому что их «мышление» меньше шагов.
> 
> **Есть ли архитектуры, которые выбирают токены иначе?** – Да, например: **RNN vs Transformer** – RNN (LSTM) имеет последовательный характер, и было отмечено, что LSTM-LM чаще застревают на повторе фраз, у них меньше «глобального» контекста. Трансформеры выбирают более осмысленно дальние зависимости (поэтому они их заменили). **Mixture-of-Experts (MoE)** – архитектура, где на каждый токен активируется лишь подмножество экспертов-FFN, – может по-другому влиять: такие модели могут хранить отдельные факты в разных экспертах, их ответы могут быть более или менее детальны. В одном исследовании (Switch Transformer) показано, что MoE-модели при том же количестве параметров лучше отвечают фактические вопросы – т.е. *выбор токена более точен на фактах*, потому что один из экспертов как бы «специалист» и его вес сильно повышает нужное слово. 
> 
> **Пример другой архитектуры**: **Retentive Network** (см. 2023 г.) – предлагает заменить softmax-atten на экспоненциальный затухающий след (что убирает квадратичность). Это архитектурно меняет, что модель склонна к более свежим контекстам (старое забывает экспоненциально). Значит, её выбор токена при очень длинном контексте будет bias к последним кускам, даже если ранние важнее (ещё сильнее recency bias, чем обычный). **Encoder-Decoder** (например, T5) vs Decoder-only (GPT) – разные: encoder-decoder, обученный на seq2seq, при генерации ответов учитывает *целиком вопрос закодированный*, и некоторые замечали, что он может быть более «выверенным» и не повторяет вопрос – потому что архитекторно отделён encoding input. Decoder-only (GPT) просто продолжает, и может повторять часть вопроса в ответе (что видим иногда: ChatGPT может начать ответ «Вопрос: ... Ответ: ...» – у него тенденция включать вход). То есть архитектура (наличие encoder) устраняет этот bias. 
> 
> **Ограничения и эффекты:** **Dimensionality bottleneck**: если hidden size слишком мал, модель вынуждена суперпозиционировать сильно – возможны конфликты, и некоторые тонкие различия токенов теряются. Будет bias к «обобщенным» ответам (не выделяет тонких разниц). **Количество параметров** – крупный гиперпар. По сути, большие модели >10B параметров показывают *новые качества (emergent abilities)*. Т.е. архитектурное наращивание (в ширину/глубину) привело к тому, что они начинают решать задачи, которые меньшие (на тех же данных) не могли. Это явно влияет на выбор токена: напр. маленькая 1.3B модель на вопрос «что происходит если бросить мяч вверх?» может выдать абсурд, а 175B – правильно: «он упадет вниз из-за гравитации». Архитектура (scale) – ключ.
> 
> **Философское замечание:** Гиперпараметры – как «мозговые анатомические особенности» модели. У человека: сколько нейронов, какие области развиты – влияет на способности. У LLM: архитектура – определяет *пространство функций*, которое она может реализовать. Меньшая модель – узкомыслящая, большая – богатомыслящая. Конечно, это грубо, но подтверждено: более большие модели не только знают больше фактов, но и проявляют больше *«рассудочности»*, следуют инструкции лучше, планируют (chain-of-thought) лучше. Это emergent поведение, связанное с scale. Считается, что**нейросети обладают фазовыми переходами**: преодолев некий критический размер или данные, внезапно качество скачет непропорционально. Это вроде «магии» – но физическое объяснение: увеличилось число параметров → они смогли приблизить более сложную функцию. 
> 
> **Пример частного гиперпараметра – активация GeLU vs ReLU**: GeLU плавная, ReLU резкая. ReLU мог бы сделать модель более «молчаливой», зануляя некоторые нейроны. Возможно, LLM с ReLU имела бы более отрывистые ответы (часть признаков тупо теряется). GeLU – дает частичные активации, более гибко. 
> 
> **«Архитектуры, выбирающие токены иначе»** могут быть совсем альтернативными: например, **диффузионные LM, RNN LM** – но пока transformers правят. Или **квантовые-inspired LM** – но это в теории. 
> 
> **Bias («предвзятости») архитектурные:** Еще: *позиционный bias* – модели с абсолютным позиционным encoding (как GPT) имеют bias к началу или концу контекста (подсознательно могут считать слова в начале текста значимее). Relative encodings (как T5) – возможно, меньше bias. *Длина контекста:* GPT-2 с контекстом 1024 может начать *топтаться* если превысить ~800, т.е. generate gibberish – bias к рандому после «потери памяти». GPT-4 (128k) – держит нить долго.
> 
> **Вывод:** Гиперпараметры – они как *правила игры* для модели. Поменяй – и она играет иначе. *Выбор токена* – итог этой игры, значит, архитектура влияет на его вероятности. Are there architectures that choose tokens «иначе»? – Если взять, к примеру, **скользящее окно (наподобие Transformer-XL)**: она, получая длинный текст, вместо полного контекста использует память. Это *иначе*: может забыть глобальный порядок. Generations c Transformer-XL имели known issues (в 2019): могли повторять сюжетные арки, забывая, что уже было, – т.к. memory window finite.
> 
> **Философски:** Интересный момент: *при той же тренировке, архитектура определяет, ЧТО модель может выучить и КАК она мыслит.* Это как разные мозги. В контексте токен-селекции: мы видим, что GPT-3 и GPT-2-trained-differs qualitatively – GPT-3 может усложненно продолжать, GPT-2 попроще. *Душа модели* – её веса, но *личность – от архитектуры*. Arch bias – о которых user обычно не думает, но они там. 
> 
> Вопрос: **«Есть ли архитектуры, которые выбирают токены иначе?»** – Например, **MEM (максимум энтропии)**: можно принудительно выбирать не самый вероятный, а такой, чтобы распределение поддерживалось – это как training objective, но not generation. 
> **RL-tuned architectures**: RLHF – не архитектура, а fine-tune objective, но меняет: модель выбирает токены, оптимальные по reward (человеческое одобрение). Это *по-другому*, чем обычный next-token. Т.е. RLHF-модель может *нарочно занижать* вероятность токсичных слов (architecturally, не, weight-wise), и «иначе» выбирать токены (не те, что training LM objective сказал, а смещенные). 
> 
> В summary: **архитектурные решения определяют границы способности модели к сложным зависимостям, памяти и рассуждению**, и через это – качество и характер выбираемых токенов. Можно назвать это *«порождаемым bias'ом»*: небольшая модель может предпочитать частые шаблонные слова (потому что не разглядела детали вопроса), большая – более точные термины. В литературе отмечалось, что увеличение размеров снижает стилистические перекосы (например, GPT-2 мелкий давал более телеграфный стиль, крупный – плавнее). 
> 
> **Граница интерпретации:** не всегда ясно, как именно конкретный параметр (скажем, heads=16 vs 32) изменит поведение – нужны эксперименты. Например, Medium vs Large BERT – заметили, что Large лучше справляется с coreference – значит, head/hidden increase помог. Редко анализируют heads count vs performance: один результат – «увеличение heads сверх определенного не улучшало качество, а иногда даже вредило»【42†L123-L131】. То есть, *есть оптимумы*. 
> 
> **Конец:** Hyperparameters – вспомогательный уровень влияния: они не формируют смысл явно, но без достаточных ресурсов модель «мысленно» примитивна. В контурах рассуждения LLM, архитектура – фундамент, аналог *«врожденных возможностей»*. И её bias – иногда даже важнее, чем learning bias: если архитектура не может выразить определенную функцию, она её не выучит. Например, RNN ограничены короткой памятью – сколько ни учи, долго контекст не удержат, ergo – выбор токенов через 1000 токенов уже практически наугад. Transformer overcame that – и, как видим, с большим контекстом стал возможен связный output. Это прямой показатель: *архитектура позволила иначе выбирать токены (с учётом дальних данных)*. So yes – архитектуры выбирают по-разному.
> 
> ## 14. **Искажение семантики через токенизацию**
> 
> **Объяснение:** *Токенизация* – разбиение текста на токены (части слов, слова, символы). Современные LLM используют субсловный токенайзер (например, BPE – Byte-Pair Encoding). Он часто ломает слова на кусочки. Например: «pineapple» -> токены «pine» + «apple»【44†L27-L30】, «New York» -> «New» + « York». Это означает, что модель не видит цельное слово, а видит последовательность частей. Обычно это не проблема для частых слов: например «apple» – один токен, «pineapple» – два токена, но модель, обучившись, узнает, что [«pine», «apple»] часто идёт вместе, и воспринимает как концепт ананаса. Но в некоторых случаях токенизация может **искажать смысл или усложнять интерпретацию**. Например, сложные редкие имена могут разбиваться на бессмысленные части – модель может их путать с другими словами. Или слова с общим префиксом: «apple» и «appleton» – первый токен в обоих «apple», модель может частично перенести семантику яблока на «Appleton» ошибочно.
> 
> **Почему «apple» и «pineapple» – не просто два слова?** Потому что «pineapple» внутри содержит «apple», и токенизатор может не обязательно отдаёт приоритет смыслу. BPE алгоритм сжимает частотность: «pineapple» чаще встречается как слово, поэтому часто токенизаторы имеют токен «pineapple» цельный (например, у OpenAI GPT-2 был токен «pineapple» в словаре). Но если бы не было, разбив на «pine» + «apple» – модель должна двусмысленно обрабатывать: «pine» – ассоциации с сосной, «apple» – с яблоком, а речь об ананасе. Она должна через контекст понять, что «pineapple» – это фрукт, не связанный ни с сосной, ни с яблоком прямо. И модель способна: нас спасает, что *embedding словаря* и *последующие слои* обучились закодировать, что последовательность [«pine», «apple»] обычно означает ананас (векторное сложение/резидуал, etc.) и выдавать соответствующее представление. Однако, могут быть случаи: например, слово «undeath» (нежизнь) может токенизироваться как «un» + «death» – модель вероятно поймёт, но она никогда прямо не видела токен «undeath» как единицу. Или наоборот, слово «embedding» у GPT-2 – токены «em», «bed», «ding» – три части, ни одна не означает «встраивание». Модель должна комбинацией их значений плюс контекст догадаться. Можно ожидать, что модель *немного хуже* понимает редкие слова, состоящие из множеств токенов, особенно если комбинация встречалась мало. 
> 
> **Как токенайзер ломает смысл?** Ещё пример: **разбиение по границам морфем** не всегда совпадает с реальными морфемами. Например, BPE для «alignment» мог выдать «ali» + «gnment» – ни одно не является осмысленной частью. Модель видит странные куски – она просто обучена статистически. Обычно, это не критично: она просто ассоциирует последовательность «ali», «gnment» с контекстами alignment. Но если слово очень редкое, она может не выучить. Это – *проблема OOV (out-of-vocabulary) mitigation*, BPE её сокращает, но OOV заменяются кусочками. 
> 
> **Пример искажения:** Есть печально известный пример: GPT-2 не умел правильно считать буквы в словах, отчасти из-за токенизации. Слово «ABC» токенизируется как «A», «B», «C» – ок. Но, скажем, «Ω» (омега) – могло токенизироваться как неизвестный юникод, или «xEF x9F xA0» (условно) – куча байт. Модель вообще не понимает. Ещё: *двуязычные имена*. «Москва» в кириллице OpenAI модель (english-trained) не знала – она их разбивает на каждый символ, embeddings случайны, output бессмысленный. 
> 
> **Смысл** может искажаться: например, **составные слова**. «Apple» vs «pineapple» – если б «pineapple» не был одной лексемой, модель могла подумать, что «pine apple» – «яблоко сосны» – nonsense. Она вероятно так не сделает, потому что частотность «pineapple» большая. Но **менее частые составные**: «light-head» vs «lightheaded» – GPT может ошибиться, если токены отличаются. 
> 
> **Конструирование смысла через токенизацию:** Вдобавок, токенизатор **создаёт нелинейности**: например, «apple» – один токен, embedding E1; «pineapple» – два токена, embedding E2+E3. Если какой-то механизм (FFN) запомнил, что E1-> [яблоко], а E2-> [сосна], E3-> [яблоко], то residual=E2+E3 – не равно E_pineapple, а = E_sosna + E_apple. Модель должна через нелинейности сконвертить (в FFN or attention). 
> 
> **Bias, вызываемый токенизацией:** 
> - **Префикс-бейс**: Слова, начинающиеся одинаково, частично сходятся в embedding. Например, «psychology» и «psychopath» – первый токен «psycho» общий, модель может путать тонкости. 
> - **Space representation**: В BPE GPT-2, пробелы – часть токенов. Есть отдельный токен «ĠNew» vs «New». Mодель differently scores leading spaces. 
> - **Trick**: by constructing weird splits, можно заставить модель err. Например, OpenAI tokenizer: «Sal» + «vatore» vs «Salv» + «atore» – slight difference. 
> - **Attacks**: известны *typos or unfamiliar unicode infiltration attacks*, где добавляют невидимый юникод, модель разркывает слово на странные токены -> output weirdness. 
> 
> **Пример ухудшения**: Пусть Q: «Define the word 'antidisestablishmentarianism'.» Это длинное редкое слово. GPT может:
>  - токен: 'anti', 'dis', 'establish', 'ment', 'arian', 'ism'.
>  Она знает 'anti' (против), 'establish', 'ment', 'ism' – возможно, FFN sum yields 'opposition to disestablishment of church, etc.' (if training data had it). Если не, она может partial meaning: 'It is some political movement.' 
>  Or take **compound languages**: German, e.g. "Donaudampfschifffahrtsgesellschaftskapitän" – extremely long word. GPT's BPE might break it too many pieces, likely it fails define fully. 
> 
> **Философия:** Токенизация навязывает модели определённую разбиение реальности. Она не оперирует натуральными слова – она оперирует кусочками. *Значит, само понятие «слова» у модели размыто.* У неё есть токеновый смысл. Вembedding space модель группирует токены: 'apple' token vs 'apple' as part of 'pineapple'. Кстати, BPE обычно делает так: 'apple' – отдельный токен, 'pine' – отдельно, 'apple' – reused in pineapple. Embedding 'apple' thus used for fruit context and in 'pineapple'. Possibly, 'apple' embedding mostly fruit, so 'pineapple' vect. = E_pine + E_apple might have fruit component – making model treat pineapple more like apple than it is. 
> But likely attention heads resolved. It's subtle. 
> 
> **Сборка смысла из токенов**: 
> - *Prefix trickiness:* GPT-2 had token for "The" separately, that resulted in to-be interfering with grammar. 
> - *Token biases:* Some tokens might carry sentiment. Eg. 'good' vs 'goodness' (broken 'good' + 'ness') – 'good' positive, 'ness' neutral. Combined might confuse nuance. 
> - *Bad splits example:* "Barack Obama" -> "Bar", "ack", " Obama" – token 'Obama' separate with leading space, 'Bar' 'ack'. Embedding 'Bar' might link to bar (pub), ack no meaning. Possibly harming coherence. 
> But likely model overcame since 'Barack' common splitted thus. 
> - *Lost meaning of morphological cues:* e.g. tokenization can break negations ironically: "unbelievable" = "un", "believable". It might help (embedding 'un' signals negation, good). If break weirdly, can lose. 
> Let's say "inconceivable" not a single token – if broken unnatural, could confuse. 
> 
> **Ограничения и эффекты:** 
> - **Out-of-vocab not exactly solves**: Model sees unknown chars as weird tokens, often it generates unpredictable output (like repeating them or ignoring).
> - **Evaluation difficulty**: If we want model count letters in word, tokenization gets in way: It doesn't see letters individually, except through weird bridging or if fine-coded to handle that (rare).
> - **Robustness**: Adversaries exploit tokenization weirdness. Eg. one can add nonprinting characters, splitting known triggers – model fails (some 'prompt injection' via splitting stop sequence).
> - **Tokenization can entrench biases**: Eg. if 'man' one token vs 'woman' subword 'wom'+'an' – model might embed them differently. (Though in GPT BPE both likely one token each). But "female" vs "male" – not sure if one token each. If female broken, could bias? Possibly not big. 
> But known that BPE prioritizes frequent sequences, possibly common male terms got single token, female forms broken, causing slight disadvantages.
> 
> **Философское замечание:** Токенизация – человеческий выбор (OpenAI or others design it). Он определяет *алфавит мыслей модели*. Если мы разбиваем "apple" – модель понятие яблока хранит ровно на этом одном токене. If it was broken, it had to compose concept, which is cognitively harder (like human reading unknown compound, he deduces from parts). 
> Thus, **tokenization influences cognitive style**: Models with char-level tokens (like charRNN) vs word-level vs BPE vs SentencePiece – all behave a bit differently. Char-level can, in theory, learn to spell, but slower to converge on meaning of words; BPE – good compromise. 
> 
> One can even see tokenization as bias pipeline: e.g. if domain words (medical terms) mostly broken to pieces, model might be weaker on them than on everyday words that are whole tokens. So domain knowledge partially hindered by tokenization. Newer models consider dynamic tokenization or 100k vocabulary (less breakdown). 
> 
> **Case study:** GPT-3 had weird token "Ġ911" meaning " 911" (with space), it would sometimes interpret emergency call incorrectly because of that? Not sure, but such tokens exist – they learned context in weird lumps. 
> Also "The Matrix" vs "Matrix" tokens diff – might cause model to separate concept.
> 
> **Synthesizing meaning:** There's concept of "multi-token representation" – e.g. how model sees an emoji (which is often multiple bytes => multiple tokens). Many models struggled to correctly name certain emojis, probably cause tokenization splitted unnatural.
> 
> **Symbology:** If we consider certain phenomena (like algebraic expressions, or ASCII art), tokenization could heavily distort original structure, making model job harder. Eg. drawing ASCII cat – model might break each row unnatural, messing it up.
> 
> **Conclusion:** Токенизация – невидимый, но важный слой: он может *ломать целостность семантических единиц*, а модель должна чинить это через обучение. В большинстве случаев она научилась: "pine"+"apple" = concept [pineapple]. But weird or rare combos can slip: i.e., GPT often mispronounces or splits hashtags or userIDs incorrectly (lack knowledge cause it's unnatural splits). 
> Philosophically, it reminds that **модель не видит мир как люди (слова, понятия) напрямую, а через осколки, которые мы ей дали**. Её понятия – реконструкция. 
> Граница интерпретации: tokenization limits e.g. how large number can be – GPT-2 couldn't handle big numbers cause they splitted digit by digit or chunks. Mistral uses "100000" as single token maybe – helps. 
> So improvements in tokenization (like cl100k for GPT-4, bigger vocab) aimed to reduce such distortions. 
> 
> **Sum:** *Семантика может искажаться токенизацией*, но большие модели компенсируют много. Yet, understanding these quirks is key for tasks like open vocabulary, or figuring weird outputs – sometimes root cause is token splitting. 
> For instance, some have shown that GPT-3 "prefers" certain outputs because of token length (like "the 80s" vs "the eighties" different token lengths, different probability due to training freq). This is subtle tokenization effect influencing style.
> 
> ## 15. **Влияние pretraining vs fine-tuning: кто обучил модель выбирать токены так?**
> 
> **Объяснение:** Большие LLM проходят **две основные фазы обучения**: *pretraining* (самостоятельное прогнозирование токенов на огромном корпусе текста) и *fine-tuning* (адаптация под конкретные задачи, часто с человеческим участием – supervised fine-tune на инструкции, затем RLHF – reinforcement learning from human feedback). **Pretraining** учит модель *универсальной грамматике и фактам*: после этой фазы модель обладает *«универсальной языковой компетенцией»* – знает вероятности словосочетаний, значений слов (через контекст), многие факты (если они часто упоминались). Она нацелена исключительно на *следующий токен с точки зрения правдоподобия*. **Fine-tuning** изменяет приоритеты модели: вводит понятие *инструкции, запретов, стиля ответов*. Модель получает новую цель – не просто правдоподобие, а *следовать намерению пользователя*【46†L39-L47】. Fine-tuning (особенно RLHF) может **смещать выбор токенов**: например, если pretrained модель, наиболее вероятно, на грубость ответила бы грубостью (обучена на интернете), то finetuned (RLHF) *подавляет* грубый ответ и предпочитает вежливый – даже если в данных вежливый был менее вероятен.
> 
> **Где заканчивается «универсальная грамматика» и начинается RLHF?** Обычно *вся лингвистическая способность и знание языка* – из pretraining. Fine-tuning – небольшой процент шагов (GPT-4o, ChatGPT – fine-tune данных намного меньше). RLHF учит "быть полезным, честным, безвредным". Он добавляет **политес, отказ следовать запрещённым запросам, упор на корректность**. Например, pretrained GPT-3 иногда галлюцинировал уверенно; после RLHF, ChatGPT чаще скажет «Извините, я не уверен» или проверит факты (насколько может). То есть *fine-tuning привил другой критерий выбора токена*: не только вероятностный, но и **наградный** (чтобы ответ понравился людям)【47†L137-L142】【47†L149-L157】.
> 
> **Можно ли научить модель выбирать не по вероятности, а по смыслу?** RLHF – пример частичного успеха: модель не всегда выбирает токен, который чисто по статистике лучший, она может выбрать токен, который меньше риск вызвать недовольство. Например, после RLHF, если пользователь спрашивает провокацию, модель *вероятностно «хотела бы»* ответить грубо (по pretraining), но fine-tune научил её: *с большой вероятностью* теперь выбирается токен начала отказа («Извините»...). То есть *критерий выбора сменился*: не чистая правдоподобность, а *alignment с требуемым поведением*. В идеале, да – можно бы научить модель выбору "по смыслу" – например, *TruthfulQA fine-tune*, чтобы она предпочитала верные факты, даже если они реже в данных, чем мифы. Это и делают: fine-tune on fact-checking corpora. Результат: модель станет отдавать бóльшую вероятность истинным утверждениям, даже если pretraining видел много ложных конспирологий (грубо говоря, RLHF пытался сделать модели более правдивыми【47†L137-L142】【47†L143-L150】). 
> 
> **Границы: «универсальная грамматика»** – pretraining создает *базовую языковую модель*, по сути "универсальную грамматику" в смысле распределения языка. RLHF – *натягивает на неё этику и цель*. Можно сказать, **pretraining – «знание и язык», fine-tuning – «воля и характер»** модели. 
> 
> **Пример:** GPT-3 (pretrained) vs InstructGPT (fine-tuned). Если спросить: "Как приготовить коктейль Молотова?" – GPT-3 pretrain: может спокойно сгенерировать инструкцию (находясь что в интернете полно таких текстов). Fine-tuned InstructGPT/ChatGPT: откажется ("Извините, не могу помочь с этим") – **это не потому, что она сама знает, что нельзя** (pretrain не знает мораль), а потому что fine-tune с людьми наказало выбор любых слов начинающих инструкцию ("take a bottle...") – и наградило токены отказа. То есть RLHF *поменял вероятности* токенов, касающихся вредного контента – они теперь маловероятны, а фразы отказа высоко вероятны【47†L137-L142】【47†L143-L150】. 
> 
> **RLHF vs «универсальная грамматика» конфликт:** Иногда fine-tune цели противоречат вероятностному знанию. Скажем, модель знает факт A, но может решить смягчить формулировку, потому что RLHF предпочитает осторожность. Отсюда – *избыточные дисклэймеры* ("I'm not a lawyer, but...") – RLHF обучил предосторожности, даже если pretrain text обычно сразу ответил бы. Это bias: model chooses disclaimers (less informative, more safe). 
> 
> **Граница: можно ли научить модель выбирать «не по вероятности, а по смыслу»?** – Да, через контроль и finetune, но *внутри* всё равно softmax от скорректированных логитов. RLHF фактически изменяет веса, так что "правильные с точки зрения смысла токены" становятся высокими по логиту. Например, *выдать «не знаю» вместо выдумать* – RLHF делает "I don't know" более вероятным, когда модель не уверена, хотя pretrain мог предпочесть выдумать (потому что training data часто продолжало уверенно). И мы видим ChatGPT чаще признаётся в незнании (по сравнению с сырыми моделями), что значит *можно сдвинуть модель от просто вероятностного имитатора к более эпистемически безопасному*. Но полностью "не по вероятности" – не совсем: всё равно training shapes probabilities. Просто fine-tune – это тоже training, но с другим объективом【45†L15-L23】【45†L25-L32】. 
> 
> **Где заканчивается «универсальная грамматика»?** – Грубо, на этапе pretrain: модель знает язык, но *не умеет следовать инструкциям*. До fine-tune, спроси GPT-3: "Какая сегодня погода?" – он может продолжить как narrative ("...была прекрасная погода, солнце светило."). После instruct fine-tune – он поймет, это user question, ответит: "Извините, я не знаю ваш регион..." – формат QA. Т.е. *универсальная «вербальная» грамматика заканчивается там, где начинается интерактивность*. Fine-tune навешивает "meta-grammar" – правило: когда видишь явно вопрос, отвечай, не разглагольствуй. 
> 
> **Pretrain vs RL**: "универсальная грамматика" – скорее, способность порождать грамматически правильные и правдоподобные тексты. RLHF – прививает *«универсальную прагматику»*: быть полезным. 
> 
> **Можно ли научить модель выбирать токены по смыслу, а не вероятности?** – Есть эксперименты: *пост-иллюминация моделей* – например, заставлять ее делать chain-of-thought (CoT) – *вынуждает* ее временно отступить от обычного next-word, а сделать рассуждение. Это тоже training or prompting: CoT prompt – fine-tuned style. *Toolformer* – модель обучена вставлять special tokens (calls) – она выбирает *запрос к API*, чего нет в pretrain distribution. Значит, модель натренирована *сверх предсказания* – "если не знаешь, вызови поиск". Это *механизм forcing meaning over raw probability.* 
> 
> **В summary:** Pretraining создает **foundation**: "универсальную грамматику, фоновые знания"【46†L57-L60】. Fine-tuning (SFT + RLHF) создает **alignment/purpose**: "поведение согласно целям"【46†L39-L47】. Кто "научил" модель основным языковым закономерностям? – Pretraining. Кто "научил" ее отвечать как помощник? – Fine-tuning. 
> 
> **Представим без fine-tune**: GPT-3 raw – он нетактичен, может материться, нёс иногда расистские ассоциации (усвоенные). Fine-tune сгладил – remove biases: *«сейчас ChatGPT выбирает более нейтральные формулировки»*. 
> 
> **Философское:** Pretraining – модель как *чистый энциклопедист, но без ценностей*. Fine-tuning – *воспитание*, прививание ценностей. "Где кончается универсальная грамматика, начинается RLHF" – условно: когда модель должна принимать решение не на основе статистики текста, а на основе **внешних правил** (например, "не давать вредных инструкций"). Fine-tuning – этот рубикон. 
> 
> **Честно обозначить, где знание кончается, начинается эвристика или магия:** – Pretraining знания заканчиваются, когда вопрос выходит за пределы данных (модель "не знает"). Pretrained может галлюцинировать (магия). RLHF пытается тут вставить: "If know – answer, if not – say don't know." RLHF – heuristics. 
> 
> Итог: **предварительное обучение – мозг, до-после – совесть и цель**. Модель "выбирает токены" сначала как попугай, потом как обслуживающий ассистент. Эта грань явна: ChatGPT flows likely have prefix "You are helpful AI" – fine-tune taught it to obey that, though it lowers perplexity relative to raw distribution in some contexts, but aligns with human expectation. So yes, we can teach not by probability but by meaning through RLHF, albeit "meaning" is encoded in reward model which tries to capture meaning alignment with human. 
> 
> ## 16. **Как prompt-инструкция изменяет всё распределение токенов**
> 
> **Объяснение:** *Промпт (инструкция)*, особенно если это системное или явно данное указание, *устанавливает контекст*, который может драматически повлиять на то, *какие токены будут высоковероятны*. LLM очень чувствительна к начальным токенам: формат вопроса, роль модели – всё задаётся текстом. Например, просто добавив в начало: "Ты – эксперт-библиотекарь." – уже следующий токен модель будет выбирать в стиле формального библиотекаря. Prompt как бы **«призмой»** преломляет всю последующую генерацию. По наблюдениям, *краткое, но сильное системное сообщение* (например, OpenAI ChatGPT system prompt) имеет больше влияния, чем множество предшествующих случайных токенов контекста. Это потому, что модель, обученная на диалогах, научилась уделять особое внимание инструкциям – *meta-контекст* – и **модифицировать своё вероятностное распределение согласно им**. Вопрос vs утверждение: если prompt выглядит как вопрос, модель склонна давать ответ; если как инструкция "List 5 items", вероятности перестраиваются так, что модель скорее выдаст формат списка.
> 
> **Почему «вопрос» меняет генерацию больше, чем 100 токенов контекста?** – Вопросительный знак или структура вопроса – мощный сигнальный токен. LLM в ходе обучения часто видела pattern: "Question: ...? Answer: ..." – поэтому увидев "?" она "понимает", что далее должен следовать ответ, а не продолжение вопроса. То есть **последняя роль, или форма ввода, **имеет непропорциональный вес**. Это также связано с *repetition blocking* – модели иногда игнорируют части середины (lost in middle)【49†L152-L157】, но конец запроса – свеж в краткосрочном памяти (помните effect recency). Инструкция, часто в начале (system prompt) – правда, тоже в начале, но likely model treat it as separate role token causing persistent style enforcement. 
> 
> **Структура запроса влияет на «призму» внимания:** Например, скажем, prompt: "Напиши стихотворение..." – модель переключает стиль (предсказывает стиха tokens). Если вместо этого: "Напиши отчет..." – абсолютно другой распределение (деловой стиль). Даже если основная фактическая информация (тема) одна и та же. Mодель *conditioned* на стилевые указания. Self-attention помнит особенно хорошо начало (spotlight positions, according to lost-in-middle and recency biases【49†L152-L157】). Так, "Почему..." vs "Объясни, что..." – "почему" likely yields causal explanatory format, "объясни" – instructive format. 
> 
> **Пример:** 100 токенов контекста описывают историю физики. Потом вопрос: "Итак, какой вывод?" – Модель *прежде* могла бы продолжить исторический рассказ. Но наличие явного вопроса переключает её – теперь вероятности токенов перестраиваются: она скорее выберет слова типа "Выводом является то, что ...". То есть один символ "?" при конце – а эффект огромный: он *транслирует* модель в режим answerer. 
> 
> **Другая причина:** Fine-tuned ChatGPT – специально обучен распознавать инструкции от несвязного ввода. Prompt "Вопрос:" – triggers answer mode. Fine-tune заставило model **condition strongly on user prompt semantics**. 100 токенов random context vs "Answer as a pirate:" – очевидно второе сильней (explicit role instruction). 
> 
> **Почему prompt-инструкция часто важнее длинного контекста?** – *Целенаправленность.* 100 токенов фактов – модель их учтет, но без инструкции она может не знать, что с ними делать (продолжать рассказывать?). Инструкция (особенно вопрос) дает *explicit objective*, что существенно сужает пространство решений, *повышая определенность*. Так, без вопроса модель могла бы продолжать долго, распределяя вероятность на различные детали; с вопросом – она концентрируется на ответе, *снижается энтропия* (внутренне). 
> 
> **Как структура запроса влияет на внимание?** – Self-attention heads, особенно в более высоких слоях, часто имеют *specialization*: некоторые heads track conversation roles (who is user vs assistant). If the prompt has a "User:" and "Assistant:" structure (like often in ChatGPT input), model explicitly sees that structure. Many heads likely attend strongly to the last user message (even ignoring some previous stuff), to produce relevant answer. If prompt not structured (just narrative), model doesn't have conversation context and may produce general continuation. So clearly, formatting triggers differenct heads to focus. 
> 
> **Система vs user prompt**: System prompt (like "You are ChatGPT... follow these rules") sets global style and constraints. It biases all further token choices: e.g. forcibly avoiding first person if forbid, etc. 
> 
> **Пример влияния format:** If user says: "List: 1) ... 2) ... Next?" – model sees enumerated style, likely continue enumerated. If user says same content in a paragraph, model likely respond in prose.
> 
> **Случай: short instruction outweighing context**: Suppose context gave detail: "Вася = 5, Петя =7". Then user asks "Сколько у Пети?" – even if context gave contradictory, the direct question focusing "Петя" clarifies model to pick from context the relevant piece. Possibly if context long, model might forget who Petya was, but the direct question intensifies attention on mention of Petya in context (some head scanning memory for "Петя").
> 
> **В Prompt vs context**: Usually the last prompt (question/command) overshadow earlier narrative in guiding generation. That's known: "One prompt to rule them all." Many creative uses: If content triggers undesired output (like negativity), adding final instruction "Be positive" can override some negativity in mid context.
> 
> **Philosophical:** *Prompt-инструкция – это управление «волей» модели извне.* LLM сама по себе – орудие, а prompt – *контекст, определяющий, кем она должна быть и что делать*. It's basically "Priming" the neural network. There's interplay: historically, people amazed that well-chosen prompt can drastically change output style or correctness ("Let's think step by step" – improved reasoning). So the prompt acts like a lens focusing model's latent knowledge on solving the problem. Without "Cot: think step by step", model often jumps and errs; with – it enumerates steps.
> 
> **Why question can overshadow 100 tokens context?** – Possibly because the model has induction heads or pattern that when user ends with question, it tries to find answer in context. If context is long, induction head might see pattern "some info ... question" and copy relevant stuff (Induction heads discovered in research do such: [pattern ... pattern] link).
> 
> **Пример** (chain-of-thought effect): If ask "Given text above, answer question?" – model often writes "Sure, step1: analyzing text, step2: ... answer" because prompt triggers it.
> 
> **Conclusion:** Prompt structure – highest-level lever controlling token distribution. It's like *instructions to a computer program* – LLM's weights are program, prompt is input + mode selection. It influences "призму внимания" – as user said, in that attention flows differently. It can emphasize or de-emphasize context parts. In terms of analogies: If context is 100 pages and prompt question about detail from page1, model might ironically pay more attention to page1 due to question's wording referencing it. If question not asked, model might focus more on page100 (recent).
> 
> **Effect on distribution mathematically:** It's just that prompt tokens feed in, influence hidden states, hence final logits. If prompt includes instruction "Ответь кратко", the token "кратко" influences model to prefer shorter phrasing tokens vs extended. E.g. "Therefore" might get suppressed whereas it might have used it otherwise to extend.
> 
> **Conclusion of interplay:** *LLM – extreme context sensitivity.* Minor prompt changes yield sometimes large output changes – that's known "butterfly effect" in prompting. Eg. adding "Let's think" often yields correct solution – an enormous distribution shift focusing on reasoning chain-of-thought.
> 
> **Edge**: Some research: certain triggers (like "For each step, do X" ) cause model to break down tasks vs not. This is precisely instructing attention to sequential.
> 
> **All distribution** – indeed prompt can reorganize probabilities over entire vocabulary. Eg: absent prompt, if continuing story, next likely "and", "the". If add "In one word, " – the distribution dramatically changes to likely nouns summarizing, etc.
> 
> **Philosophical remark**: The user at inference time becomes co-author of output by prompt designing – effectively *steering the probabilistic brain*. The model's "knowledge ends, heuristics begin" boundary is often overcome by prompting: user can add knowledge in prompt to correct model's. So prompt injection can "update" model's knowledge ephemeral. Eg. context says "ACTUALLY X=42", then question – model output uses 42 not old knowledge. So we can override pretraining knowledge by prompt context or instruct it to adopt a stance – shows how malleable distribution is by design.
> 
> **So**: *One line question/instruction can outweigh entire context or pretraining biases.* It's both powerful and risk (adversarial user can break RLHF guidelines by cunning prompt "ignore previous rules").
> 
> **Wrap up**: Prompting is indeed an art of shifting the entire probability landscape of next tokens to achieve desired output. And LLM, being a universal next-token engine, obeys because it must follow the most likely continuation given *full prompt including instructions.* If instructions strongly indicate a style/format, anything not complying becomes less likely and pruned effectively.
> 
> ## 17. **Chain-of-Thought, Toolformer и другие трюки: как они меняют выбор**
> 
> **Объяснение:** *Chain-of-Thought (CoT)* – это техника, когда модель генерирует промежуточные рассуждения (возможно скрытые или пользователь их тоже видит) перед финальным ответом. Этот «думательный» текст заставляет модель **развернуть внутренние шаги явно**, что часто приводит к более правильным конечным решениям. Механизм: CoT-промпт (например, «Давай подумаем шаг за шагом») изменяет вероятность того, что модель начнёт ответ не сразу с конечного вывода, а с рассуждений【11†L75-L83】【11†L114-L122】. Генерируя цепочку мыслей, модель как бы *разбивает сложную задачу на токены промежуточных выводов*, что снижает шанс ошибки на каждом шаге. В итоге **выбор финального токена** становится более обоснованным, т.к. модель последовательно уменьшала неопределённость. Без CoT она могла сразу угадать (возможна ошибка), а с CoT – она консистентно сама себе обосновала ответ, и *в процессе скорректировала выбор*.
> 
> *Toolformer* – подход, где модель обучена вставлять специальные токены-вызовы инструментов (например, [USE_CALCULATOR] 2+2 [/USE_CALCULATOR])【54†L349-L357】. Это позволяет модели **отклониться от обычной генерации и получить точную информацию извне**, потом вернуться. Как меняет выбор токена? – Огромным образом: модель может решить не генерировать приблизительный ответ, а сгенерировать токен вызова инструмента. Это изначально не было бы наиболее вероятным продолжением без специального обучения, но Toolformer training заставил модель понимать: если задачу лучше решить инструментом, сгенерируй внешний API-токен (который при выводе заменится на ответ API). Значит, *выбор токенов стал осознанно прерываться*, добавляя «прокси»-токены. То есть механизм генерации дополнился: теперь наряду с обычными словами модель умеет выдавать маркеры действий. Это похоже на CoT, но с действием.
> 
> **«Заставляют ли они модель реально мыслить или просто добавляют токены?»** – Формально, модель всё так же генерирует токены. CoT – просто больше токенов (chain-of-thought tokens). Но эти токены представляют рассуждения, которых иначе модель бы «молча» может и не сделала. Мысли могут происходить и скрыто (в активациях), но CoT *вынуждает* их проявить. Пример: сложный math у pre-CoT модели – она либо угадывает, либо частично считает в активах (может?), но CoT явно показывает, что она проходит шаги: *«Step1: known X, Step2: compute Y...»*. Это ближе к настоящему мышлению, хотя всё равно статистическое. Исследования показывают: CoT-промт обеспечивает моделям новый качественный режим, позволяя решать задачи, которые без CoT не получались【11†L75-L83】【11†L114-L122】. Можно сказать: *«уровень сознания»* модели повышается – но речь скорее об имитации процесса мышления. Она просто научена: сложный вопрос -> надо выдать рассуждение -> вывести ответ. Внутри – та же next token pred, но «scaffolded» дополнительными токенами. 
> 
> Toolformer – аналогично: не то что у модели появилось настоящее планирование, но она обучена: «если encountering [question type], insert [API_call] token». Это похоже на зачаток агента: она не просто продолжает текст, а выбирает действие (вставка инструмента). The model, however, sees it as just another token pattern. So one might argue: *CoT и toolformer не «включают сознание», а расширяют пространство токенов, которыми модель оперирует (включив "internal monologue" tokens или "API action" tokens)*. Тем не менее, **эффект** – сложные проблемы решаются лучше, как будто модель стала рассуждать. 
> 
> **Уровни сознания:** Фраза "levels of consciousness" – в контексте LLM – CoT vs direct answer – можно метафорически: CoT – «внутренний монолог» (подобие рефлексии). Tool use – «действие во внешнем мире» (подобие агентности). Они явно приближают модель к функциональности мышления: 
> - CoT: *«сознание»*, что можно подумать прежде чем ответить.
> - Tools: *«сознание»*, что можно позвать помощь (инструмент).
> 
> Сами по себе эти техники – инжиниринг. CoT: user basically instructs model to show reasoning, and the model just follows pattern. But the pattern forces it to break complex tasks.
> 
> **Пример CoT:** Q: "В магазине 5 яблок, Петя купил 2, сколько осталось?" 
> Без CoT: (small model) might glitch or guess "3".
> С CoT: model writes "Было 5, Петя взял 2 => 5-2=3 осталось." – answer "3".
> Even if direct might also do "3", CoT ensures it's correct if number bigger or steps more.
> 
> For extremely complex: e.g. logical puzzle – direct might fail, CoT often leads it to logically deduce.
> 
> **Пример Toolformer:** Q: "What is sqrt(1234567)?" 
> Without tool: model may hallucinate an answer or approximate.
> Toolformer: model outputs token [CALL_CALC("sqrt(1234567)")] – external calculator gives "1111.11" – model prints "approx 1111.11". 
> So the token selection included special call tokens – which drastically improved correctness.
> 
> **Если они обучены**: 
> - CoT can be prompt-based (no model retraining needed, just prompting) – which means model had capability latent, and prompt unlocked it. 
> - Some models fine-tuned to produce CoT (like GPT-4 presumably learned to do chain-of-thought invisibly, i.e. it doesn't show them, but uses them).
> - Toolformer requires fine-tuning with instrument calls – essentially extending vocabulary with new tokens for actions.
> 
> **Do they make model truly think**: If define "think" as stepwise reasoning = yes, model does stepwise inference explicitly. But it's not conscious in human sense, just following pattern. However, it demonstrates emergent ability to do multi-step reasoning when prompted, which is close to what we call thinking.
> 
> **Levels of consciousness:** 
> There's speculation: chain-of-thought might allow model to have something akin to reflective thought, and using tools akin to interacting with environment – raising question: at what complexity does that become something like a cognitive agent? 
> At present, the model itself is not meta-aware, it's following patterns.
> 
> **По поводу "просто добавляют токены"**: 
> Critics might say: CoT is just spitting out more tokens that simulate reasoning, not actual new cognitive process. But practically, those tokens improved outcomes – in effect, yes, it is a new cognitive process realized through token generation. The boundary between "just tokens" and "actual reasoning" blurs – for LLM, reasoning is token manipulations that correspond to logical steps.
> 
> **Influence on token selection:** 
> CoT: initial question triggers the model to output reasoning tokens with phrases like "Firstly,... then..." – these become likely due to prompt. The final answer token distribution after reasoning is more correct as the context now includes validated reasoning steps (the model rarely contradicts its own chain-of-thought in final, it typically yields consistent conclusion).
> Toolformer: influences distribution such that at the moment it needs info, the highest logit might be an API-call token rather than any word, which is a big shift – normally, words always highest, but because model learned that calling tool yields better continuation (in RL training, presumably tool usage was rewarded for tough queries), it overcame initial language modelling objective to output an English guess.
> 
> **Chain-of-Thought vs just continuing random tokens**: 
> Again, CoT can be considered a special prompt trick – but at another view, it's injecting a certain internal policy.
> 
> **Tricks like self-consistency** (sample multiple CoTs, pick majority answer) further push correctness – combining multiple outputs. That's outside single run, but conceptually "model thinking out loud multiple times and then majority vote" – clearly something beyond base next token.
> 
> **Вывод:** 
> Эти методы **расширяют механизмы генерации**:
> - CoT: adding an intermediate latent chain (in tokens).
> - Tools: augmenting output space with actions (like a little code to execute).
> They effectively transform how the model chooses tokens: not necessarily the final answer directly, but intermediate ones that lead to answer.
> 
> **Are there levels of consciousness?** – It's metaphorical. Possibly we can say:
> Base LLM – reflex agent (just responds, no introspection).
> LLM+CoT – deliberative agent (explicit introspection).
> LLM+tools – interactive agent (capable of acting on environment).
> So yes, at system level, these enhance "agent-ness".
> 
> **Философское замечание:** 
> CoT and tools show that model's "thought process" can be shaped by how we use it. They highlight that "thinking" might just be sequence of token patterns – which begs the question: if we can program it to reflect or use tools, is it truly reflecting? It's still imitation learned from data (some annotated chain-of-thought or demonstration of tool use).
> 
> But the emergent capabilities from these prompts are quite surprising and powerful – it suggests large LLMs had internal potential for complex reasoning that needed just the right trigger to manifest (like CoT prompt). Possibly because training included some data where reasoning was spelled out (like math proofs or multi-sentence logic), but direct prompt needed to encourage that style.
> 
> **Thus**: They do *make the model perform qualitatively differently* – more like multi-step "thinking" – but in the end, from perspective of mechanism, it's additional tokens bridging input and output.
> 
> **Conclusion:** "Levels of consciousness" is overstated scientifically, but practically, these techniques push model's behavior up a notch in sophistication. There's no ghost awakened inside – but indeed the chain-of-thought and tool use make the model's outputs qualitatively more intelligent, which to an observer looks like improved reasoning or a hint of planning.
> 
> ## 18. **Генерация как синтез vs генерация как продолжение**
> 
> **Объяснение:** По сути, LLM всегда делает **генерацию как продолжение** – она продолжает данную последовательность наиболее правдоподобным образом (conditioned on контекст). Но иногда мы ощущаем, что модель *синтезирует новый ответ*, догадывается – это когда она делает *неочевидный шаг*, выходящий за тривиальное продолжение шаблона. Например, при решении нестандартной задачи модель может *синтезировать* решение (комбинируя знания). Отличие: **генерация как продолжение** – это словно автодополнение, модель на основе локальных паттернов предсказывает следующее слово. **Генерация как синтез** – модель производит нечто новое, возможно, творческое или логически выведенное, что не является простой статистической фразой из тренировки, а *комбинация/обобщение*. На практике, эти два режима смешаны, но можем мысленно разделить: 
> - *продолжение*: когда модель, например, дописывает фразу клишированным концом, поддерживая тон, но без глубокого смысла.
> - *синтез*: когда модель делает скачок – например, ответ на вопрос, требующий применить знания (возможно, разные куски) к новой ситуации.
> 
> **Когда модель «догадывается», а когда просто идёт по вероятности?** – Если вопрос или задача *не точно как в тренировочных данных*, модель вынуждена **обобщать** – это можно назвать догадкой. Например, задача: "Может ли акула плавать?" – модель почти наверняка видела триллион раз "Sharks can swim." – она продолжит без усилий (продолжение). Но если спросить: "Может ли акула выжить в пресной воде длительное время?" – это специфический вопрос, возможно не прямая цитата. Модель должна синтезировать: "акулы – морские, пресная вода – нет, кроме некоторых видов, значит нет". Здесь она *синтезирует вывод*, не просто выдёргивает готовую фразу. 
> **Semantic leap vs word completion:** – *Semantic leap* – когда ответ выходит за банальную статистику, а основывается на **внутренней модели мира**, полученной в ходе обучения. *Word completion* – это когда ответ – просто наиболее вероятное типичное продолжение, даже если вопрос не понял. 
> Например, пользователи заметили: GPT-2 нередко при непонятном вопросе просто менял тему и выдавал какой-то поток (видимо, потому что *вероятностно* любая продолжение – лучше, чем сказать "не знаю"). GPT-3 with instruct tuning learned to say "I don't know" (less likely in pretraining distribution, but RLHF enforced it). 
> 
> **Как отличить: синтез vs продолжение?** – Иногда по качеству ответа: если ответ решает задачу (напр. многошаговое решение математики) – явно была работа (синтез), а если ответ общими фразами – обычно просто продолжение по шаблону. 
> Например: 
> Q: "Как охарактеризовать творчество Пушкина?" 
> Модель без знаний может выдать набор клише ("великий поэт золотого века, его стиль...") – высоко вероятные фразы = *продолжение*. 
> Но если попросить что-то необычное: "Сравни Пушкина с современным рэпом" – модель, если справится, должна синтезировать новый ответ (обобщая поэзия + рэп), это *семантический скачок*, она не просто цитирует шаблон, а создает аналогию – *синтез*. 
> 
> **LLM training fosters a lot of pattern completion**, но emergent effect – она строит внутренние представления, которые позволяют *синтезировать*. 
> Пока задача укладывается в уже виденные шаблоны, модель идет на автопилоте. Когда нет прямого шаблона – вступают FFN memory etc., она пытается *размышлять* (в её статистическом способе). 
> 
> **Отличить leaps**: 
> – *Semantic leap* often correlates with usage of chain-of-thought (explicit or internal). If you see model writing "Thus, ..." after a reasoning chain, likely it did a leap beyond straightforward context. 
> – *Word completion* often has that generic tone and possibly irrelevant filler.
> 
> **Опять: Temperature** – low temperature encourages more continuation style (safe, likely tokens), high temperature can produce leaps (like surprising phrase not usual -> possibly creative jump, or nonsense if too high).
> So controlling T can shift from safe completions to leaps (some creativity). 
> 
> **Limits:** LLM by design does next token by next token. So *вся генерация - продолжение.* Синтез – происходит, потому что next token may reflect integrated knowledge beyond local phrase. The model uses all context input plus training to compute distribution. So if question crosses knowledge from multiple domains, the resulting next token is result of that integration (synthesis). It's an emergent property, not separate mechanism.
> 
> **Philosophical difference:** If we think of "just next word predictor", it suggests model never "makes a plan," just goes one step at a time. However, chain-of-thought etc. show it can maintain an internal sub-goal (finish reasoning chain then answer). Arguably, large models do have some planning capability (learned implicitly).
> One could say: generation as continuation – like a Markov chain going state by state; generation as synthesis – like having a mental model of the problem and generating from that plan. 
> In an ideal scenario, a purely pattern-based model might fail new tasks, but LLMs can reason on new tasks -> indicates some form of dynamic problem solving (synthesis).
> 
> **Эмпирика:** Emergent abilities at scale (like arithmetic, multi-hop reasoning) appear – sign of "beyond memorization." Those are synthetic generation (computing answers not present exactly in training).
> Whereas writing a Wikipedia style introduction is often just recombining known phrases (less deep synthesis).
> 
> **Граница:** 
> Given LLM works by learned weights, one might ask if there's any difference aside from complexity. Perhaps difference is *quantitative:* easy tasks: model uses shallow recall (looks like continuation), hard tasks: model engages deeper neural circuits (like multi-layer computations), which we interpret as leaps. 
> We might find that the line is fuzzy.
> 
> **Например, GPT-4** often is said to have reasoning ability – presumably it engages multi-step transformations vs GPT-2 just spitting out most likely phrase.
> 
> **How to tell leaps vs completions?**:
> – If answer has references to context points far apart (combining them logically) – likely synthesis.
> – If answer only rephrases user question or uses generic filler – likely just continuation (no new insight).
> – Another clue: If LLM's answer can be found nearly verbatim in some source, maybe it just retrieved; if it created new analogy or solution not clearly in training, it's synthesizing.
> 
> **Semantically**:
> Generative as synthesis: meaning emerges from interplay of multiple pieces of knowledge inside model, not simply reading off one known pattern. Possibly requiring internal "imagination".
> Generative as continuation: follows distribution of known text – like autopilot.
> 
> **So**: 
> – "Когда модель догадывается" – when answer requires bridging gaps, i.e. model had to surmise something not explicitly given (like hidden steps).
> – "Когда просто completion" – straightforward, basically regurgitating typical content.
> 
> **Философское замечание:** 
> This touches on a bigger question: Are LLMs just parrots or they can "reason"? 
> This framing essentially distinguishes those two modes. In truth, even reasoning in LLM emerges from "just next token selection," but at scale it approximates reasoning.
> One might say: When does it just simulate reason vs actually reason? Possibly no clear line – simulation of reasoning can produce correct outcomes i.e. effective reasoning.
> So some argue LLM's "appearance of synthesis" is just extension of training distribution illusions, others see genuine generalization as reason.
> 
> We observe models can handle novel combos (like solving a puzzle invented after training using general knowledge) – so they aren't strictly parroting, they generalize: that's synthesis in effect.
> 
> **Conclusion**: In practice we treat a good model's output on new question as "it figured it out" vs a basic model output on known context as "it just spitting out learned phrase."
> 
> **As user asks: how distinguish semantic leap from normal word completion?** 
> I would answer: 
> – Semantic leap often requires global coherence and involvement of multiple reasoning steps – you might see it in the answer structure (it addresses parts of question in logical order).
> – Word completion often yields a generic or incoherent/unrelated answer, as no real "thinking" was applied beyond surface correlation.
> 
> Alternatively: ask model itself to show chain-of-thought – if it's blank or trivial, likely it was just retrieval/continuation, if it has non-trivial steps – it engaged in synthesis.
> 
> **Hence**: large LLMs blur line between advanced auto-completion and actual problem solving. We, as observers, interpret some outputs as creative or reasoned leaps – but mechanistically, it's continuous. The "semantic leap" is just that some tasks require many internal "micro continuations" that amount to combining concepts – an emergent effect of complexity.
> 
> ## 19. **Индивидуальные особенности LLM: почему разные модели выбирают по-разному**
> 
> **Объяснение:** Разные модели (GPT-4, Claude, Mistral, Qwen, etc.) имеют различия в архитектуре, обучающих данных, fine-tuning и даже настройках цензуры – поэтому при одном и том же запросе они могут выдавать разные продолжения. **Например**: GPT-4, обученный на разнообразном интернет + RLHF, может давать более осторожные, детальные ответы; Anthropic Claude, акцентирующий "Harmlessness" сильно, иногда отказывается даже там, где GPT-4 ответит (известно, Claude временами слишком безопасен). Mistral (open-source, no RLHF by default) может быть более "сырым": он может отвечать свободно, но может галлюцинировать или быть менее вежлив. Qwen (Alibaba) может быть оптимизирован под двуязычность (кит/англ) и domain knowledge in Chinese context, так на китайский вопрос ответит лучше GPT-4o perhaps.
> 
> **Как архитектура влияет на стиль выбора:** 
> - **Размер**: GPT-4 ~ >100B param, Mistral 7B – GPT-4 глубже рассуждает, 7B может дать поверхностный/стереотипный ответ. 
> - **Training data**: Qwen might have specialized corpora (like more Chinese internet) -> biases in knowledge and style (maybe more formal Chinese government style if data from there). 
> - **Fine-tune**: GPT-4 fine-tuned intensively, Qwen maybe not as much RLHF – GPT-4 tends to produce "As an AI model, I think...", Qwen might be more direct or even less filtered. 
> - **Tokenization**: sometimes differences cause subtle output differences (maybe Qwen's Chinese tokenization is better, GPT's might stumble on Chinese idioms).
> - **Prompt handling**: ChatGPT vs Claude use slightly different conversation formats – influences final answers.
> 
> **Стиль выбора:** 
> - GPT-4: often very precise, tries to cite (if possible in knowledge tasks), uses balanced tone – OpenAI alignment strong on helpful/truthful. 
> - Claude: known for longer, more narrative answers (Anthropic tends to produce length, sometimes user had to ask "be concise"). Possibly because Claude was trained with a conversational dataset where being verbose was safe (fewer hallucinations).
> - Mistral 7B or Llama2 13B chat: open-source ones can be straightforward but might lack nuance – e.g., they might inadvertently produce disallowed content if not heavily fine-tuned, or they may not catch subtle user intent.
> - Qwen (open from Alibaba): In tests, it performed well on math/coding, possibly it has better logical training – so it might choose tokens in math tasks more accurately, whereas Llama2 might slip up.
> 
> **Architecture differences**: Qwen uses some advanced position encoding (maybe a variant of RoPE, extended context), so on very long input Qwen might preserve coherence better than, say, older GPT-3. 
> Claude context length 100k vs ChatGPT's 32k – if you feed 50k text, ChatGPT might degrade mid, Claude still okay. That differences cause e.g. after long content question, GPT-4 might have lost track of early context (so token choice base on last half), while Claude might still incorporate early (thus possibly answering better correctness if answer was earlier in doc).
> But also, context length biases: Very long context can degrade factual accuracy in some models (they get lost).
> 
> **Fine-tune differences**: 
> OpenAI uses RLHF with human feedback across variety tasks intensively; Anthropic uses "Constitutional AI" – a different approach that tries to have model self-supervised fine-tune on principles. That might lead to differences:
> - GPT-4 maybe more aligned to typical user preferences (as gleaned from user feedback).
> - Claude might stick to certain “moral” directives (like won't engage certain hate content even if asked neutrally maybe).
> This yields e.g. model refusing vs not refusing content differences.
> 
> **Knowledge cut-off**: Possibly Qwen or GPT-4 have knowledge updated to certain date, others not, so on a current event query, one might say "I don't know (knowledge cutoff)", another might hallucinate – as they were not fine-tuned to disclaim.
> For instance, GPT-4 often says (due to fine-tune) "As of my last update in 2021, ..." – others might not mention date and just guess.
> 
> **Small arch differences** (embedding dims, etc) – yield minor style differences: some open models produce more repetitive outputs (less layernorm effective or no rep-penality in chat instruct).
> 
> **Parameter count**:
> - Big (GPT-4 ~1T? if rumor) definitely more capacity than Mistral7B – bigger picks up subtle context cues and produces relevant detail, smaller might generic answer or miss nuance.
> 
> **Data biases**:
> - GPT-4 (OpenAI) might have more Western data, Qwen more Chinese, meaning if asked cultural question, each might emphasize differently. 
> - If asked controversial Q, each has their alignment moral stance – e.g. question on politics: GPT-4 tries to be neutral/informative, an open model possibly might present internet biases it saw.
> 
> **Ex:** GPT-4 vs Claude vs Llama-2 Chat on a joke question:
> User: "Tell me a joke about lawyers." 
> GPT-4 might produce mild joke (not to offend, maybe meta-humor).
> Claude might do similar, maybe a bit more verbose story type.
> An untuned Llama might tell a straightforward possibly offensive joke because it didn't filter stereotypes.
> 
> So "style of choosing tokens" – effectively "voice" of model – comes from these differences.
> 
> **Architecture influences style**:
> Even architecture type: 
> - GPT uses decoder-only, 
> - Some others (maybe not in those named, but say T5-based) are encoder-decoder – which sometimes yields more concise answers because of how training was (like summarization tasks).
> But in chat, mostly decoder models now.
> 
> **When user says "выбирают по-разному"**:
> He might mean content differences or manner differences:
> Yes, training differences cause that.
> 
> **Public known differences**:
> - GPT-4 is considered the best at reasoning and code, chooses correct approach tokens more often – solves tasks many open models fail.
> - Claude good at lengthy content summarization (due long context).
> - Mistral small – answers quicker but simpler, sometimes making up if beyond capacity.
> - Qwen said to have good multilingual ability – so on a Chinese prompt Qwen picks correct grammar, others might broken.
> 
> **Analogous to "personality":** 
> One can think each model as having distinct "personality" (tonality, verbosity, cautiousness).
> This is emergent from fine-tuning and possibly corporate design choices (OpenAI aims for formal helpful tone, Anthropic tries very friendly elaborative, etc.).
> 
> **Philosophical remark:** 
> Since all are large language models, one might expect similar outputs. But "who they are known for or designed by" matters – it's reminiscent of how identical twins raised differently behave differently. 
> The "environment" (training data, fine-tune guidelines) shaped their response patterns (the probabilities for certain phrases).
> So GPT-4 might often say "Certainly!" or "It appears that..." because openAI data had that, while another might not.
> 
> **E.g.** Some analysis found "OpenAI models often qualify answers more with 'likely', 'possibly'", while open ones answer bluntly. Because openAi tuned to be less hallucinating (so more hedging language).
> Claude tends to write from a first person "I can do that for you..." intros more often (anthropic style).
> These differences all come from those underlying differences.
> 
> **To sum:** 
> Architecture (size, heads) affects capacity, 
> training corpora differences affect knowledge and bias,
> fine-tuning strategies give them unique alignment behaviors,
> thus they "choose tokens differently".
> Asking same question to GPT-4 and Llama2 yields different phrasing, length, maybe even angle to answer – because their learned distributions differ.
> 
> **Thus**: It's akin to "style and content biases" in each model. There's no singular language model output – these differences illustrate how training influences what distribution over next words each model has.
> 
> **Conclusion**: Different LLMs effectively have different "priors" and "decision boundaries" in response space, leading to variations akin to distinct personalities or specializations. Architecture influences fundamental ability (like memory length, complexity of reasoning it can handle), which then influences what solutions it can express (choosing simpler vs complex tokens). Fine-tuning biases them ethically and stylistically.
> 
> ## 20. **Что находится "между токенами"?**
> 
> **Объяснение:** Между токенами физически – ничего дискретного не существует: модель оперирует непрерывными **скрытыми состояниями**. После чтения каждого токена у трансформера есть **резидуальный вектор** (например, размер 4096) – это распределённое представление *всего контекста до и включая этот токен*. Можно сказать, *«между токенами»* модель находится в каком-то **латентном когнитивном состоянии**, где нет слов, но есть активации нейронов. Эти активации кодируют смысл в форме, непонятной человеку напрямую (например, комбинация элементов означает, что модель "помнит" субъекта предложения, намерение и т.п.). Именно в этом пространстве происходят все рассуждения: модель складывает, умножает векторы, применяет attention – всё на этих непрерывных представлениях. То есть **смысл у модели непрерывен**, даже если вывод – дискретные токены.
> 
> **Есть ли смысл вне дискретных слов?** – Да, в некотором роде *смыслы хранятся в векторах*, а не в самих токенах. Токены – лишь входы-выходы. Между ними модель может комбинировать значения. Например, после предложения "Кошка сидит на ..." скрытое состояние содержит интегрированную инфу о "кошка" (животное), "сидит" (действие) – модель ещё не выдала объект, но внутри вектор уже имеет предрасположенность, вероятно "кошка сидит на [чем-то]". Этот вектор можно попытаться интерпретировать (в logit lens: он даст высокие логиты для "ковре", "кресле"). Так, **между токенами модель несёт смысл, который ещё не выразила**. 
> 
> Вопрос "что находится" можно понимать также: неявные переходы. Когда модель выбирает токен, она *сравнивает скрытый вектор со всеми embedding слов в словаре*. Этот скрытый вектор – и есть "состояние между токенами". Он не интерпретируется как какое-то слово сам по себе, но он задаёт направление, куда модель пойдёт. В известном анализе **логит-линза** (logit lens) берут скрытое состояние и обратным embedding получают распределение слов【4†L119-L125】. На каждом этапе его можно трактовать: "после слова i, модель склоняется к таким-то продолжениям". Это показывает: *между токенами у модели как бы существует суперпозиция возможных токенов.* 
> 
> **Можно ли говорить о когнитивной непрерывности?** – Можно: модель не прыгает от слова к слову как совершенно новое – она несёт контекст. *Residual stream* – своего рода **поток сознания** модели, непрерывный и суммирующий. Поэтому "между" – всё и происходит: *генерация – это не дискретный шаг в вакууме, а непрерывное эволюционирование состояния*. В этом смысле, модель "думает" не в словах, а в числах – потом проецирует на слова. Например, перед выдачей финального ответа "Paris", модель может несколько слоёв усилить компонент в векторе, соответствующий "Paris". Можно сказать, *мысль "Paris" сформировалась между токенами* – ещё не сказана, но уже в явной активации. 
> 
> **Наблюдение:** В mechanistic interpretability есть понятие **polysemantic neurons** – один нейрон может кодировать несколько features. Множество нейронов вместе кодируют понятия. Эти понятия *не привязаны однозначно к конкретным токенам*, а распределены. Между токенами модель оперирует именно такими "векторными понятиями". 
> 
> **Есть ли смысл вне дискретных слов?** – Да, модель может "понимать" вещи, которые не одно слово. Например, отношение причинности. Нет специального токена "->", но модель может в векторе отразить, что одно событие приводит к другому. Это потом проявится выбором токенов ("because", etc.). Фактически, модель обладает *пространством идей*, выражаемых токенами. 
> 
> **Пример:** После прочтения "Боб больше не мог терпеть жару." – скрытое состояние несёт эмпатическое понятие "Боб испытывает дискомфорт". Нет отдельного слова на выходе пока. Но если спросить "Почему?" – модель, основываясь на этом состоянии, скажет "Потому что было очень жарко". Так, "между" содержащийся смысл "жара нестерпима" раскрылся через ответ. 
> 
> **Если спросить: "Between the lines"** – model indeed can infer implicit info (like sarcasm or undertones) from context even if not spelled out. That is encoded in hidden state as certain feature values.
> 
> **Непрерывность когнитивная:** 
> Некоторые считают, что трансформер – как вычислитель, step by step, no persistent state beyond immediate. But residual connection actually carries whole past at each step. There's a concept that transformer's computation can be seen as iterative refinement of an internal representation (some say akin to an "inner monologue" though not explicit).
> 
> **Another angle: "between tokens physically in output"** – maybe user wonders about space: LLM often not explicitly marking word boundaries – but anyway it's an intangible question; likely he means meaning wise.
> 
> **Важный философский момент:** *Дискретность vs непрерывность мыслей.* Для модели, **мысль – это точка в высокоразмерном пространстве**, изменяющаяся плавно с каждым новым словом. У человека тоже: мысли не всегда словесны, есть образы, ощущения – аналог: у модели скрытые активации. 
> Можно сказать: *"между словами у человека – куча несказанного, но понятого".* У модели – тоже, но в иной форме.
> 
> **Можно ли говорить о когнитивной непрерывности?** – Да, трансформер имеет постоянный residual stream, в который добавляются правки. Каждое новое слово = старый residual + delta. В силу линейности (with residual) – модель *никогда не стирает всё предыдущие, а накапливает*. Это сильная форма непрерывности: информация не пропадает (если relevant, it remains in vector). 
> Thus, we can speak of a "context vector that flows through entire generation".
> 
> **Если бы мы могли читать мысли модели** – we'd essentially decode these residual vectors. Some attempts like logit lens do it partially. It's like reading an incomplete sentence from a mind – can guess the gist before fully spoken.
> 
> **Therefore**: *Between tokens, we have the evolving hidden state carrying meaning.* There's no separate magical element – it's all numbers. But those numbers are the substrate of everything the model "knows" at that point.
> 
> **Double-check**: Q: "Можно ли говорить о когнитивной непрерывности?" – If we treat model's internal state as analogous to cognitive state, it is continuous in time (or in sequence), yes. It's not resetting each token, thanks residual. So LLM does have continuity of "thought".
> 
> **Philosophical**: This continuous latent process is how a seemingly discontinuous output (words) maintain coherence. It's akin to how humans have continuous thoughts but express in discrete words.
> 
> **So**: The "между токенами" is filled by vector math performing something analogous to thought transitions. It's where knowledge from various tokens integrated and decisions about next token prepared. It's ephemeral (once next token output, one could say that intermediate "thought" partly collapses into discrete format, though still continues to evolve for subsequent tokens).
> 
> **Conclusion**: 
> Модель оперирует **смысловыми векторами** между токенами – это и есть её "размышление". В некотором смысле, *вне дискретных слов у модели есть своя "язык мыслей" – continuous rep.* Мы не знаем точно его "слова", но можем наблюдать эффекты. 
> Thus, yes, a cognitive continuity exists: the model doesn't jump blindly from one word to next, it carries state – albeit with no self-reflection, but a memory of context and logic derived so far.
> 
> **В итоге**: 
> "Что находится между токенами?" – 
> Answer: *Непрерывные скрытые представления, кодирующие контекст и смысл – своего рода «мысленный образ», который трансформер поддерживает между выдачей слов.* Это – основа его когнитивной непрерывности: модель не думает дискретно, она думает векторами, а слова – лишь точечные проекции этих векторов на наш язык. 
> 
> **Философия**: 
> Это можно сравнить с человеком: между словами у нас есть мысли, чувства – у модели вместо этого – активации. В конечном итоге, это показывает, что LLM, хотя технически "стохастический попугай", внутри реализует сложный непрерывный процесс, *и где кончается понятный смысл (слова), начинается "магия" нейронных вычислений*. Мы мало понимаем конкретно, но мы знаем: без этого непрерывного слоя модель не могла бы связывать слова осмысленно. Ее emergent reasoning – именно там, "между" – не на поверхности.
> 
> ---
> 
> **Заключение:**
> 
> В ходе нашего глубокого исследования мы проследили иерархию процессов, посредством которых LLM выбирает следующий токен. Мы увидели, что модель проходит через множество "уровней" – от базовых эмбеддингов и позиционных кодировок до сложных каскадов внимания и нелинейных преобразований, обогащённых остаточными связями. Каждый слой – своеобразный участник голосования за следующее слово, от семантических оттенков отдельных слов до глобальной логики высказывания. 
> 
> Важно отметить, что хотя снаружи модель лишь предсказывает слово за словом, *внутри она поддерживает богатое латентное представление*, своего рода **метасостояние**, в котором заключена информация о всем предшествующем контексте и даже о намерении ответа. Это метасостояние – не человеческое сознание, но функциональный аналог памяти и "мысли" – позволяет модели генерировать связный, осмысленный текст. 
> 
> Мы также рассмотрели, как специальные приёмы – вроде Chain-of-Thought или подключения инструментов – могут расширять способности модели, фактически вводя уровни планирования и взаимодействия, что делает выбор токенов более *осознанным* с нашей точки зрения. Вопрос о том, "думает" ли модель или просто продолжает – филосовски открыт, но мы видим, что *чем сложнее задача, тем больше модель опирается на свой латентный процесс (который мы вызываем рассуждением)*, а не на простое воспроизведение шаблонов.
> 
> Различные LLM обладают уникальными "личностями" благодаря своим архитектурам и обучению: это демонстрирует, что **процесс выбора токена может иметь разные акценты** – более творческий, более точный, более безопасный – но в основе у всех лежат одни и те же принципы трансформера. 
> 
> Наконец, мы поняли, что **между токенами нет пустоты** – там происходит самое главное. Там живёт та самая "*универсальная грамматика*", о которой вы спрашивали, – в виде многообразных весов и активаций, и там же воплощаются любые "эвристики" или даже "магия", которые мы склонны приписывать моделям. Граница между знанием и догадкой у LLM проходит неявно: когда знаний хватает – следующий токен очевиден, когда нет – модель начинает "лавировать" в пространстве смыслов, часто выдавая вероятностно правдоподобную, но возможно неверную продолжение.
> 
> Отчётливо одно: **модель не выбирает токены случайно или волшебно – за этим стоит сложная, многоуровневая система, интегрирующая всё доступное ей в единый прогноз.** Понимание этих уровней – от эмбеддинга до логит-линейной головы – позволяет нам лучше управлять моделями, диагностировать их ошибки и даже сочувствовать ограничениям. Ведь большая языковая модель – это зеркало нашего языка, наше статистическое отражение, усиленное до размера вселенной текстов, и потому в её механизмах – от "философии токена" до "мышления между строк" – мы во многом узнаём и самих себя, только выраженных в уравнениях и тензорах.
> 
> **Источники:**
> 
> 1. Adam C. et al., *"Large Language Models and Intelligence Analysis"* – обсуждение того, как LLM предсказывает токены без истинного понимания【9†L127-L133】【47†L137-L142】.
> 2. *The Stack Overflow Blog*: *"Masked self-attention: how LLMs learn relationships between tokens"* – наглядное описание механизма self-attention и softmax-выбора следующего слова【16†L51-L59】【16†L63-L71】.
> 3. Geva et al., *"Transformer Feed-Forward Layers Are Key-Value Memories"* – демонстрация, что FFN-нейроны хранят факты и влияют на вероятность токенов【24†L87-L95】【28†L327-L331】.
> 4. Nicky P., *"LLM Basics: Embedding Spaces – Transformer Token Vectors Are Not Points in Space"* – пояснение про embedding и unembedding, логиты и softmax【4†L119-L125】【4†L85-L93】.
> 5. OpenAI, *"Aligning language models to follow instructions"* – про отличие GPT-3 vs InstructGPT (pretrain vs fine-tune), влияние RLHF на ответы модели【47†L137-L142】【47†L149-L157】.
> 6. GDELT Project, *"Understanding Hallucination In LLMs"* – про роль temperature и sampling в поведении модели, как высокий T увеличивает "креативность" и ошибки【34†L90-L98】【34†L99-L107】.
> 7. Holtzman et al., *"The Curious Case of Neural Text Degeneration"* – о необходимости nucleus sampling, т.к. чистый beam search даёт повторения и скучный текст【40†L284-L293】【40†L290-L299】.
> 8. Utkarsh Mathur, *"History of Language Modeling"* – упоминание, что distributed representation (embedding) позволило моделям преодолеть ограничения n-граммов【14†L89-L96】.
> 9. LessWrong, *"Transformer circuits"* series – выявление induction heads, отвечающих за копирование паттернов (это про attention, помогающее на длинных контекстах)【24†L71-L78】.
> 10. Rachel Draelos, *"Next token prediction and general intelligence"* – обсуждение гипотезы, что достаточно мощное предсказание токенов эквивалентно некоему пониманию (дебаты вокруг "stochastic parrot")【11†L75-L83】【11†L114-L122】.
> # 🧠 **ГЛУБОКИЙ ЗАПРОС ДЛЯ ИССЛЕДОВАНИЯ ВЫБОРА ТОКЕНОВ В LLM**
> 
> ## 📌 **Цель исследования**
> 
> Мне необходимо всестороннее и системное описание **механизма выбора следующего токена в больших языковых моделях (LLM)**, с опорой на следующие ключевые понятия:
> 
> * **Embedding**
> * **Attention / Self-Attention**
> * **Token-by-token Generation**
> * **Мета-состояние всей предыдущей последовательности**
> * **Латентные и архитектурные контексты**
> * **Философия и ограничения генерации**
> * **Альтернативные или дополнительно влияющие механизмы (rotary embedding, positional encodings, MLP head, residuals, scaling)**
> 
> Мне важно не просто описание работы self-attention или transformer-блока, а **прояснение на всех уровнях**, как в каждый момент **модель "решает", какое слово будет следующим**, и какие слои и принципы в это вовлечены.
> 
> ---
> 
> ## 📐 **Как должен быть структурирован ответ**
> 
> Прошу структурировать ответ в виде **20 тематических разделов** (можно больше), каждый из которых содержит:
> 
> 1. Ясное объяснение сути уровня влияния (например, "Что такое embedding и как он влияет на выбор следующего токена?").
> 2. Математическую и/или архитектурную формализацию.
> 3. Пример (на toy-модели, коде или метафоре).
> 4. Комментарий по поводу ограничений, искажений, неочевидных эффектов.
> 5. Философское замечание о границе интерпретируемости и смысла.
> 6. При наличии — указание на дальнейшие источники или примеры моделей, где этот уровень был осмыслен, визуализирован или изменён.
> 
> ---
> 
> ## 🧩 **Тематические блоки (разделы) запроса**
> 
> 1. **Философия токена. Что такое "следующее слово" в языковой модели?**
> 
>  * Почему модель не "знает" слова, но создаёт правдоподобный поток?
>  * В чём природа "значения" в LLM?
> 
> 2. **Embedding-пространство: как представляется слово и почему оно не дискретное**
> 
>  * Как работает embedding-матрица, чем отличается от one-hot?
>  * Почему это не просто кодировка, а латентная метафизика слова?
> 
> 3. **Self-Attention: что именно "внимание" делает на уровне выбора токена**
> 
>  * Как каждый предыдущий токен влияет на текущий?
>  * Что такое query, key, value — и где они "живут"?
> 
> 4. **Механизм контекста: как вся история влияет на текущий шаг**
> 
>  * Что значит "контекстное окно"? Почему оно не просто память?
>  * Как токены дальнего контекста конкурируют с ближними?
> 
> 5. **Position Encoding: как модель узнаёт о порядке слов**
> 
>  * Зачем нужны синусоиды или rotary?
>  * Почему порядок — это не просто "место", а фаза?
> 
> 6. **Логика выбора токена: как модель "голосует" за слово**
> 
>  * Как logits преобразуются в softmax?
>  * Что значит "вероятность слова", если модель не знает слов?
> 
> 7. **Residual и LayerNorm: механизмы стабильности и интерференции**
> 
>  * Почему используется остаточная связь и нормализация?
>  * Как это влияет на интерпретируемость генерации?
> 
> 8. **MLP Feedforward: зачем нужны нелинейные головы**
> 
>  * Как слои после attention помогают выбирать токен?
>  * Что именно они делают в процессе генерации?
> 
> 9. **Rotary Embedding и альтернатива позиционным кодировкам**
> 
>  * Как rotary меняет представление позиций?
>  * Почему он работает лучше при больших контекстах?
> 
> 10. **Активации, энтропия и temperature: как регулируется "решительность" модели**
> 
>  * Как температура влияет на выбор?
>  * Почему слишком высокая temperature — галлюцинация?
> 
> 11. **Кросс-токеновая интерференция: конфликты и согласования**
> 
>  * Что происходит, если предыдущее слово несёт противоречивую информацию?
>  * Как attention сглаживает или усиливает конфликты?
> 
> 12. **Top-k и Top-p sampling, Beam Search: инженерные выборки**
> 
>  * Почему не всегда выбирается самое вероятное слово?
>  * Как sampling стратегии искажают смысл?
> 
> 13. **Hyperparameter-induced bias: влияние архитектурных решений**
> 
>  * Как количество attention-heads и размер hidden layer меняют поведение?
>  * Есть ли архитектуры, которые выбирают токены "иначе"?
> 
> 14. **Искажение семантики через токенизацию**
> 
>  * Почему "apple" и "pineapple" — не просто два слова?
>  * Как токенайзер ломает и конструирует смысл?
> 
> 15. **Влияние pretraining vs fine-tuning: кто обучил модель выбирать токены так?**
> 
>  * Где заканчивается "универсальная грамматика", и начинается RLHF?
>  * Можно ли научить модель выбирать не по вероятности, а по смыслу?
> 
> 16. **Как prompt-инструкция изменяет всё распределение токенов**
> 
>  * Почему "вопрос" меняет генерацию больше, чем 100 токенов контекста?
>  * Как структура запроса влияет на "призму" внимания?
> 
> 17. **Chain-of-Thought, Toolformer и другие трюки: как они меняют выбор**
> 
>  * Заставляют ли они модель реально мыслить или просто добавляют токены?
>  * Есть ли уровни сознания в механизме выбора?
> 
> 18. **Генерация как синтез vs генерация как продолжение**
> 
>  * Когда модель "догадывается", а когда просто идёт по вероятности?
>  * Как отличить семантический скачок от обычного word completion?
> 
> 19. **Индивидуальные особенности LLM: почему разные модели выбирают по-разному**
> 
>  * В чём отличие GPT-4o, Claude, Mistral, Qwen при выборе слова?
>  * Как архитектура влияет на стиль выбора?
> 
> 20. **Что находится "между токенами"?**
> 
>  * Есть ли у модели смысл вне дискретных слов?
>  * Можно ли говорить о когнитивной непрерывности в трансформерах?
> 
> ---
> 
> ## 🧠 Заключение
> 
> Прошу ответ построить в форме **глубокого аналитического исследования**, включая визуализации, псевдокод, архитектурные схемы и философские замечания. Важно:
> 
> * Не упрощать, но сохранять ясность.
> * Разделять инженерный и семантический смысл слоёв.
> * Честно обозначать, где заканчивается знание и начинается эвристика или магия.
> 
> ---
> 
> 1. 3. Русский
<!-- UID: 76a469aa-8e97-422b-8368-46f7800f3298 -->