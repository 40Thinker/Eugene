>[!nexus_user] **User** - 25.07.2025 at 12:58
> –î–∞
<!-- UID: c245a7c2-e79f-4204-8ecb-5eb10dc85378 -->




>[!nexus_agent] **Assistant** - 25.07.2025 at 12:58
> –ü—Ä–∏–Ω—è—Ç–æ. –Ø –ø–æ–¥–≥–æ—Ç–æ–≤–ª—é –≥–ª—É–±–æ–∫–∏–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –ø–æ—Å–≤—è—â—ë–Ω–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ attention heads –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö.
> 
> –ë—É–¥–µ—Ç —Ä–∞—Å–∫—Ä—ã—Ç–æ 20 —Ä–∞–∑–¥–µ–ª–æ–≤ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏, —Ñ–æ—Ä–º—É–ª–∞–º–∏, –∫–æ–¥–æ–º, –ø—Ä–∏–º–µ—Ä–∞–º–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–º–∏ —Å—Ö–µ–º–∞–º–∏ –∏ —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏. –ö–∞–∫ —Ç–æ–ª—å–∫–æ –æ—Ç—á—ë—Ç –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤ ‚Äî —Å–æ–æ–±—â—É.
<!-- UID: 829b7925-0127-4733-a4bd-b516c9efe50c -->

---

>[!nexus_agent] **Assistant** - 25.07.2025 at 13:07
> # üß† **–ì–õ–£–ë–û–ö–ò–ô –ó–ê–ü–†–û–°: –°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ì–û–õ–û–í –í–ù–ò–ú–ê–ù–ò–Ø –ò –ú–ï–•–ê–ù–ò–ö–ê SELF-ATTENTION –í LLM**
> 
> ## 1. **–ß—Ç–æ —Ç–∞–∫–æ–µ ¬´–≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è¬ª: —Ñ–æ—Ä–º–∞–ª—å–Ω–æ, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ** 
> **–°—É—Ç—å:** *–ì–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è* ‚Äì —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ—è –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–º –∞—Å–ø–µ–∫—Ç–µ –≤—Ö–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –§–æ—Ä–º–∞–ª—å–Ω–æ –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä—ë—Ö –æ–±—É—á–∞–µ–º—ã—Ö –ø—Ä–æ–µ–∫—Ü–∏–π: –¥–ª—è *–∑–∞–ø—Ä–æ—Å–æ–≤* (query), *–∫–ª—é—á–µ–π* (key) –∏ *–∑–Ω–∞—á–µ–Ω–∏–π* (value). –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç —Å–≤–æ–∏ –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –∫–ª—é—á–∞–º–∏, –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —ç—Ç–∏ –≤–µ—Å–∞ –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∑–Ω–∞—á–µ–Ω–∏—è–º. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ** –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì —ç—Ç–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –±–ª–æ–∫, –ø–æ–ª—É—á–∞—é—â–∏–π –Ω–∞ –≤—Ö–æ–¥ —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –≤—ã—á–∏—Å–ª—è—é—â–∏–π –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É –∑–Ω–∞—á–µ–Ω–∏–π, –≥–¥–µ –≤–µ—Å ‚Äì —ç—Ç–æ *softmax* –æ—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∫–ª—é—á (—Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º)„Äê22‚Ä†L201-L210„Äë„Äê57‚Ä†L53-L62„Äë. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤—ã–¥–µ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ‚Äì –Ω–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–µ—Ç —É–¥–µ–ª—è—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–º—É —Å–ª–æ–≤—É, —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –∏–º—ë–Ω –∏–ª–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.
> 
> **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ (–ø—Ä–∏–º–µ—Ä):** 
> –í –ø—Å–µ–≤–¥–æ–∫–æ–¥–µ –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø–∏—Å–∞–Ω–∞ —Ç–∞–∫: 
> ```python
> # X ‚Äì –º–∞—Ç—Ä–∏—Ü–∞ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤, —Ä–∞–∑–º–µ—Ä [seq_len √ó d_model])
> Q = X @ W_Q # –∑–∞–ø—Ä–æ—Å—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d_k
> K = X @ W_K # –∫–ª—é—á–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d_k
> V = X @ W_V # –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d_v
> scores = softmax((Q @ K^T) / sqrt(d_k)) # –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
> head_output = scores @ V # –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π
> ``` 
> –ó–¥–µ—Å—å `W_Q, W_K, W_V` ‚Äì –æ–±—É—á–∞–µ–º—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–æ–µ–∫—Ü–∏–π –¥–ª—è –¥–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è. –ò—Ç–æ–≥ `head_output` ‚Äì —ç—Ç–æ –Ω–æ–≤–∞—è —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è, —Å–æ–±—Ä–∞–Ω–Ω–∞—è –∏–∑ Value —Å —É—á—ë—Ç–æ–º —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É Query –∏ Key.
> 
> **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ *scaled dot-product attention* –¥–ª—è –≥–æ–ª–æ–≤—ã: 
> 
> \[ \text{Attention}(Q, K, V) = \mathrm{softmax}\!\Big(\frac{Q K^T}{\sqrt{d_k}}\Big)\, V. \] 
> 
> –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —Ä–µ–∞–ª–∏–∑—É–µ—Ç –∏–º–µ–Ω–Ω–æ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é: Query \(Q_i = x W^q_i\), Key \(K_i = x W^k_i\), Value \(V_i = x W^v_i\) –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ \(x\), –ø–æ—Å–ª–µ —á–µ–≥–æ —Å—á–∏—Ç–∞–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π„Äê22‚Ä†L203-L212„Äë„Äê59‚Ä†L342-L350„Äë. *–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ* —ç—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –º–µ—Ö–∞–Ω–∏–∑–º—É –∞–¥—Ä–µ—Å–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: Query –æ–ø–∏—Å—ã–≤–∞–µ—Ç, —á—Ç–æ –∏—â–µ–º; Key ‚Äì —á—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç; Value ‚Äì –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —ç–ª–µ–º–µ–Ω—Ç–∞. –ì–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤—ã—á–∏—Å–ª—è–µ—Ç, –∫–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã –∑–∞–ø—Ä–æ—Å—É, –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏—Ö. –í –∏—Ç–æ–≥–µ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —É—á–∏—Ç—Å—è –≤—ã–ø–æ–ª–Ω—è—Ç—å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é ‚Äì –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏—Å–∫–∞—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∏–ª–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å.
> 
> **–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:** –ò–¥–µ—è –≤–Ω–∏–º–∞–Ω–∏—è –≤–æ–∑–Ω–∏–∫–ª–∞ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞—Ö –∏ –ø–∞–º—è—Ç—å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –Ω–æ *—Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ (self-attention)* –±—ã–ª–æ –ø–æ–ø—É–ª—è—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º Vaswani et al., 2017„Äê24‚Ä†L252-L259„Äë. –û–Ω–∏ –≤–≤–µ–ª–∏ –ø–æ–Ω—è—Ç–∏–µ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –≥–¥–µ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ ‚Äì –æ—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –≤—Ö–æ–¥–∞. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è —Ä–∞–∑–Ω—ã–º *–æ—Ç–Ω–æ—à–µ–Ω–∏—è–º* –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –î–æ –ø–æ—è–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –≤ –ø–∞—Ä–µ —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ (Bahdanau et al., 2015), –æ–¥–Ω–∞–∫–æ Vaswani et al. –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–º–µ–Ω–∏–ª–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ü–∏—é –≤–Ω–∏–º–∞–Ω–∏–µ–º. –° —Ç–µ—Ö –ø–æ—Ä –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è —Å—Ç–∞–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) ‚Äì –∏–º–µ–Ω–Ω–æ —á–µ—Ä–µ–∑ –Ω–∏—Ö –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∏ –ø–µ—Ä–µ–¥–∞—á–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏.
> 
> **–†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö:** –í BERT_base 12 —Å–ª–æ—ë–≤, –ø–æ 12 –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ; –≤ GPT-3 ‚Äì 96 —Å–ª–æ—ë–≤, –ø–æ 96 –≥–æ–ª–æ–≤; –≤ LLaMA-2 ‚Äì 32 –≥–æ–ª–æ–≤—ã –≤ —Å–ª–æ–µ –ø—Ä–∏ —Ä–∞–∑–º–µ—Ä–µ 13B. –í –∫–æ–¥–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ (PyTorch `nn.MultiheadAttention`) –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–º—É —Å—Ä–µ–∑—É –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–∞—Ä–∞–º–µ—Ç—Ä `num_heads` –∑–∞–¥–∞—ë—Ç —á–∏—Å–ª–æ –≥–æ–ª–æ–≤, –∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å–∞–º–∞ —Ä–∞–∑—Ä–µ–∂–∞–µ—Ç –ø—Ä–æ–µ–∫—Ü–∏–∏ –Ω–∞ –≥–æ–ª–æ–≤—ã. *–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ* –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –≤–æ–∑–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å: –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ—Ç–∫–ª—é—á–∞—é—Ç –∏–ª–∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –∏—Ö –≤–∫–ª–∞–¥„Äê20‚Ä†L130-L139„Äë„Äê20‚Ä†L158-L167„Äë.
> 
> **–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:** –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —à–∞–±–ª–æ–Ω–∞—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ GPT-2 –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≥–æ–ª–æ–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ–≥–¥–∞ —Å–æ–µ–¥–∏–Ω—è—é—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞: –≤ –æ–¥–Ω–æ–π –∏–∑ —Ä–∞–±–æ—Ç –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã —è–≤–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç —Å–ø–∏—Å–∫–∏ (–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã), –¥—Ä—É–≥–∏–µ ‚Äì –≥–ª–∞–≥–æ–ª—ã, —Ç—Ä–µ—Ç—å–∏ ‚Äì –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã„Äê44‚Ä†L15-L23„Äë„Äê44‚Ä†L55-L63„Äë. –¢–æ –µ—Å—Ç—å –¥–∞–∂–µ –±–µ–∑ —è–≤–Ω–æ–π –ø–æ–¥—Å–∫–∞–∑–∫–∏ –º–æ–¥–µ–ª—å —Å–∞–º–∞ *—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç* –≥–æ–ª–æ–≤—É –Ω–∞ –ø–æ–∏—Å–∫ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ü—Ä–∏ –∞–±–ª—è—Ü–∏–∏ (–æ—Ç–∫–ª—é—á–µ–Ω–∏–∏) –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –≥–æ–ª–æ–≤ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –ø–æ—á—Ç–∏ –Ω–µ –ø–∞–¥–∞–µ—Ç, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞–∂–Ω—ã—Ö (—Å–º. —Ä–∞–∑–¥–µ–ª 10): —ç—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ –º–Ω–æ–≥–∏–µ –≥–æ–ª–æ–≤—ã –∏–∑–±—ã—Ç–æ—á–Ω—ã, –∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ ‚Äì –∫—Ä–∏—Ç–∏—á–Ω—ã.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–æ-–∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:** –ì–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì —ç—Ç–æ –∫–∞–∫ ‚Äú–º–∏–Ω–∏-–º–æ–¥—É–ª—å –≤–Ω–∏–º–∞–Ω–∏—è‚Äù –≤–Ω—É—Ç—Ä–∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏ –º—ã—à–ª–µ–Ω–∏—è. –ú–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–æ–≥–∏—é —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ–º: —É –Ω–∞—Å —Ç–æ–∂–µ –µ—Å—Ç—å —Ä–∞–∑–Ω—ã–µ ‚Äú—Ñ–æ–∫—É—Å—ã‚Äù ‚Äì –∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π, —Å–ª—É—Ö–æ–≤–æ–π –∏ —Ç.–¥. –í —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ ‚Äì —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π *–∫–∞–Ω–∞–ª –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è*, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω. –ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ —ç—Ç–æ –±–ª–æ—á–Ω–∞—è –º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å: –ª–µ–≥–∫–æ —É–≤–µ–ª–∏—á–∏—Ç—å —á–∏—Å–ª–æ –≥–æ–ª–æ–≤, —Ä–∞—Å—à–∏—Ä—è—è –≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. –ù–æ –¥–æ –∫–∞–∫–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç —Å—á–∏—Ç–∞—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ–π ‚Äú–º—ã—Å–ª—å—é‚Äù? –í –∫–æ–Ω–µ—á–Ω–æ–º —Å—á—ë—Ç–µ, –≥–æ–ª–æ–≤—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ, –∏ –ø–æ–Ω—è—Ç–∏–µ ‚Äú–≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è‚Äù ‚Äì —ç—Ç–æ —É–¥–æ–±–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –ø—Ä–∏–±–ª–∏–∂–∞—é—â–µ–µ –Ω–∞—Å –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤–æ–∏ —Ä–µ—Å—É—Ä—Å—ã –≤–Ω–∏–º–∞–Ω–∏—è.
> 
> **–°—Å—ã–ª–∫–∏:** Vaswani et al., 2017„Äê24‚Ä†L252-L259„Äë; –±–ª–æ–≥ AllenNLP –æ ‚Äúattention is not explanation‚Äù„Äê57‚Ä†L53-L62„Äë; –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (Vig et al., 2019)„Äê44‚Ä†L15-L23„Äë.
> 
> ## 2. **–ó–∞—á–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤, –∞ –Ω–µ –¥–µ–ª–∞–µ—Ç—Å—è –æ–¥–Ω–∏–º –±–ª–æ–∫–æ–º** 
> **–°—É—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:** –ü–æ—á–µ–º—É –±—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω–æ ‚Äú–º–æ–Ω–æ–ª–∏—Ç–Ω–æ–µ‚Äù –≤–Ω–∏–º–∞–Ω–∏–µ —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º, –≤–º–µ—Å—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤? –û—Ç–≤–µ—Ç: *–º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ* (**multi-head attention**) –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ–¥ —Ä–∞–∑–Ω—ã–º–∏ —É–≥–ª–∞–º–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –ï—Å–ª–∏ –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ ‚Äì —ç—Ç–æ —Ñ–æ–∫—É—Å –Ω–∞ –æ–¥–Ω–æ–º –ø–∞—Ç—Ç–µ—Ä–Ω–µ, —Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –¥–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ—á—å —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –û–¥–Ω–æ –≤–Ω–∏–º–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—é –≤—Å–µ—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å—Ä–∞–∑—É, —á—Ç–æ —Ç–µ—Ä—è–µ—Ç –Ω—é–∞–Ω—Å—ã„Äê24‚Ä†L252-L259„Äë. –ù–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤, –Ω–∞–ø—Ä–æ—Ç–∏–≤, –º–æ–≥—É—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è: –æ–¥–Ω–∞ ‚Äì –Ω–∞ –±–ª–∏–∑–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö, –¥—Ä—É–≥–∞—è ‚Äì –Ω–∞ –¥–∞–ª—å–Ω–∏—Ö, —Ç—Ä–µ—Ç—å—è ‚Äì –Ω–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–µ –∏ —Ç.–¥. Vaswani et al. –ø—Ä—è–º–æ –æ—Ç–º–µ—á–∞–ª–∏: ‚ÄúMulti-head attention –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö *–ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π*‚Äù„Äê24‚Ä†L252-L259„Äë. –° –æ–¥–Ω–∏–º –≥–æ–ª–æ–≤–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (averaging) –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å ‚Äì –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ —Å–Ω–∏–º–∞–µ—Ç —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ.
> 
> **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ:** –í –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ –∫–∞–∂–¥—É—é –≥–æ–ª–æ–≤—É –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–ª—É—á–∞—é—â–∏–π —Å–≤–æ—é –ø—Ä–æ–µ–∫—Ü–∏—é –≤—Ö–æ–¥–∞. –ó–∞—Ç–µ–º –≤—ã—Ö–æ–¥—ã –≥–æ–ª–æ–≤ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –∏ —Å–º–µ—à–∏–≤–∞—é—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–º —Å–ª–æ–µ–º. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ü–∏–π, –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–µ–Ω—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (—á–∞—â–µ –≤—Å–µ–≥–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ \(d_{\text{model}}\) –¥–µ–ª–∏—Ç—Å—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –Ω–∞ —á–∏—Å–ª–æ –≥–æ–ª–æ–≤ \(h\)). –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ \(d_{\text{model}}=768\) –∏ \(h=12\), —Ç–æ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –æ–ø–µ—Ä–∏—Ä—É–µ—Ç –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64. –≠—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç ‚Äú–ø–æ–ª–µ –∑—Ä–µ–Ω–∏—è‚Äù –≥–æ–ª–æ–≤—ã, –∑–∞—Å—Ç–∞–≤–ª—è—è –µ—ë –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –∫–∞–∫–æ–º-—Ç–æ –æ–¥–Ω–æ–º –∞—Å–ø–µ–∫—Ç–µ. –í—Å–µ –≥–æ–ª–æ–≤—ã –≤–º–µ—Å—Ç–µ –ø–æ–∫—Ä—ã–≤–∞—é—Ç –ø–æ–ª–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–∞—Ç—Ä–∏—Ü –≤–µ—Å–æ–≤ \(W^q_i, W^k_i, W^v_i\) –∏ –∑–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤„Äê20‚Ä†L99-L108„Äë„Äê20‚Ä†L114-L122„Äë. –ù–∞ —Å—Ö–µ–º–µ (–Ω–∏–∂–µ) —Å–ø—Ä–∞–≤–∞ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–ª–æ–∫–æ–≤ Scaled Dot-Product Attention, –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –∞ —Å–ª–µ–≤–∞ ‚Äì —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–µ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –æ–¥–Ω–æ –≤–Ω–∏–º–∞–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –±—ã–ª–æ –±—ã –º–µ–Ω–µ–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ.
> 
> **–ü–æ—á–µ–º—É —Ç–∞–∫ –ª—É—á—à–µ:** *–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ*, –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ ‚Äì –∫–∞–∫ –æ–¥–∏–Ω –¥–µ—Ç–µ–∫—Ç–æ—Ä —à–∞–±–ª–æ–Ω–æ–≤. –ï—Å–ª–∏ –º—ã —Å–¥–µ–ª–∞–µ–º –µ—ë –æ—á–µ–Ω—å —à–∏—Ä–æ–∫–æ–π, –æ–Ω–∞ –º–æ–∂–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –≤—ã—É—á–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤, –Ω–æ –ø–æ—Å–∫–æ–ª—å–∫—É –≤—ã—Ö–æ–¥ —É—Å—Ä–µ–¥–Ω—è–µ—Ç—Å—è –≤ –æ–¥–Ω–∏ –∑–Ω–∞—á–µ–Ω–∏—è, —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å–º–µ—à–∏–≤–∞—é—Ç—Å—è. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ–≤—ã ‚Äì –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—É–±—Å–µ—Ç–µ–π, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —É—á–∏—Ç —Å–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫. –≠—Ç–æ –Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å: –Ω–µ—Å–∫–æ–ª—å–∫–æ ‚Äú–º–Ω–µ–Ω–∏–π‚Äù (–≥–æ–ª–æ–≤) –ª—É—á—à–µ, –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–∞–∂–¥—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è, –∞ –ø–æ—Ç–æ–º –∏—Ö —Å–æ–≤–æ–∫—É–ø–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –±–æ–ª–µ–µ –±–æ–≥–∞—Ç–æ. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –æ—Ç–º–µ—á–∞–ª–∞, —á—Ç–æ —Å –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤–æ–π –º–æ–¥–µ–ª—å —Ö—É–∂–µ —É–ª–∞–≤–ª–∏–≤–∞–ª–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ *—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ–¥–∞–≤–ª—è–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å*„Äê24‚Ä†L252-L259„Äë.
> 
> **–≠–≤–æ–ª—é—Ü–∏—è –ø–æ–¥—Ö–æ–¥–∞:** –ü–µ—Ä–≤—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (Transformer 2017) –≤—ã–±—Ä–∞–ª–∏ 8 –≥–æ–ª–æ–≤ –¥–ª—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 512 –∏ 16 –≥–æ–ª–æ–≤ –¥–ª—è 1024. –≠—Ç–æ –≤–æ –º–Ω–æ–≥–æ–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –º–Ω–æ–≥–æ –≥–æ–ª–æ–≤ –ø–æ–º–æ–≥–∞–ª–æ –≤ –∑–∞–¥–∞—á–∞—Ö –≤—Ä–æ–¥–µ –ø–µ—Ä–µ–≤–æ–¥–∞, –≥–¥–µ —Ä–∞–∑–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –ø–æ —Ä–æ–¥—É, —á–∏—Å–ª—É, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –ø–æ —Å–ª–æ–≤–∞–º) –≤–∞–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ä–∞–∑–¥–µ–ª—å–Ω–æ. –ü–æ–∑–¥–Ω–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, Voita et al., 2019; Michel et al., 2019) –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–ª–µ–∫–æ –Ω–µ –≤—Å–µ –≥–æ–ª–æ–≤—ã –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω—É–∂–Ω—ã ‚Äì –º–Ω–æ–≥–∏–µ –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞„Äê20‚Ä†L132-L140„Äë„Äê20‚Ä†L158-L167„Äë. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Ö –Ω–∞–ª–∏—á–∏—è –≤–∞–∂–Ω–∞: –º–æ–¥–µ–ª—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–∏–µ –≥–æ–ª–æ–≤—ã –±—É–¥—É—Ç ‚Äú—Ä–∞–±–æ—á–∏–º–∏‚Äù.
> 
> **–ü—Ä–∏–º–µ—Ä –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö:** –í BERT –∏–ª–∏ GPT —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≥–æ–ª–æ–≤—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–∏–µ (64 –∏–ª–∏ 128), –Ω–æ –∏—Ö –º–Ω–æ–≥–æ. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: –≤—Å–µ –≥–æ–ª–æ–≤—ã —Å—á–∏—Ç–∞—é—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞ GPU. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ —ç—Ç–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –æ–¥–Ω–æ–º—É –±–æ–ª—å—à–æ–º—É –≤–Ω–∏–º–∞–Ω–∏—é, –Ω–æ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏–µ –¥–∞—ë—Ç –≤—ã–∏–≥—Ä—ã—à –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –∑–∞ —Å—á—ë—Ç –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, PyTorch `MultiheadAttention` –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º –ø—Ä–æ—Å—Ç–æ –¥–µ–ª–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∞ `num_heads` —á–∞—Å—Ç–µ–π –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –±–∞—Ç—á–µ–≤–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ. 
> 
> **–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:** –í–∞—Å–≤–∞–Ω–∏ —Å —Å–æ–∞–≤—Ç. –∑–∞–º–µ—Ç–∏–ª–∏, —á—Ç–æ –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ multi-head (–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–π –ø–æ–ª–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏) –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Ö—É–∂–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö„Äê24‚Ä†L252-L259„Äë. –ë–æ–ª–µ–µ –ø–æ–∑–¥–Ω–∏–µ –æ–ø—ã—Ç—ã –ø–æ *–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏* –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –µ—Å–ª–∏ –≤–≤–µ—Å—Ç–∏ –Ω–µ–±–æ–ª—å—à–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–æ–ª–æ–≤ –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é, —Ç–æ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç —Å–≤–æ–π —à–∞–±–ª–æ–Ω (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è). –í –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã —è–≤–Ω–æ –±—Ä–∞–ª–∏ –Ω–∞ —Å–µ–±—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ (alignment) —Å—É–±—ä–µ–∫—Ç–æ–≤, –¥—Ä—É–≥–∏–µ ‚Äì –≥–ª–∞–≥–æ–ª–æ–≤. –ï—Å–ª–∏ –∂–µ –æ—Å—Ç–∞–≤–∏—Ç—å –æ–¥–Ω—É –≥–æ–ª–æ–≤—É, –µ–π –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è —É—Å—Ä–µ–¥–Ω—è—Ç—å –≤—Å—ë —ç—Ç–æ, –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–∞–¥–∞–µ—Ç. 
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:** –†–∞–∑–±–∏–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –≥–æ–ª–æ–≤—ã ‚Äì –ø—Ä–∏–º–µ—Ä **—Ä–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π** –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö. –≠—Ç–æ –∫–∞–∫ –µ—Å–ª–∏ –±—ã –Ω–∞—à –º–æ–∑–≥ –∏–º–µ–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ü–µ–Ω—Ç—Ä–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: –æ–¥–∏–Ω —Å–ª–µ–¥–∏—Ç –∑–∞ –¥–≤–∏–∂–µ–Ω–∏–µ–º, –¥—Ä—É–≥–æ–π ‚Äì –∑–∞ —Ü–≤–µ—Ç–æ–º, —Ç—Ä–µ—Ç–∏–π ‚Äì –∑–∞ –∑–≤—É–∫–æ–º, –∏ –≤—Å–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ. –ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ —ç—Ç–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–π –æ–≥—Ä–æ–º–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è –º—ã –∏–º–µ–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–º–µ–Ω—å—à–µ. –ú–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º *–º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º* ‚Äì –∫–æ–≥–¥–∞ –º—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Å–∏—Ç—É–∞—Ü–∏–∏. –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —ç—Ç–æ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∞. 
> 
> **–°—Å—ã–ª–∫–∏:** Vaswani et al. 2017 –ø—Ä–æ multi-head„Äê24‚Ä†L252-L259„Äë; –∞–Ω–∞–ª–∏–∑ Voita et al. 2019 (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vs –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –≥–æ–ª–æ–≤)„Äê20‚Ä†L134-L142„Äë„Äê20‚Ä†L158-L167„Äë.
> 
> ## 3. **–ö–∞–∫ –∑–∞–¥–∞—ë—Ç—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: —Ñ–æ—Ä–º—É–ª—ã –∏ –±–∞–ª–∞–Ω—Å** 
> **–§–æ—Ä–º—É–ª–∞ –∏ —Å–≤—è–∑—å —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é:** –ß–∏—Å–ª–æ –≥–æ–ª–æ–≤ \(h\) ‚Äì –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä, –æ–±—ã—á–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º—ã–π —Ç–∞–∫, —á—Ç–æ–±—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–µ–ª–∏–ª–∏—Å—å –Ω–∞—Ü–µ–ª–æ. –ï—Å–ª–∏ \(d_{\text{model}}\) ‚Äì –æ–±—â–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∞ \(d_h\) ‚Äì —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã, —Ç–æ \(d_h = d_{\text{model}} / h\) (–ø—Ä–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º —Ä–∞–∑–±–∏–µ–Ω–∏–∏)„Äê20‚Ä†L107-L115„Äë„Äê50‚Ä†L491-L500„Äë. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ Vaswani \(d_{\text{model}}=512\) –∏ \(h=8\), –ø–æ—ç—Ç–æ–º—É \(d_h = 64\). –¢–∞–∫–∞—è –ø—Ä–æ–ø–æ—Ä—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—â–µ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–º –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö \(h\): –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —á–∏—Å–ª–∞ –≥–æ–ª–æ–≤ \(h\) –º—ã —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã \(d_h\), —á—Ç–æ–±—ã –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ \(W^q, W^k, W^v\) (—Ä–∞–∑–º–µ—Ä \(d_{\text{model}}\times d_h\) –∫–∞–∂–¥–æ–µ) –æ—Å—Ç–∞–≤–∞–ª–æ—Å—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–º„Äê20‚Ä†L107-L115„Äë. **Trade-off (–∫–æ–º–ø—Ä–æ–º–∏—Å—Å):** —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –≥–æ–ª–æ–≤ ‚Äì –∏ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–µ–¥–æ–≤—ã—Ä–∞–∑–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏; —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äì –∏ –≥–æ–ª–æ–≤—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —É–∑–∫–∏–º–∏ –∏ –º–æ–≥—É—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å –¥—Ä—É–≥ –¥—Ä—É–≥–∞, –∞ —Ç–∞–∫–∂–µ —Å–ª–æ–∂–Ω–µ–µ –æ–±—É—á–∞—é—Ç—Å—è. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —á–∞—Å—Ç–æ –≤—ã–±–∏—Ä–∞—é—Ç \(h\) –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ \(\sqrt{d_{\text{model}}}\) –∏–ª–∏ —Ç–∞–∫, —á—Ç–æ–±—ã \(d_h = 64\) –∏–ª–∏ 128 ‚Äì —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç.
> 
> **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ:** –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –æ–±—ã—á–Ω–æ –ø–æ–¥–±–∏—Ä–∞—é—Ç \(h\) —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ GPT-2 –¥–ª—è –º–æ–¥–µ–ª–∏ 768-–¥–∏–º (117M) –≤–∑—è—Ç–æ 12 –≥–æ–ª–æ–≤ (–ø–æ 64), –∞ –¥–ª—è –º–æ–¥–µ–ª–∏ 1600-–¥–∏–º (1.5B) ‚Äì 25 –≥–æ–ª–æ–≤ –ø–æ 64 (1600 = 25*64). –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ –≥–æ–ª–æ–≤ –ø–æ–≤—ã—à–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º (–º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –º–∞—Ç—Ä–∏—á–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ), –Ω–æ —Ç–æ–∂–µ –¥–æ –ø–æ—Ä—ã: –µ—Å–ª–∏ –≥–æ–ª–æ–≤ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å/–∫–ª—é—á/–∑–Ω–∞—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö, –∏ –ø–æ–ª–µ–∑–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –º–∞–ª–æ. –°—É—â–µ—Å—Ç–≤—É—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ —è–∑—ã–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∑–∞–º–µ—Ç–∏–ª–∏, —á—Ç–æ \(d_h < 32\) –Ω–∞—á–∏–Ω–∞–µ—Ç —É—Ö—É–¥—à–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑-–∑–∞ —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–æ–π —Å—É–±–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –∞ \(d_h > 128\) ‚Äì —Ç–æ–∂–µ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –ª—É—á—à–µ –¥–æ–±–∞–≤–∏—Ç—å –µ—â—ë –æ–¥–Ω—É –≥–æ–ª–æ–≤—É –∏ —Å–Ω–∏–∑–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å.
> 
> **–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:** –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Michel et al. (2019) ¬´Are Sixteen Heads Really Better than One?¬ª –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –º–Ω–æ–≥–∏–µ –≥–æ–ª–æ–≤—ã –≤ BERT –º–æ–∂–Ω–æ –≤—ã–∫–∏–Ω—É—Ç—å (–ø—Ä–æ—Ä–µ–¥–∏—Ç—å), –ø–æ—á—Ç–∏ –Ω–µ –ø–æ—Ç–µ—Ä—è–≤ —Ç–æ—á–Ω–æ—Å—Ç–∏„Äê20‚Ä†L132-L140„Äë„Äê20‚Ä†L158-L167„Äë. –¢–æ –µ—Å—Ç—å –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ –ø–µ—Ä–µ–∏–∑–±—ã—Ç–æ—á–Ω–∞ –ø–æ –≥–æ–ª–æ–≤–∞–º. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è *–≤–∞–∂–Ω–æ—Å—Ç–∏ –≥–æ–ª–æ–≤—ã* –ø–æ –∏–∑–º–µ–Ω–µ–Ω–∏—é –ª–æ—Å—Å–∞ –ø—Ä–∏ –µ—ë –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏„Äê20‚Ä†L129-L137„Äë„Äê20‚Ä†L139-L147„Äë. –û–∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ –¥–æ ~80% –≥–æ–ª–æ–≤ –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Å–Ω–∏–∂–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞, –∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –≥–æ–ª–æ–≤—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –ª–∏—à—å –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ—è—Ö. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ \(h\) –¥–∞—ë—Ç –≤–æ–∑–≤—Ä–∞—â–∞—é—â—É—é—Å—è —É–±—ã–ª—å ‚Äì –Ω–æ–≤—ã–µ –≥–æ–ª–æ–≤—ã –ª–∏–±–æ –¥—É–±–ª–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ, –ª–∏–±–æ ‚Äú—Å–ø—è—Ç‚Äù. –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –¥—Ä—É–≥–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ (Wang et al., 2020) –æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –≤ –∑–∞–¥–∞—á–∞—Ö —Å—á—ë—Ç–∞ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –±–æ–ª—å—à–µ –≥–æ–ª–æ–≤ –º–æ–≥—É—Ç –∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –≥–ª—É–±–∏–Ω—É –º–æ–¥–µ–ª–∏ (–∫–æ–Ω—Ü–µ–ø—Ü–∏—è ¬´Leaner Transformers: More Heads, Less Depth¬ª) ‚Äì –≤–ø–ª–æ—Ç—å –¥–æ —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ \(h\) –º–æ–∂–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —á–∏—Å–ª–æ —Å–ª–æ—ë–≤„Äê19‚Ä†L5-L13„Äë„Äê19‚Ä†L29-L34„Äë. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –≥–æ–ª–æ–≤—ã –º–æ–≥—É—Ç —á–∞—Å—Ç–∏—á–Ω–æ –±—Ä–∞—Ç—å –Ω–∞ —Å–µ–±—è —Ä–∞–±–æ—Ç—É –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ—ë–≤, –µ—Å–ª–∏ –∏—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–Ω–æ–≥–æ.
> 
> **–§–æ—Ä–º—É–ª—ã –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ü–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì \(O(n^2 \cdot d_{\text{model}})\) –æ–ø–µ—Ä–∞—Ü–∏–π (–¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã \(n\)). –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ \(h\) –≥–æ–ª–æ–≤ –Ω–µ –º–µ–Ω—è–µ—Ç –∞—Å–∏–º–ø—Ç–æ—Ç–∏–∫—É, –Ω–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ª—É—á—à–µ –∏–∑-–∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è SIMD: –º–∞—Ç—Ä–∏—Ü—ã –º–µ–Ω—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ª–µ–≥—á–µ –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–∏. –ü—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º \(d_{\text{model}}\) —É–≤–µ–ª–∏—á–µ–Ω–∏–µ \(h\) —Å–ª–µ–≥–∫–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç overhead –Ω–∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é –∏ –ø—Ä–æ–µ–∫—Ü–∏—é –æ–±—Ä–∞—Ç–Ω–æ (–º–∞—Ç—Ä–∏—Ü–∞ \(W^O\) —Ä–∞–∑–º–µ—Ä–æ–º \(d_{\text{model}} \times d_{\text{model}}\) –≤—Å—ë —Ä–∞–≤–Ω–æ –µ—Å—Ç—å, –Ω–µ –∑–∞–≤–∏—Å—è—â–∞—è –æ—Ç \(h\)), –Ω–æ —ç—Ç–æ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ. –ü–æ—ç—Ç–æ–º—É —á–∞—Å—Ç–æ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –¥–µ—Ä–∂–∞—Ç—å \(d_{\text{model}}\) –∏ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ –∏–≥—Ä–∞—Ç—å \(h\) –¥–ª—è –±–∞–ª–∞–Ω—Å–∞. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Äì –æ–±—ã—á–Ω–æ —Ä–∞—Å—Ç—è—Ç \(d_{\text{model}}\) –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Å–æ—Ö—Ä–∞–Ω—è—è \(d_h \approx 64\) –∏–ª–∏ 128.
> 
> **–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π –±–∞–ª–∞–Ω—Å:** *Trade-off:* –º–∞–ª–æ–µ \(h\) ‚Äì –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —à–∏—Ä–µ, –Ω–æ –∏—Ö –º–∞–ª–æ (–º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —É–ø—É—Å—Ç–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–≤—è–∑–∏); –±–æ–ª—å—à–æ–µ \(h\) ‚Äì –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —É–∑–∫–∞—è, –Ω–æ –∏—Ö –º–Ω–æ–≥–æ (—Ä–∏—Å–∫ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è). –í –ø—Ä–∞–∫—Ç–∏–∫–µ DL –≤—Å–µ–≥–¥–∞ –∏—â—É—Ç ‚Äú–∑–æ–ª–æ—Ç—É—é —Å–µ—Ä–µ–¥–∏–Ω—É‚Äù. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM (GPT-3, LLaMA) –æ–±—ã—á–Ω–æ \(d_h = 128\) –∏ \(h = d_{\text{model}}/128\). –≠—Ç–æ —Å—Ç–∞–ª–æ –≤—Ä–æ–¥–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞, –∏—Å—Ö–æ–¥—è –∏–∑ –æ–ø—ã—Ç–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: \(h\) –∫—Ä–∞—Ç–Ω–æ —á–∏—Å–ª—É GPU-—Ç–µ–Ω–∑–æ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤, –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π –∞—Å–ø–µ–∫—Ç:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ ‚Äì —ç—Ç–æ –∫–∞–∫ **–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö ‚Äú–≤–∑–≥–ª—è–¥–æ–≤‚Äù –º–æ–¥–µ–ª–∏**. –ï—Å–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –º–æ–∑–≥–∞, –º–æ–∂–Ω–æ —Å–ø—Ä–æ—Å–∏—Ç—å: —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –≤–µ—â–µ–π –º—ã –º–æ–∂–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ? –ù–∞–≤–µ—Ä–Ω–æ–µ, –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äì —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–æ–≤–æ–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ (–∫–∞–∂—É—Ç, 7¬±2 –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏). –£ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —ç—Ç–æ —á–∏—Å–ª–æ –≤—ã—à–µ, –Ω–æ —Ç–æ–∂–µ –Ω–µ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äú–≤–∑–≥–ª—è–¥–æ–≤‚Äù –∏ –º–æ–¥–µ–ª—å –ø—É—Ç–∞–µ—Ç—Å—è –∏–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö. –í –∫–∞–∫–æ–º-—Ç–æ —Å–º—ã—Å–ª–µ, –≤—ã–±–æ—Ä \(h\) ‚Äì —ç—Ç–æ –≤—ã–±–æ—Ä —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏. –ò–Ω–∂–µ–Ω–µ—Ä—ã —Ç—É—Ç –≤—ã—Å—Ç—É–ø–∞—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞–º–∏ —Ä–∞–∑—É–º–∞: —Ä–µ—à–∞—é—Ç, –Ω–∞ —Å–∫–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ. –ò –ø–æ—Ö–æ–∂–µ, —á—Ç–æ –¥–∞–∂–µ –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –æ—Å—Ç–∞—ë—Ç—Å—è —É–º–µ—Ä–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, GPT-4 –≤–µ—Ä–æ—è—Ç–Ω–æ –Ω–µ –∏–º–µ–µ—Ç —Ç—ã—Å—è—á –≥–æ–ª–æ–≤, –∞ –ø–æ—Ä—è–¥–∫–∞ —Å–æ—Ç–µ–Ω). –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è **–Ω—É–∂–Ω–∞ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è**, –∏ —Å–ª–∏—à–∫–æ–º —Å–≤–æ–±–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –¥–∞—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª—å–∑—ã.
> 
> **–°—Å—ã–ª–∫–∏:** –ë–ª–æ–≥ CMU about Heads (Michel et al. 2020)„Äê20‚Ä†L134-L142„Äë„Äê20‚Ä†L205-L212„Äë; Vaswani 2017 (–ø–æ–¥—Ä–∞–∑–¥–µ–ª 3.2)„Äê24‚Ä†L244-L252„Äë„Äê24‚Ä†L252-L259„Äë.
> 
> ## 4. **–ß—Ç–æ –¥–µ–ª–∞—é—Ç Query, Key –∏ Value ‚Äì –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ–ª—å** 
> **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ–ª—å:** *Query* (–∑–∞–ø—Ä–æ—Å), *Key* (–∫–ª—é—á) –∏ *Value* (–∑–Ω–∞—á–µ–Ω–∏–µ) ‚Äì —Ç—Ä–∏ –ø—Ä–æ–µ–∫—Ç–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è. –ï—Å–ª–∏ –≤—Ö–æ–¥ ‚Äì —ç—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ \(X\) (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞), —Ç–æ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è: \(Q = X W^Q\), \(K = X W^K\), \(V = X W^V\), –≥–¥–µ \(W^Q, W^K, W^V\) ‚Äì –æ–±—É—á–∞–µ–º—ã–µ –º–∞—Ç—Ä–∏—Ü—ã. **Query** ‚Äì –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ \(d_k\), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∏–ª–∏ ‚Äú–ø–æ–∏—Å–∫–æ–≤—ã–π –æ–±—Ä–∞–∑‚Äù –¥–∞–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞. **Key** ‚Äì –≤–µ–∫—Ç–æ—Ä \(d_k\), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –µ–≥–æ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏. **Value** ‚Äì –≤–µ–∫—Ç–æ—Ä \(d_v\), –Ω–µ—Å—É—â–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∞, –µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç –≤—ã–±—Ä–∞–Ω. –ú–µ—Ö–∞–Ω–∏–∑–º –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–æ–º —Å Query \(q_i\) –∏ –∫–∞–∂–¥—ã–º –¥—Ä—É–≥–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º —Å Key \(k_j\) —á–µ—Ä–µ–∑ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ \(q_i \cdot k_j\) (—Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º softmax). –ó–∞—Ç–µ–º –≤—ã—Ö–æ–¥–æ–º —è–≤–ª—è–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ Value-–≤–µ–∫—Ç–æ—Ä–æ–≤: \(y_i = \sum_j \alpha_{ij} v_j\)„Äê22‚Ä†L203-L212„Äë. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, *Query‚ÄìKey* —Å–ª—É–∂–∞—Ç –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è, –∞ *Value* ‚Äì –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏–π.
> 
> **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ–ª—å (–∞–Ω–∞–ª–æ–≥–∏—è):** –ü—Ä–µ–¥—Å—Ç–∞–≤–∏–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: *Key* ‚Äì —ç—Ç–æ –∏–Ω–¥–µ–∫—Å –∏–ª–∏ –ø–æ–ª–µ –≤ –±–∞–∑–µ, *Value* ‚Äì —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏, *Query* ‚Äì –∑–∞–ø—Ä–æ—Å –∫ –±–∞–∑–µ. –¢–æ–≥–¥–∞ –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì —ç—Ç–æ –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–∞–º: –º—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º Query —Å –∫–∞–∂–¥—ã–º Key (–Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω–∏ ‚Äú–ø–æ—Ö–æ–∂–∏‚Äù), –∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º Value. –í NLP –∞–Ω–∞–ª–æ–≥–∏—è: Query ‚Äì –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Äú—á—Ç–æ –º–Ω–µ –Ω—É–∂–Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞?‚Äù, Key ‚Äì ‚Äú–∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ–¥–µ—Ä–∂–∏—Ç —ç—Ç–æ —Å–ª–æ–≤–æ?‚Äù, Value ‚Äì ‚Äú–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞‚Äù. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ ‚Äú–∫–æ—à–∫–∞ –µ—Å—Ç —Ä—ã–±—É‚Äù, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–æ ‚Äú–µ—Å—Ç‚Äù (Query), –æ–Ω–∞ –º–æ–∂–µ—Ç –∏—Å–∫–∞—Ç—å —Å—É–±—ä–µ–∫—Ç ‚Äì —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å Query("–µ—Å—Ç") —Å–æ –≤—Å–µ–º–∏ Key, –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—è, —á—Ç–æ Key("–∫–æ—à–∫–∞") –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω, –∏ –±—Ä–∞—Ç—å Value("–∫–æ—à–∫–∞") –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≥–ª–∞–≥–æ–ª–∞. –¢–∞–∫ –≤—ã—è–≤–ª—è–µ—Ç—Å—è —Å–≤—è–∑—å —Å—É–±—ä–µ–∫—Ç-–≥–ª–∞–≥–æ–ª. –¢.–µ. Query –∫–æ–¥–∏—Ä—É–µ—Ç *–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—É—é –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å*, Key ‚Äì *–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ*, Value ‚Äì *—Å–∞–º–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ*. 
> 
> **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏:** Query –∏ Key –≤–≤–æ–¥—è—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (similarity). –ß–∞—â–µ –≤—Å–µ–≥–æ —ç—Ç–æ \(\text{score}(q, k) = q \cdot k\) (—Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ). –ú–æ–∂–Ω–æ –¥—É–º–∞—Ç—å: Query –∏ Key ‚Äì –≤–µ–∫—Ç–æ—Ä—ã –≤ –æ–±—â–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, —á–µ–º –±–ª–∏–∂–µ –æ–Ω–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã, —Ç–µ–º –≤—ã—à–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ. Softmax –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —ç—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±–µ—Ä—É—Ç—Å—è Value. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ \(1/\sqrt{d_k}\) –≤ —Ñ–æ—Ä–º—É–ª–µ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è dot-product –ø—Ä–∏ –±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏„Äê22‚Ä†L203-L212„Äë. 
> 
> **–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:** –í –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ 2017 Query, Key, Value ‚Äì –æ–±—â–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –ø–æ–∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç (–Ω–∞–ø—Ä. ‚ÄúMemory networks‚Äù, Bahdanau attention). –ò—Ö –≤–≤—ë–ª –µ—â–µ Daniluk et al. (2017) –∏ –¥—Ä. Vaswani et al. –ø–æ–ø—É–ª—è—Ä–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —ç—Ç–∏ —Ç–µ—Ä–º–∏–Ω—ã, —á—ë—Ç–∫–æ –æ–ø–∏—Å–∞–≤: ‚ÄúAttention function –º–æ–∂–Ω–æ –æ–ø–∏—Å–∞—Ç—å –∫–∞–∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ Query –∏ –Ω–∞–±–æ—Ä–∞ –ø–∞—Ä Key-Value –≤ –≤—ã—Ö–æ–¥‚Ä¶ –≤—ã—Ö–æ–¥ ‚Äì –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ Values, –≥–¥–µ –≤–µ—Å–∞ ‚Äì —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ Query —Å Key‚Äù„Äê22‚Ä†L195-L202„Äë.
> 
> **–ü—Ä–∏–º–µ—Ä—ã –≤ –∫–æ–¥–µ:** –í PyTorch `nn.MultiheadAttention` –≤—ã –ø–µ—Ä–µ–¥–∞—ë—Ç–µ —Ç—Ä–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞: query, key, value. –î–ª—è self-attention —á–∞—Å—Ç–æ –∑–∞–¥–∞—é—Ç –∏—Ö –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ (query=key=value=–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è), –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –≤–Ω—É—Ç—Ä–∏ —Å–∞–º–∞ —Å–¥–µ–ª–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏. –≠—Ç–æ —É–¥–æ–±—Å—Ç–≤–æ API: –º–æ–∂–Ω–æ –∏ —Ä–∞–∑–¥–µ–ª—å–Ω–æ –ø–æ–¥–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è *–∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è* Query ‚Äì —Å –¥–µ–∫–æ–¥–µ—Ä–∞, Key, Value ‚Äì —Å —ç–Ω–∫–æ–¥–µ—Ä–∞). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, Query/Key/Value ‚Äì —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–Ω–∏–º–∞–Ω–∏—è.
> 
> **–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã:** –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å, –∫–∞–∫–∏–µ Query —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –∫–∞–∫–∏–º–∏ Key. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ *Duplicate Token Heads* –∏—â—É—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è: Query, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —Ç–µ–∫—É—â–∏–º —Ç–æ–∫–µ–Ω–æ–º, —Ö–æ—Ä–æ—à–æ —Å–æ–≤–ø–∞–¥—ë—Ç —Å Key –∏–¥–µ–Ω—Ç–∏—á–Ω–æ–≥–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞„Äê37‚Ä†L347-L356„Äë„Äê37‚Ä†L357-L365„Äë. –≠—Ç–æ –∏ –µ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤. –í coreference-–≥–æ–ª–æ–≤–∞—Ö Query –æ—Ç –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –±—É–¥–µ—Ç –±–ª–∏–∑–æ–∫ –∫ Key –≤–æ–∑–º–æ–∂–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–º—ë–Ω ‚Äì —Ç–∞–∫ –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç. Semantically, Query –ª–æ–≤–∏—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—É—é ‚Äú–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å‚Äù (–Ω–∞–ø—Ä., ‚Äú–∫–æ–≥–æ –∑–∞–º–µ–Ω—è–µ—Ç —ç—Ç–æ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ?‚Äù), Key ‚Äú–ø–æ–¥—Å–≤–µ—á–∏–≤–∞–µ—Ç‚Äù —ç–ª–µ–º–µ–Ω—Ç—ã, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏–µ –µ—ë (–∏–º—è —Å —Ç–∞–∫–∏–º –∂–µ —Ä–æ–¥–æ–º/—á–∏—Å–ª–æ–º), –∞ Value –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∏–º—è). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Ç—Ä–æ–π–∫–∞ Q-K-V ‚Äì —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π: *–Ω–∞–π—Ç–∏, –∫—Ç–æ –Ω—É–∂–µ–Ω*, *–∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å, –∫—Ç–æ –µ—Å—Ç—å –∫—Ç–æ*, *–∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ*.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** Query, Key, Value ‚Äì —ç—Ç–æ –ø—Ä–æ–æ–±—Ä–∞–∑ –∞–¥—Ä–µ—Å–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –∏ –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –í –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–º —Å–º—ã—Å–ª–µ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–æ–≥–∏—é —Å **–≤–Ω–∏–º–∞–Ω–∏–µ–º –≤ –º–æ–∑–≥–µ**: –∫–æ–≥–¥–∞ –º—ã —á–∏—Ç–∞–µ–º, —É –Ω–∞—Å –µ—Å—Ç—å *–≤–æ–ø—Ä–æ—Å –≤ –≥–æ–ª–æ–≤–µ* (Query: –Ω–∞ —á—Ç–æ –º–Ω–µ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ?), –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏–º–µ–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ (Key: –Ω–∞–ø—Ä–∏–º–µ—Ä, —è—Ä–∫–æ—Å—Ç—å, —Ñ–æ—Ä–º–∞), –∏ –µ—Å—Ç—å —Å–∞–º–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ/—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (Value). –ú–æ–∑–≥ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞—à —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (Query) —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤ (Key) –≤–æ–∫—Ä—É–≥ –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –Ω—É–∂–Ω–æ–º (–±–µ—Ä—ë—Ç Value). –í —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä–æ–π. –≠—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –ø—Ä–∏—ë–º ‚Äì –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å —Ö–∞–æ—Ç–∏—á–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –≤–Ω–∏–º–∞–Ω–∏—è –≤ —á—ë—Ç–∫—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á—É. –û–Ω –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∫ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–º—É –∞–Ω–∞–ª–æ–≥—É—É –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ *–≤—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è* –∏–ª–∏ *–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è*.
> 
> **–°—Å—ã–ª–∫–∏:** Vaswani et al. 2017, —Å–µ–∫—Ü–∏—è 3.2.1 (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Query-Key-Value)„Äê22‚Ä†L195-L204„Äë; —É—á–µ–±–Ω–∏–∫ D2L (–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ QKV)„Äê59‚Ä†L294-L303„Äë„Äê59‚Ä†L315-L323„Äë.
> 
> ## 5. **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç self-attention –Ω–∞ —Ç–æ–∫–µ–Ω–∞—Ö: —à–∞–≥ –∑–∞ —à–∞–≥–æ–º** 
> **–®–∞–≥ 1: –í—Ö–æ–¥–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.** –ò–º–µ–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Å–ª–æ–≤) –¥–ª–∏–Ω—ã \(n\). –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤–µ–∫—Ç–æ—Ä–æ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ \(d_{\text{model}}\) ‚Äì –æ–±—ã—á–Ω–æ —ç—Ç–æ —Å—É–º–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–ª–æ–≤–∞ –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –û–±–æ–∑–Ω–∞—á–∏–º —ç—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è \(\{x_1, x_2, \ldots, x_n\}\). –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤–æ–∑—å–º—ë–º –æ–¥–∏–Ω —Å–ª–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. 
> 
> **–®–∞–≥ 2: –ü—Ä–æ–µ–∫—Ü–∏—è Q, K, V.** –î–ª—è self-attention (—Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è) –º—ã –±–µ—Ä—ë–º –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –Ω–∞–±–æ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –≤—ã—á–∏—Å–ª—è–µ–º –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ *Query*, *Key* –∏ *Value*. –¢–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ \(i\): 
> \[ q_i = x_i W^Q, \quad k_i = x_i W^K, \quad v_i = x_i W^V, \] 
> –≥–¥–µ \(W^Q, W^K, W^V\) ‚Äì –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–æ–µ–∫—Ü–∏–∏ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π \(d_{\text{model}}\times d_k\), \(d_{\text{model}}\times d_k\), \(d_{\text{model}}\times d_v\) —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ). –ï—Å–ª–∏ –≥–æ–ª–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ, —Ç–æ —É –∫–∞–∂–¥–æ–π —Å–≤–æ–∏ \(W\), –Ω–æ –ø–æ–∫–∞ –æ–ø–∏—à–µ–º –æ–¥–Ω—É –≥–æ–ª–æ–≤—É.
> 
> **–®–∞–≥ 3: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è (attention scores).** –î–ª—è –∫–∞–∂–¥–æ–≥–æ *–ø–∞—Ä—ã* —Ç–æ–∫–µ–Ω–æ–≤ \((i, j)\) –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å–∫–∞–ª—è—Ä: 
> \[ \text{score}_{ij} = q_i \cdot k_j = \sum_{l=1}^{d_k} q_i^{(l)} k_j^{(l)}. \] 
> –≠—Ç–æ –º–µ—Ä–∞ ‚Äú—Å—Ö–æ–¥—Å—Ç–≤–∞‚Äù —Ç–æ–∫–µ–Ω–∞ \(j\) —Å —Ç–æ–∫–µ–Ω–æ–º \(i\) –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∫–ª—é—á–µ–π- –∑–∞–ø—Ä–æ—Å–æ–≤. –í—Å–µ —Ç–∞–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –º–∞—Ç—Ä–∏—Ü—É \(S\) —Ä–∞–∑–º–µ—Ä–∞ \(n \times n\). –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º *–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ* \(1/\sqrt{d_k}\) –∫ –º–∞—Ç—Ä–∏—Ü–µ \(S\) (—ç—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤ softmax –ø—Ä–∏ –±–æ–ª—å—à–æ–º \(d_k\)„Äê22‚Ä†L203-L212„Äë).
> 
> **–®–∞–≥ 4: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å–∫–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ).** –í —Å–ª—É—á–∞–µ –∞–≤—Ç–æ–ø–æ—Ä–æ–∂–¥–µ–Ω–∏—è (–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –∫–∞–∫ –≤ GPT) –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –º–∞—Å–∫–∞ –Ω–∞ –±—É–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã: –¥–ª—è \(j > i\) –≤ –º–∞—Ç—Ä–∏—Ü–µ –æ—Ü–µ–Ω–æ–∫ —Å—Ç–∞–≤–∏–º \(-\infty\), —á—Ç–æ–±—ã —Ç–æ–∫–µ–Ω \(i\) *–Ω–µ –≤–∏–¥–µ–ª* —Ç–æ–∫–µ–Ω—ã —Å–ø—Ä–∞–≤–∞ (–±—É–¥—É—â–∏–µ)„Äê24‚Ä†L281-L288„Äë. –î–ª—è *bidirectional* (–Ω–∞–ø—Ä–∏–º–µ—Ä, BERT) –º–∞—Å–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞ ‚Äì –≤—Å–µ –≤–∏–¥—è—Ç –≤—Å–µ—Ö. –ú–∞—Å–∫–∞ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ (–Ω–µ —É—á–∏—Ç—ã–≤–∞—Ç—å <PAD> —Ç–æ–∫–µ–Ω—ã).
> 
> **–®–∞–≥ 5: Softmax ‚Äì –ø–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è.** –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ \(i\) –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è softmax –ø–æ –≤—Å–µ–º \(j\): 
> \[ \alpha_{ij} = \frac{\exp(\text{score}_{ij}/\sqrt{d_k})}{\sum_{m=1}^{n} \exp(\text{score}_{im}/\sqrt{d_k})}. \] 
> –í–µ–∫—Ç–æ—Ä \(\{\alpha_{i1}, \ldots, \alpha_{in}\}\) ‚Äì —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ \(i\) –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º (–Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω ‚Äú–≤–Ω–∏–º–∞—Ç–µ–ª–µ–Ω‚Äù –∫ –∫–∞–∂–¥–æ–º—É –¥—Ä—É–≥–æ–º—É –∏ —Å–µ–±–µ —Ç–æ–∂–µ). –≠—Ç–∏ \(\alpha\) —Ç–∞–∫–∂–µ –Ω–∞–∑—ã–≤–∞—é—Ç *–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è* –∏–ª–∏ *attention weights*. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —ç—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ \(n \times n\) (–¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã —Å–≤–æ—è).
> 
> **–®–∞–≥ 6: –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –∏ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ.** –¢–µ–ø–µ—Ä—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ \(i\) –≤—ã—á–∏—Å–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä –∫–∞–∫ —Å—É–º–º—É Value –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤, –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã: 
> \[ y_i = \sum_{j=1}^{n} \alpha_{ij} \, v_j. \] 
> –í –º–∞—Ç—Ä–∏—á–Ω–æ–π —Ñ–æ—Ä–º–µ: –µ—Å–ª–∏ \(A\) ‚Äì –º–∞—Ç—Ä–∏—Ü–∞ \(\alpha_{ij}\) (—Ä–∞–∑–º–µ—Ä \(n \times n\)), –∞ \(V\) ‚Äì –º–∞—Ç—Ä–∏—Ü–∞ –∑–Ω–∞—á–µ–Ω–∏–π (—Ä–∞–∑–º–µ—Ä \(n \times d_v\)), —Ç–æ –≤—ã—Ö–æ–¥ \(Y = A V\) (—Ä–∞–∑–º–µ—Ä \(n \times d_v\)). –≠—Ç–æ –∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç self-attention ‚Äì –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è \(y_i\) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –µ–º—É.
> 
> **–®–∞–≥ 7: (–ï—Å–ª–∏ multi-head) –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≥–æ–ª–æ–≤.** –ï—Å–ª–∏ –≥–æ–ª–æ–≤ \(h>1\), —Ç–æ –∫–∞–∂–¥—ã–π \(y_i\) –≤—ã—à–µ ‚Äì —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥–æ–ª–æ–≤—ã (—Ä–∞–∑–º–µ—Ä \(d_v\) –æ–±—ã—á–Ω–æ = \(d_{\text{model}}/h\)). –ú—ã –ø–æ–ª—É—á–∞–µ–º \(h\) –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞: \(y_i^{(1)}, \ldots, y_i^{(h)}\). –≠—Ç–∏ –≤–µ–∫—Ç–æ—Ä—ã –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –≤ –æ–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π \([y_i^{(1)} \| \cdots \| y_i^{(h)}]\) —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ \(d_{\text{model}}\). –ó–∞—Ç–µ–º –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –æ–±—â—É—é –º–∞—Ç—Ä–∏—Ü—É \(\ W^O\) (—Ä–∞–∑–º–µ—Ä \(d_{\text{model}}\times d_{\text{model}})\), —á—Ç–æ–±—ã —Å–º–µ—à–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≥–æ–ª–æ–≤ –∏ –ø–æ–ª—É—á–∏—Ç—å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ —Å–ª–æ—è –≤–Ω–∏–º–∞–Ω–∏—è: \(z_i = [y_i^{(1)} \| \cdots \| y_i^{(h)}] W^O\)„Äê24‚Ä†L244-L252„Äë„Äê50‚Ä†L493-L502„Äë.
> 
> **–®–∞–≥ 8: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ residual –∏ LayerNorm.** –í—ã—Ö–æ–¥ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ \(z_i\) —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º \(x_i\) (residual connection), –∏ –ø–æ—Ç–æ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è Layer Normalization. –û–±—ã—á–Ω–æ –¥–∞–ª—å—à–µ –∏–¥—ë—Ç FFN —Å–ª–æ–π, –Ω–æ —ç—Ç–æ —É–∂–µ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ self-attention –º–µ—Ö–∞–Ω–∏–∑–º–∞.
> 
> **–ü—Ä–∏–º–µ—Ä –ø–æ—à–∞–≥–æ–≤–æ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ):** –ü—É—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ‚ÄúThe cat sat‚Äù. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∞ ‚Äúsat‚Äù (–ø–æ–∑–∏—Ü–∏—è 3). 
> - Query(‚Äúsat‚Äù) —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å Key(‚ÄúThe‚Äù), Key(‚Äúcat‚Äù), Key(‚Äúsat‚Äù). –î–æ–ø—É—Å—Ç–∏–º, ‚Äúsat‚Äù –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –ø–æ—Ö–æ–∂ –ø–æ –∫–ª—é—á—É –Ω–∞ ‚Äúcat‚Äù (—Å—É–±—ä–µ–∫—Ç –≥–ª–∞–≥–æ–ª–∞). 
> - Softmax –¥–∞—Å—Ç –Ω–∞ ‚Äúcat‚Äù –≤—ã—Å–æ–∫–∏–π –≤–µ—Å, –Ω–∞ ‚ÄúThe‚Äù –∏ ‚Äúsat‚Äù ‚Äì –Ω–∏–∂–µ. 
> - –í—ã—Ö–æ–¥ –¥–ª—è ‚Äúsat‚Äù: \(y_{\text{sat}} = \alpha_{\text{sat,The}} V_{\text{The}} + \alpha_{\text{sat,cat}} V_{\text{cat}} + \alpha_{\text{sat,sat}} V_{\text{sat}}\). –ï—Å–ª–∏ \(\alpha_{\text{sat,cat}}\) –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç, —Ç–æ –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä ‚Äúsat‚Äù —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ ‚Äúcat‚Äù (–∫—Ç–æ —Å–∏–¥–µ–ª). 
> 
> **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è (PyTorch):** –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ. –ù–∞–ø—Ä–∏–º–µ—Ä: 
> ```python
> attn_output, attn_weights = multihead_attn(query=X, key=X, value=X)
> ``` 
> –≤—ã–ø–æ–ª–Ω–∏—Ç —à–∞–≥–∏ 2‚Äì7 —Å—Ä–∞–∑—É –¥–ª—è –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ —Å–ª—É—á–∞—è, –≤–µ—Ä–Ω—É–≤ –∏ –≤—ã—Ö–æ–¥ \(Y\), –∏ –≤–µ—Å–∞ \(\alpha\) (attn_weights). –ú–æ–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ `attn_weights` ‚Äì —Ç–µ–Ω–∑–æ—Ä [batch, num_heads, n, n].
> 
> **–ò—Å—Ç–æ—Ä–∏—è –∏ —ç–≤–æ–ª—é—Ü–∏—è:** Self-attention –≤–ø–µ—Ä–≤—ã–µ —è–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—ë–Ω –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö (2017). –†–∞–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –±—ã–ª–æ –≤ seq2seq, –Ω–æ —Ç–∞–º Query ‚Äì —Å –¥–µ–∫–æ–¥–µ—Ä–∞, Key/Value ‚Äì —Å —ç–Ω–∫–æ–¥–µ—Ä–∞. *Self*-attention –∑–Ω–∞—á–∏—Ç Query=Key=Value=—Ç–æ—Ç –∂–µ –≤—Ö–æ–¥. –≠—Ç–∞ –∏–¥–µ—è –ø–æ–∑–≤–æ–ª–∏–ª–∞ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ü–∏–∏ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –®–∞–≥–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ—Ç—Ç–æ–≥–¥–∞ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ. –í–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏ –±—ã–ª–∏ —É–ª—É—á—à–µ–Ω–∏—è (FlashAttention –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —à–∞–≥–∏ 3‚Äì6, –≤—ã—á–∏—Å–ª—è—è –≤—Å—ë –±–µ–∑ —è–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–≥—Ä–æ–º–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã \(n \times n\)). –ù–æ –ª–æ–≥–∏—á–µ—Å–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–µ–º –∂–µ.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** –°–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì —ç—Ç–æ –∫–∞–∫ –≥—Ä—É–ø–ø–æ–≤–æ–π –º–æ–∑–≥–æ–≤–æ–π —à—Ç—É—Ä–º —Å–ª–æ–≤: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω ‚Äú—Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç‚Äù —É –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ —É –Ω–∏—Ö –≤–∞–∂–Ω–æ–≥–æ. –®–∞–≥ –∑–∞ —à–∞–≥–æ–º, –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –∑–∞—Ç–µ–º —Å–æ–±–∏—Ä–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã. –ó–∞–º–µ—Ç–∏–º, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å *—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π* (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è, –Ω–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞) ‚Äì —ç—Ç–æ –≤ –∫–æ—Ä–Ω–µ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç RNN, –≥–¥–µ —à–∞–≥–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã. –í –Ω–µ–∫–æ–º —Å–º—ã—Å–ª–µ, self-attention ‚Äì —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ–±–º–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–µ —Å–ª–æ–≤. –≠—Ç–æ –ø–æ–¥–≤–æ–¥–∏—Ç –∫ —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–æ–π –∏–¥–µ–µ *–≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞* (Global Workspace Theory): –≤—Å–µ —á–∞—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –¥–µ–ª—è—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ. –®–∞–≥–∏ 1‚Äì8 ‚Äì —ç—Ç–æ –æ–¥–∏–Ω such iteration.
> 
> **–°—Å—ã–ª–∫–∏:** Vaswani 2017 (—Ä–∞–∑–¥–µ–ª ‚ÄúScaled Dot-Product Attention‚Äù)„Äê22‚Ä†L203-L212„Äë; Illustrated Transformer (–æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ—à–∞–≥–æ–≤–æ); –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ PyTorch MultiheadAttention; FlashAttention (Tri Dao 2022) ‚Äì –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —à–∞–≥–∞ softmax.
> 
> ## 6. **–ß—Ç–æ —Ç–∞–∫–æ–µ multi-head attention: —Å–±–æ—Ä–∫–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞** 
> **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** *Multi-head attention* ‚Äì —ç—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ (h) –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (attention heads) –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –∞ –∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –≤—ã—Ö–æ–¥–∞„Äê24‚Ä†L244-L252„Äë„Äê50‚Ä†L445-L454„Äë. –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ ‚Äì –æ—Ç–¥–µ–ª—å–Ω—ã–π scaled dot-product attention (—Å–º. —Ä–∞–∑–¥–µ–ª 5), —Ä–∞–±–æ—Ç–∞—é—â–∏–π –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–æ–µ–∫—Ü–∏–∏ Q, K, V. –ü–æ—Å–ª–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –≥–æ–ª–æ–≤ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –∏ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π \(W^O\). –§–æ—Ä–º–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –≥–æ–ª–æ–≤—ã –¥–∞—é—Ç –≤—ã—Ö–æ–¥—ã \(h_i = \text{Attention}(QW^q_i, KW^k_i, VW^v_i)\) –¥–ª—è \(i=1,\dots,h\), —Ç–æ multi-head –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö: 
> \[ \text{MultiHead}(Q, K, V) = [h_1 \parallel h_2 \parallel \cdots \parallel h_h] W^O, \] 
> –≥–¥–µ \(\parallel\) ‚Äì –æ–ø–µ—Ä–∞—Ü–∏—è –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏„Äê50‚Ä†L493-L502„Äë. –¢–∞–∫ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è ‚Äú—Å–±–æ—Ä–∫–∞ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è‚Äù ‚Äì —Ä–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã –¥–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∞ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π —Å–º–µ—à–∏–≤–∞–µ—Ç –∏—Ö –≤ –æ–¥–Ω–æ.
> 
> **–°–±–æ—Ä–∫–∞ (concatenation):** –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –≤—ã–¥–∞—ë—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∞ \(i\) –≤–µ–∫—Ç–æ—Ä \(y_{i}^{(head)}\) —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ \(d_h\). –ú—ã –æ–±—Ä–∞–∑—É–µ–º –¥–ª–∏–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä \(y_i^{\text{concat}} = [y_{i}^{(1)}; y_{i}^{(2)}; \dots; y_{i}^{(h)}]\) —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ \(h \cdot d_h = d_{\text{model}}\). –ü–æ —Å—É—Ç–∏, –ø—Ä–æ—Å—Ç–æ —Å—Ç–∞–≤–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–æ–ª–æ–≤ ‚Äú–±–æ–∫–æ–º‚Äù –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É. –í —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —á–µ—Ä–µ–∑ reshape –æ–ø–µ—Ä–∞—Ü–∏–π —Ç–µ–Ω–∑–æ—Ä–æ–≤„Äê48‚Ä†L709-L717„Äë„Äê48‚Ä†L731-L738„Äë. 
> 
> **–ê–≥—Ä–µ–≥–∞—Ü–∏—è (–ø—Ä–æ–µ–∫—Ü–∏—è \(W^O\)):** –î–∞–ª–µ–µ —ç—Ç–æ—Ç —Å–æ—Å—Ç–∞–≤–Ω–æ–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π \(W^O \in \mathbb{R}^{(h d_h) \times d_{\text{model}}}\). –û–±—ã—á–Ω–æ \(h d_h = d_{\text{model}}\), —Ç–∞–∫ —á—Ç–æ \(W^O\) ‚Äì –∫–≤–∞–¥—Ä–∞—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∞—è, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞ –¥—Ä—É–≥–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—ã—Ö–æ–¥–∞). \(W^O\) –æ–±—É—á–∞–µ—Ç—Å—è, —á—Ç–æ–±—ã –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≥–æ–ª–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, –¥—Ä—É–≥–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫—É, —Ç–æ \(W^O\) –º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å—Å—è –∏–∑–≤–ª–µ–∫–∞—Ç—å –Ω—É–∂–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ –æ–±–µ–∏—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è.
> 
> **–ó–∞—á–µ–º –Ω—É–∂–Ω–∞ –ø—Ä–æ–µ–∫—Ü–∏—è \(W^O\):** –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —Å–∞–º–∞ –ø–æ —Å–µ–±–µ –ø—Ä–æ—Å—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç, –Ω–æ –Ω–µ —Å–º–µ—à–∏–≤–∞–µ—Ç. –ë–µ–∑ \(W^O\) –º–æ–¥–µ–ª—å –Ω–µ –º–æ–≥–ª–∞ –±—ã –æ–±—É—á–∏—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –≤—ã—Ö–æ–¥–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤ ‚Äì –æ–Ω–∏ –±—ã –ø—Ä–æ—Å—Ç–æ —à–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –∫–∞–Ω–∞–ª–∞–º–∏. \(W^O\) –¥–∞—ë—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω–æ —Å–ª–æ–∂–∏—Ç—å –∏—Ö: –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –¥–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤ –≤–∞–∂–Ω—ã —Ç–æ–ª—å–∫–æ –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏, \(W^O\) —ç—Ç–æ —É—á—Ç—ë—Ç. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏, \(W^O\) –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–ª–µ–¥—É—é—â–µ–º—É —Å–ª–æ—é –≤–∏–¥–µ—Ç—å –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤.
> 
> **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è —Å—Ö–µ–º–∞:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –±–ª–æ–∫: –Ω–∞ –≤—Ö–æ–¥–µ ‚Äì \(Q, K, V\). –ö–∞–∂–¥—ã–π –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (–Ω–∞ —Ä–∏—Å—É–Ω–∫–∞—Ö –æ–±—ã—á–Ω–æ —Ä–∏—Å—É—é—Ç 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü—ã, –≤—ã—Ö–æ–¥—è—â–∏–µ –Ω–∞ h –∫–æ–ø–∏–π). –î–∞–ª–µ–µ —Ä–∏—Å—É—é—Ç h –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö ‚ÄúScaled Dot-Product Attention‚Äù –±–ª–æ–∫–æ–≤, –≤—ã—Ö–æ–¥—ã –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ö–æ–¥—è—Ç—Å—è –∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è. –ó–∞—Ç–µ–º ‚Äì –æ–¥–∏–Ω –≤—ã—Ö–æ–¥–Ω–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π. –≠—Ç–æ –∏ –µ—Å—Ç—å multi-head. –í –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ Transformer (Figure 2) –ø—Ä—è–º–æ –ø–æ–∫–∞–∑–∞–Ω–æ: —Å–ø—Ä–∞–≤–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö attention, –ø–æ—Ç–æ–º concat„Äê24‚Ä†L240-L248„Äë.
> 
> **–ü—Ä–∏–º–µ—Ä:** –í BERT (12 –≥–æ–ª–æ–≤) ‚Äì –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 12 —Ä–∞–∑–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–∏ 12 –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–æ–µ–¥–∏–Ω—è—é—Ç—Å—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è. –ï—Å–ª–∏ –≤–∑–≥–ª—è–Ω—É—Ç—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä, —Å–∫–∞–∂–µ–º, —Å–ª–æ–≤–æ ‚Äúbank‚Äù –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Å–ª–æ–≤–µ ‚Äúriver‚Äù (–∑–Ω–∞—á–µ–Ω–∏–µ ‚Äì –∫–∞–∫ –±–µ—Ä–µ–≥ —Ä–µ–∫–∏), –¥—Ä—É–≥–∞—è ‚Äì –Ω–∞ —Å–ª–æ–≤–µ ‚Äúfinance‚Äù (–∑–Ω–∞—á–µ–Ω–∏–µ ‚Äì –±–∞–Ω–∫ –∫–∞–∫ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ). Multi-head concat –æ–±—ä–µ–¥–∏–Ω–∏—Ç –æ–±–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∞ \(W^O\) –ø–æ–º–æ–∂–µ—Ç —Å–ª–∏—Ç—å –∏—Ö –≤ –µ–¥–∏–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞ ‚Äúbank‚Äù —Å —É—á—ë—Ç–æ–º –æ–±–µ–∏—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –º–æ–¥–µ–ª—å —É–¥–µ—Ä–∂–∏—Ç –∏ —Ç—É, –∏ –¥—Ä—É–≥—É—é –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É, —á—Ç–æ –ø–æ–≤—ã—Å–∏—Ç —ë–º–∫–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.
> 
> **–°–≤—è–∑—å —Å —Ñ–æ—Ä–º—É–ª–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:** Multi-head –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ: —É –Ω–∞—Å \(W^Q, W^K, W^V\) –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã, –∏—Ç–æ–≥–æ \(3h \cdot (d_{\text{model}} \times d_h)\) –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø–ª—é—Å \(W^O\) ‚Äì \((h d_h \times d_{\text{model}})\). –ï—Å–ª–∏ \(d_h = d_{\text{model}}/h\), —Å—É–º–º–∞—Ä–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–≤–Ω—ã \(3d_{\text{model}} d_h h + d_{\text{model}} d_h h = 4 d_{\text{model}} d_h h\). –ù–æ —Ç–∞–∫ –∫–∞–∫ \(d_h h = d_{\text{model}}\), —Ç–æ —ç—Ç–æ \(4 d_{\text{model}}^2\) ‚Äì —Å—Ç–æ–ª—å–∫–æ –∂–µ, —Å–∫–æ–ª—å–∫–æ –±—ã–ª–æ –±—ã –¥–ª—è –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã —Ä–∞–∑–º–µ—Ä–æ–º \(d_{\text{model}}\) (–∫–æ—Ç–æ—Ä–∞—è –¥–∞–ª–∞ –±—ã \(W^Q, W^K, W^V, W^O\) –∫–∞–∂–¥—ã–π \(d_{\text{model}}\times d_{\text{model}}\), –∏—Ç–æ–≥–æ —Ç–∞–∫–∂–µ \(4d_{\text{model}}^2\)). –¢–æ –µ—Å—Ç—å –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–≤—ã—à–∞–µ—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å„Äê20‚Ä†L107-L115„Äë„Äê24‚Ä†L258-L261„Äë.
> 
> **–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:** –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–æ–∫–∞–∑–∞–ª–æ —Å–≤–æ—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –º–∞—à–ø–µ—Ä–µ–≤–æ–¥–µ: –º–æ–¥–µ–ª—å —Å multi-head —Ç–æ—á–Ω–µ–µ, —á–µ–º single-head —Å —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö. Attention-–º–∞—Ç—Ä–∏—Ü—ã —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤ —á–∞—Å—Ç–æ —Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è ‚Äì –æ–¥–Ω–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ—á—Ç–∏ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–π (–∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Å–µ–±—è –∏–ª–∏ —Å–æ—Å–µ–¥–µ–π), –¥—Ä—É–≥–∞—è ‚Äì –∏–º–µ—Ç—å —à–∏—Ä–æ–∫–æ–µ —Ä–∞–∑–º—ã—Ç–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (—É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç)„Äê41‚Ä†L91-L99„Äë„Äê41‚Ä†L131-L139„Äë. –≠—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∏–¥–µ—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, multi-head attention –¥–∞–ª —Ç–æ–ª—á–æ–∫ –∫ –∏–∑–æ–±–∏–ª–∏—é *–∞–Ω–∞–ª–∏–∑–æ–≤ –≥–æ–ª–æ–≤* (—Å–º. —Ä–∞–∑–¥–µ–ª 8): –ø–æ—Å–∫–æ–ª—å–∫—É –≥–æ–ª–æ–≤ –º–Ω–æ–≥–æ, –∏—Ö –ª–µ–≥—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏.
> 
> **–ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:** Multi-head –ª–µ–≥—á–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—Å—è, –∫–∞–∫ –æ—Ç–º–µ—á–∞–ª–æ—Å—å. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–≤—ë—Ä—Ç—ã–≤–∞–µ—Ç –≤—Å–µ heads –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –º–∞—Ç—Ä–∏—á–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è —á—Ç–æ Q, K, V –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ [batch, n, h, d_h] –ø–æ—Å–ª–µ `transpose_qkv`„Äê48‚Ä†L729-L737„Äë, –∞ –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –º–∞—Ç—Ä–∏—á–Ω—ã–µ —É–º–Ω–æ–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ [batch*h, n, d_h] √ó [batch*h, d_h, n] = [batch*h, n, n] –¥–ª—è softmax –∏ [batch*h, n, n] √ó [batch*h, n, d_h] –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è [batch*h, n, d_h], –∏ –ø–æ—Ç–æ–º –≤–µ—Ä–Ω—É—Ç—å –æ–±—Ä–∞—Ç–Ω–æ —Ñ–æ—Ä–º—É [batch, n, h*d_h]„Äê48‚Ä†L705-L713„Äë„Äê48‚Ä†L718-L726„Äë. –¢–æ –µ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–æ–¥–∞ multi-head ‚Äì —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ, –ø–æ—á—Ç–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ü–∏–∫–ª–∞. –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É, –¥–∞–∂–µ –¥–µ—Å—è—Ç–∫–∏ –≥–æ–ª–æ–≤ –Ω–µ —Å–∏–ª—å–Ω–æ –∑–∞–º–µ–¥–ª—è—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** Multi-head attention –º–æ–∂–Ω–æ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å –∫–∞–∫ **–∫–æ–º–∏—Ç–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤**. –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ ‚Äì ‚Äú—ç–∫—Å–ø–µ—Ä—Ç‚Äù –ø–æ —Å–≤–æ–µ–º—É –≤–æ–ø—Ä–æ—Å—É, –∏ –º—ã –≤ –∫–æ–Ω—Ü–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö –∑–Ω–∞–Ω–∏—è. –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—É—é —Ç–µ–æ—Ä–∏—é, –≥–¥–µ –º–æ–∑–≥ –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π –≤–Ω–∏–º–∞–Ω–∏—è, –∞ –∑–∞—Ç–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ–±—â–µ–µ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–µ–ª–∞–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ ‚Äì –Ω–∏–∫–∞–∫–æ–≥–æ —Å–ª–æ–∂–Ω–æ–≥–æ ‚Äú—Å–ø–æ—Ä–∞‚Äù –º–µ–∂–¥—É –≥–æ–ª–æ–≤–∞–º–∏ –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ –ª–∏–Ω–µ–π–Ω–∞—è —Å–º–µ—Å—å. –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ —É–ø—Ä–æ—â–µ–Ω–∏–µ: –≤ –º–æ–∑–≥–µ –º–æ–¥—É–ª–∏ –º–æ–≥—É—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å. –ù–æ –¥–ª—è –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–π –ø—Ä–æ—Å—Ç–æ—Ç—ã –ª–∏–Ω–µ–π–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –æ–∫–∞–∑–∞–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ—â–Ω–æ. Multi-head ‚Äì –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –ø—Ä–∏–º–µ—Ä, –∫–∞–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–∏–Ω–µ—Ä–≥–∏–∏ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏, –∏ –∫–∞–∫ –≤ –∏—Å–∫—É—Å—Ç–≤–µ–Ω–Ω–æ–º ‚Äú–≤–Ω–∏–º–∞–Ω–∏–∏‚Äù –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –∞–Ω–∞–ª–æ–≥ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è.
> 
> **–°—Å—ã–ª–∫–∏:** Vaswani 2017 (–æ–ø–∏—Å–∞–Ω–∏–µ multi-head –∏ —Ä–∏—Å.2)„Äê24‚Ä†L244-L252„Äë; –±–ª–æ–≥ ML@CMU (—Ñ–æ—Ä–º—É–ª–∞ multi-head)„Äê20‚Ä†L100-L108„Äë; –∫–æ–¥ D2L (—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è transpose –¥–ª—è heads)„Äê48‚Ä†L709-L717„Äë.
> 
> ## 7. **–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è** 
> **–ù–∞–±–ª—é–¥–µ–Ω–∏–µ –æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏:** –í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –±–æ–ª—å—à–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —á–∞—Å—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Ä–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã –Ω–∞—á–∏–Ω–∞—é—Ç –æ—Ç–≤–µ—á–∞—Ç—å –∑–∞ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö. *–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è* –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äú–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞‚Äù –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —Ç–∏–ø –æ—Ç–Ω–æ—à–µ–Ω–∏–π: –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏—Ç –ø–∞—Ä—ã —Å–∫–æ–±–æ–∫, –¥—Ä—É–≥–∞—è ‚Äì —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–º—ë–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö, —Ç—Ä–µ—Ç—å—è ‚Äì —Ä–µ–∞–ª–∏–∑—É–µ—Ç –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –≠—Ç–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç *–Ω–µ —è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω–æ*, –∞ –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç, —á—Ç–æ –≤—ã–≥–æ–¥–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Ä–∞–±–æ—Ç—É –º–µ–∂–¥—É –≥–æ–ª–æ–≤–∞–º–∏. 
> 
> **–ú–µ—Ö–∞–Ω–∏–∑–º –æ–±—É—á–µ–Ω–∏—è:** –í –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏) –≤—Å–µ –≥–æ–ª–æ–≤—ã —Å–ª—É—á–∞–π–Ω—ã –∏ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã (–æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è). –û–¥–Ω–∞–∫–æ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, –ø—Ä–æ—Ö–æ–¥—è –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã, –≥–¥–µ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ ‚Äú–Ω–∞—à–ª–∞ –Ω–∏—à—É‚Äù. –§–æ—Ä–º–∞–ª—å–Ω–æ, –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –ø–æ –≥–æ–ª–æ–≤–∞–º ‚Äì —ç—Ç–æ –µ–¥–∏–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å. –ù–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã \(W^Q_i, W^K_i, W^V_i\) –±—É–¥—É—Ç —Ä–∞–∑–Ω—ã–º–∏, –∑–∞–≤–∏—Å—è—â–∏–º–∏ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫—É—é —Ä–æ–ª—å —ç—Ç–∞ –≥–æ–ª–æ–≤–∞ –Ω–∞—á–∞–ª–∞ –∏–≥—Ä–∞—Ç—å. –ï—Å–ª–∏ –¥–≤–µ –≥–æ–ª–æ–≤—ã –ø—ã—Ç–∞—é—Ç—Å—è –¥–µ–ª–∞—Ç—å –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ, —É –Ω–∏—Ö —Å—Ö–æ–∂–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –∏, –≤–æ–∑–º–æ–∂–Ω–æ, –æ–¥–Ω–∞ –∏–∑ –Ω–∏—Ö –Ω–∞—á–Ω–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –≤ –¥—Ä—É–≥–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∏–∑-–∑–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ñ–ª—É–∫—Ç—É–∞—Ü–∏–π, —á—Ç–æ–±—ã –¥–æ—Å—Ç–∏—á—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è –ª–æ—Å—Å–∞ ‚Äì —Ç–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç *—Å–∏–º–º–µ—Ç—Ä–∏—è-–Ω–∞—Ä—É—à–µ–Ω–∏–µ*. –í –∏—Ç–æ–≥–µ, —Å–ø—É—Å—Ç—è –º–Ω–æ–≥–æ —ç–ø–æ—Ö, –º–æ–∂–Ω–æ –≤–∏–¥–µ—Ç—å, —á—Ç–æ, —Å–∫–∞–∂–µ–º, –≥–æ–ª–æ–≤a ‚Ññ5 –Ω–∞ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –∞–∫—Ç–∏–≤–Ω–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞ –≥–æ–ª–æ–≤a ‚Ññ8 ‚Äì –ø—Ä–∏ –¥—Ä—É–≥–æ–π. –≠—Ç–æ –∏ –µ—Å—Ç—å –∏—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
> 
> **–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏**: –†–∞–±–æ—Ç—ã Olah et al. (OpenAI Circuits) –∏ Neel Nanda et al. –ø–æ–∫–∞–∑–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: 
> - –í GPT-2 –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã *–∏–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã (induction heads)* ‚Äì –ø–∞—Ä–∞ –≥–æ–ª–æ–≤ (–æ–±—ã—á–Ω–æ –∏–∑ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤), –∫–æ—Ç–æ—Ä—ã–µ –≤–º–µ—Å—Ç–µ —Ä–µ–∞–ª–∏–∑—É—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º: –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –∫–æ–ø–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω A, –¥—Ä—É–≥–∞—è ‚Äì –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –ø–æ—è–≤–ª–µ–Ω–∏–∏ A –≤ —Ç–µ–∫—Å—Ç–µ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π –∑–∞ –ø–µ—Ä–≤—ã–º A —Ç–æ–∫–µ–Ω B –∏ –∫–æ–ø–∏—Ä—É–µ—Ç –µ–≥–æ –ø–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ A„Äê0‚Ä†L1-L4„Äë„Äê35‚Ä†L56-L64„Äë. –≠—Ç–æ —Å–ª–æ–∂–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ, –ø—Ä–æ—è–≤–∏–≤—à–µ–µ—Å—è –ª–∏—à—å –Ω–∞—á–∏–Ω–∞—è —Å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –º–æ–¥–µ–ª–∏. 
> - *Name mover heads:* –í —Ç–æ–π –∂–µ GPT-2 –º–∞–ª–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞—à–ª–∏ –≥–æ–ª–æ–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ –∫–æ–ø–∏—Ä—É—é—Ç –∏–º—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑ –Ω–∞—á–∞–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –∫–æ–Ω–µ—Ü (—Ä–µ—à–∞—è –∑–∞–¥–∞—á—É coreference)„Äê37‚Ä†L363-L370„Äë„Äê37‚Ä†L367-L375„Äë. –û–Ω–∏ –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–æ–∫–µ–Ω–µ <END> –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Ç—è–Ω—É—Ç Value —Å –ø–æ–∑–∏—Ü–∏–π –∏–º—ë–Ω. –ó–Ω–∞—á–∏—Ç, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≥–æ–ª–æ–≤—ã —É—á–∞—Ç—Å—è –∑–∞–¥–∞—á–∞–º –≤—Ä–æ–¥–µ ‚Äú—Å–∫–æ–ø–∏—Ä—É–π –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è‚Äù. 
> - *Syntactic heads:* Lena Voita –∏ —Å–æ–∞–≤—Ç–æ—Ä—ã (2019) –≤ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Å–º–æ–≥–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –≥–æ–ª–æ–≤—ã: —É –Ω–∏—Ö –±—ã–ª–∏ ‚Äúhead‚Äù –æ—Ç–≤–µ—á–∞—é—â–∏–µ –∑–∞ –ø—Ä—è–º—ã–µ –æ–±—ä–µ–∫—Ç–Ω—ã–µ —Å–≤—è–∑–∏, –∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–∏—Å–µ–ª (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ/–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ) –∏ —Ç.–¥. –û–Ω–∏ –¥–∞–∂–µ –≤—Ä—É—á–Ω—É—é –≤—ã–∫–ª—é—á–∞–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã –∏ –≤–∏–¥–µ–ª–∏, —á—Ç–æ –Ω–∞–ø—Ä–∏–º–µ—Ä –±–µ–∑ ‚Äú—Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –≥–æ–ª–æ–≤—ã‚Äù –ø–∞–¥–∞–ª–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –ø–æ–¥–ª–µ–∂–∞—â–µ–≥–æ –∏ —Å–∫–∞–∑—É–µ–º–æ–≥–æ –≤ –ø–µ—Ä–µ–≤–æ–¥–∞—Ö„Äê20‚Ä†L158-L167„Äë.
> 
> **–ü–æ—á–µ–º—É –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:** –° –æ–¥–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –º–æ–¥–µ–ª—å *–∏–∑–±—ã—Ç–æ—á–Ω–∞* ‚Äì —É –Ω–µ—ë –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á–µ–º –Ω—É–∂–Ω–æ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –ª–æ—Å—Å–∞; SGD —á–∞—Å—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–±–æ—Ç—É –º–µ–∂–¥—É –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –° –¥—Ä—É–≥–æ–π, —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å–∏–≥–Ω–∞–ª–æ–≤ —Ç—Ä–µ–±—É—é—Ç —Ä–∞–∑–Ω—ã—Ö ‚Äú—Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤–Ω–∏–º–∞–Ω–∏—è‚Äù. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –ø—ã—Ç–∞–ª–∞—Å—å –¥–µ–ª–∞—Ç—å –≤—Å—ë –ø–æ–Ω–µ–º–Ω–æ–≥—É, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ (—Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏) —Ä–∞–∑–¥–µ–ª–∏—Ç—å: —Ç–∞–∫ –±—ã—Å—Ç—Ä–µ–µ —Å–Ω–∏–∑–∏—Ç—å –æ—à–∏–±–∫—É. –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ ‚Äú–∫—Ä—É—Ç–æ–π —Ç—Ä–æ–ø–µ‚Äù —É–º–µ–Ω—å—à–µ–Ω–∏—è –ª–æ—Å—Å–∞ –≤ —Å–≤–æ–µ–π –ø–æ–¥—Å—Ñ–µ—Ä–µ. –≠—Ç–æ –Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –º–æ–∑–≥–µ –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ —Å—Ç–∏–º—É–ª—ã.
> 
> **–≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è:** –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 2025 –≥. Zheng et al. ‚ÄúAttention Heads of LLMs: A Survey‚Äù –æ—Ç–º–µ—á–∞–µ—Ç, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∞–¥–∏—è—Ö ‚Äú–º—ã—à–ª–µ–Ω–∏—è‚Äù –º–æ–¥–µ–ª–∏ ‚Äì –æ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤ –¥–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞„Äê9‚Ä†L23-L31„Äë„Äê9‚Ä†L51-L57„Äë. –û–Ω–∏ –≤–≤–æ–¥—è—Ç —á–µ—Ç—ã—Ä–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: *Knowledge Recalling heads*, *In-Context Identification heads*, *Latent Reasoning heads*, *Expression Preparation heads*. –ù–∞–ø—Ä–∏–º–µ—Ä, *Knowledge recalling* ‚Äì –≥–æ–ª–æ–≤—ã, –≤—ã—Ç–∞—Å–∫–∏–≤–∞—é—â–∏–µ –Ω—É–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã (—á–∞—Å—Ç–æ –∏–∑ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–ª–æ—ë–≤); *Expression preparation* ‚Äì –≥–æ–ª–æ–≤—ã —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ—ë–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç –≤ –∫—É—á—É, –∫–∞–∫ –æ—Ç–≤–µ—Ç —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å„Äê9‚Ä†L23-L31„Äë„Äê9‚Ä†L43-L52„Äë. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≥–æ–ª–æ–≤—ã –Ω–µ —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç –∑–∞–¥–∞—á–∏, –Ω–æ –∏ *–º–µ–∂–¥—É —Å–ª–æ—è–º–∏* –∏–¥—ë—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Å–∫–∞–¥–æ–º.
> 
> **–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:** –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–æ–ª–æ–≤ –Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –æ—Ç–¥–µ–ª–æ–≤ –≤ –∫–æ–º–ø–∞–Ω–∏–∏: –≤–Ω–∞—á–∞–ª–µ –≤—Å–µ —Ä–∞–±–æ—Ç–Ω–∏–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ –ø—ã—Ç–∞—é—Ç—Å—è –¥–µ–ª–∞—Ç—å –≤—Å—ë, –Ω–æ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –æ–Ω–∏ –Ω–∞—Ö–æ–¥—è—Ç, –∫—Ç–æ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —á–µ–º, –∏ –Ω–∞—á–∏–Ω–∞—é—Ç —Ä–∞–∑–¥–µ–ª—è—Ç—å –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏. –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ ‚Äì –∫–∞–∫ —ç–≤–æ–ª—é—Ü–∏—è –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç —Å–≤–æ–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã. –í –∏—Ç–æ–≥–µ –ø–æ—è–≤–ª—è–µ—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—è: –Ω–∏–∂–Ω–∏–µ —Å–ª–æ–∏/–≥–æ–ª–æ–≤—ã –∑–∞–Ω–∏–º–∞—é—Ç—Å—è –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–º–∏ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ —à–∞–±–ª–æ–Ω–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, –±–ª–∏–∑–ª–µ–∂–∞—â–∏–µ —Å–ª–æ–≤–∞), —Å—Ä–µ–¥–Ω–∏–µ ‚Äì —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º–∏ (—Å–∏–Ω—Ç–∞–∫—Å–∏—Å, –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏), –≤–µ—Ä—Ö–Ω–∏–µ ‚Äì –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º–∏ (–≤—ã–±–æ—Ä –Ω—É–∂–Ω–æ–≥–æ —Ñ–∞–∫—Ç–∞, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ –æ—Ç–≤–µ—Ç–∞). –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –∏ –≤ *–∫–æ–æ–ø–µ—Ä–∏—Ä—É—é—â–∏—Ö –≥—Ä—É–ø–ø–∞—Ö –≥–æ–ª–æ–≤*: –Ω–∞–ø—Ä–∏–º–µ—Ä, induction heads —Ä–∞–±–æ—Ç–∞—é—Ç –ø–∞—Ä–æ–π (–æ–¥–Ω–∞ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –¥—Ä—É–≥–∞—è –∑–∞–≤–µ—Ä—à–∞–µ—Ç —à–∞–±–ª–æ–Ω)„Äê0‚Ä†L1-L4„Äë„Äê35‚Ä†L58-L66„Äë. –¢–æ –µ—Å—Ç—å –≥–æ–ª–æ–≤—ã –º–æ–≥—É—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –æ–¥–∏–Ω–æ—á–∫—É —Å–ø–µ—Ü–∑–∞–¥–∞—á—É —Ä–µ—à–∞—Ç—å, –Ω–æ –∏ –æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å ‚Äú–ø–æ–¥—Ü–∏–∫—Ç—ã‚Äù –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.
> 
> **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:** –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤—ã—è–≤–ª—è—é—Ç –º–µ—Ç–æ–¥–∞–º–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏: –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è (attention patterns) ‚Äì –∫–æ–≥–¥–∞ —Ä–∏—Å—É—é—Ç –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è, —á–∞—Å—Ç–æ –≤–∏–¥—è—Ç —è–≤–Ω—ã–π –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–π –∏–ª–∏ –æ—Ñ—Ñ-–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–π —É–∑–æ—Ä, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–∏–∞–≥–æ–Ω–∞–ª—å —Å–æ —Å–º–µ—â–µ–Ω–∏–µ–º = ‚Äú—Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω‚Äù); *–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –ø—Ä–æ–±–∏–Ω–≥–æ–º:* –æ–±—É—á–∞—é—Ç –ø—Ä–æ—Å—Ç—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏–Ω–≥–≤. —Å–≤–æ–π—Å—Ç–≤–∞ –ø–æ –≤—ã—Ö–æ–¥–∞–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤; *–∞–±–ª—è—Ü–∏—è:* –æ—Ç–∫–ª—é—á–∞—é—Ç –≥–æ–ª–æ–≤—É –∏ —Å–º–æ—Ç—Ä—è—Ç, —á—Ç–æ –ª–æ–º–∞–µ—Ç—Å—è –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏„Äê20‚Ä†L130-L139„Äë„Äê20‚Ä†L158-L167„Äë. –ù–∞–ø—Ä–∏–º–µ—Ä, ablation –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ —É–¥–∞–ª–µ–Ω–∏–µ ‚Äú–∏–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –≥–æ–ª–æ–≤‚Äù —Ä–µ–∑–∫–æ —Å–Ω–∏–∂–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ Zero-Shot –æ–±—É—á–µ–Ω–∏–∏„Äê35‚Ä†L58-L66„Äë. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —Ç–µ –≥–æ–ª–æ–≤—ã –æ—Ç–≤–µ—á–∞–ª–∏ –∑–∞ in-context learning, —Ç.–µ. –Ω–∞—É—á–∏–ª–∏—Å—å —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π –≤—ã–≤–æ–¥:** –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Äì –ø—Ä–æ—è–≤–ª–µ–Ω–∏–µ *—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç—Ä—É–¥–∞* –≤–Ω—É—Ç—Ä–∏ –ò–ò. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ, –±–µ–∑ —è–≤–Ω—ã—Ö —Å—É–±-—Ü–µ–ª–µ–π. –≠—Ç–æ —Ä–æ–¥–Ω–∏—Ç –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Å –º–æ–∑–≥–æ–º: —Ç–∞–º —Ç–æ–∂–µ —Ä–∞–∑–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã/–æ–±–ª–∞—Å—Ç–∏ —Ä–µ–∞–≥–∏—Ä—É—é—Ç –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Å—Ç–∏–º—É–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–π—Ä–æ–Ω ‚Äú–î–∂–µ–π–Ω–∏—Ñ–µ—Ä –≠–Ω–∏—Å—Ç–æ–Ω‚Äù ‚Äì –∫—Ä–∞–π–Ω–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!). –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ ‚Äì –∫–∞–∫ –º–∞–ª–µ–Ω—å–∫–∏–π —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ–±—à–∏—Ä–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞—ë—Ç –Ω–∞–¥–µ–∂–¥—É –Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å: –º—ã –º–æ–∂–µ–º —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–µ –∫–∞–∫ –º–∏–ª–ª–∏–∞—Ä–¥—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–ø–ª–æ—à–Ω—è–∫–æ–º, –∞ –∫–∞–∫ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç—å –º–æ–¥—É–ª–µ–π (–≥–æ–ª–æ–≤), —É –∫–∞–∂–¥–æ–≥–æ ‚Äì –±–æ–ª–µ–µ —è—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –û–¥–Ω–∞–∫–æ —Ç–∞–∫–∂–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –≤–æ–ø—Ä–æ—Å: –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–∏ –º—ã —á—Ç–æ-—Ç–æ –≤–Ω–µ –≥–æ–ª–æ–≤? (–í–µ–¥—å –µ—Å—Ç—å –µ—â—ë FFN-—Å–ª–æ–∏, –≥–¥–µ —Ç–æ–∂–µ —á—Ç–æ-—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç). –ù–æ –≥–æ–ª–æ–≤—ã ‚Äì –Ω–∞–∏–±–æ–ª–µ–µ –æ–±–æ–∑—Ä–∏–º–∞—è –µ–¥–∏–Ω–∏—Ü–∞ ‚Äú–º—ã—Å–ª–∏‚Äù –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è.
> 
> **–°—Å—ã–ª–∫–∏:** Olsson et al. 2022 (Induction Heads)„Äê35‚Ä†L56-L64„Äë; Wang et al. 2022 (IOI circuit, Name mover heads)„Äê37‚Ä†L357-L365„Äë„Äê37‚Ä†L367-L375„Äë; Zheng et al. 2024 (Survey on head functions)„Äê9‚Ä†L23-L31„Äë„Äê9‚Ä†L51-L57„Äë.
> 
> ## 8. **–ö–∞–∫–∏–µ –≥–æ–ª–æ–≤—ã –¥–µ–ª–∞—é—Ç —á—Ç–æ: copy, induction, coreference, syntactic, lexical** 
> –í —Ö–æ–¥–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤—ã–¥–µ–ª–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ *—Ç–∏–ø–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è* –ø–æ –∏—Ö —Ñ—É–Ω–∫—Ü–∏—è–º:
> 
> - **Copy-heads (–≥–æ–ª–æ–≤—ã –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è):** –≠—Ç–∏ –≥–æ–ª–æ–≤—ã –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ –ø—Ä—è–º–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏–º–µ—Ä ‚Äì *Duplicate Token Head*: –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞. –í —Å—Ö–µ–º–µ IOI (Indirect Object Identification) Duplicate Token Heads –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è –Ω–∞ –≤—Ç–æ—Ä–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–∏ –∏–º–µ–Ω–∏ –∏ –ø–æ–º–µ—á–∞—é—Ç —Ñ–∞–∫—Ç –¥—É–±–ª–∏–∫–∞—Ç–∞„Äê37‚Ä†L355-L363„Äë. –ï—Å—Ç—å –≥–æ–ª–æ–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–ø–∏—Ä—É—é—Ç –∑–Ω–∞–∫ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏–ª–∏ –∫–∞–≤—ã—á–∫–∏ –∏–∑ –Ω–∞—á–∞–ª–∞ –≤ –∫–æ–Ω–µ—Ü. –í –º–æ–¥–µ–ª—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ copy-heads –º–æ–≥—É—Ç –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞ –≤ –≤—ã—Ö–æ–¥ (–ø–æ–ª–µ–∑–Ω–æ –≤ summarization, –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω –∏ –¥–∞—Ç). –ù–∞–ø—Ä–∏–º–µ—Ä, –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —à–∞–±–ª–æ–Ω: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π —Ç–æ–∫–µ–Ω —Ä–∞–Ω—å—à–µ ‚Äì —ç—Ç–æ —á–∏—Å—Ç–æ ‚Äú—Å–∫–æ–ø–∏—Ä—É–π –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Ç–∞–∫–æ–µ –∂–µ —Å–ª–æ–≤–æ‚Äù. –ò–º–µ–Ω–Ω–æ induction heads (—Å–º. –Ω–∏–∂–µ) —á–∞—Å—Ç–∏—á–Ω–æ —è–≤–ª—è—é—Ç—Å—è copy-–º–µ—Ö–∞–Ω–∏–∑–º–æ–º: –ø–µ—Ä–≤–∞—è –≥–æ–ª–æ–≤–∞ –∫–æ–ø–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω A, –≤—Å—Ç—Ä–µ—Ç–∏–≤ –µ–≥–æ —Å–Ω–æ–≤–∞„Äê0‚Ä†L1-L4„Äë. Copy-heads —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –≤–Ω–∏–º–∞–Ω–∏–µ —Ç–∏–ø–∞ ‚Äú—Å–ª–æ–≤–æ -> —Ç–æ –∂–µ —Å–ª–æ–≤–æ —Ä–∞–Ω–µ–µ‚Äù –∏–ª–∏ ‚Äú—Å–∏–º–≤–æ–ª -> —Ç–∞–∫–æ–π –∂–µ —Å–∏–º–≤–æ–ª‚Äù. –û–Ω–∏ –∂–µ –º–æ–≥—É—Ç –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏, –∫–∞–≤—ã—á–∫–∏, –∫–æ–ø–∏—Ä—É—è –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–π —Å–∏–º–≤–æ–ª.
> 
> - **Induction heads (–∏–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã):** –û—Å–æ–±—ã–π –∫–ª–∞—Å—Å, –≤—ã—è–≤–ª–µ–Ω–Ω—ã–π –∫–æ–º–∞–Ω–¥–æ–π Anthropic„Äê35‚Ä†L56-L64„Äë. –≠—Ç–æ –ø–∞—Ä—ã –≥–æ–ª–æ–≤ –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–æ—è—Ö: –ø–µ—Ä–≤–∞—è –≥–æ–ª–æ–≤–∞ (–≤ —Å–ª–æ–µ L) –±–µ—Ä—ë—Ç —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω A –∏ –∏—â–µ—Ç –µ–≥–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ; –≤—Ç–æ—Ä–∞—è –≥–æ–ª–æ–≤–∞ (—Å–ª–æ–π L+1) –±–µ—Ä—ë—Ç —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è A (–Ω–∞–∑–æ–≤—ë–º –µ–≥–æ B) –∏ –∫–æ–ø–∏—Ä—É–µ—Ç B –≤ –ø–æ–∑–∏—Ü–∏—é –ø–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ A. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –±—ã *–ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —à–∞–±–ª–æ–Ω—É*. –≠—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç —Ñ–µ–Ω–æ–º–µ–Ω *in-context learning*: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–ª–∞ —à–∞–±–ª–æ–Ω ‚ÄúA ... B ... A ... ?‚Äù, –æ–Ω–∞ –≤—ã–¥–∞—Å—Ç B –ø–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ A, –∏–º–∏—Ç–∏—Ä—É—è –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ª–µ—Ç—É. –ò–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã –ø–æ—è–≤–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞—á–∏–Ω–∞—è —Å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–∏ (–≤ –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö –Ω–µ—Ç). –≠—Ç–æ –º–æ—â–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º: –ø–æ —Å—É—Ç–∏, –¥–≤–µ –≥–æ–ª–æ–≤—ã —Ä–µ–∞–ª–∏–∑—É—é—Ç –ø—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —à–∞–±–ª–æ–Ω—É„Äê35‚Ä†L58-L66„Äë. –ò—Ö –Ω–∞–ª–∏—á–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å —Ä–µ–∑–∫–∏–º —É–ª—É—á—à–µ–Ω–∏–µ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ Zero-Shot –æ–±—É—á–µ–Ω–∏—é –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è ‚Äúbump‚Äù –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ –ª–æ—Å—Å–∞)„Äê35‚Ä†L60-L64„Äë. –¢–æ –µ—Å—Ç—å –º–æ–¥–µ–ª—å –≤–Ω–µ–∑–∞–ø–Ω–æ —É—á–∏—Ç—Å—è —É—á–∏—Ç—å—Å—è ‚Äì –±–ª–∞–≥–æ–¥–∞—Ä—è —Ä–∞–∑–≤–∏—Ç–∏—é —Ç–∞–∫–∏—Ö –≥–æ–ª–æ–≤.
> 
> - **Coreference heads (–≥–æ–ª–æ–≤—ã –∫–æ—Ä—Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏):** –≠—Ç–∏ –≥–æ–ª–æ–≤—ã –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç, –∫ –∫–∞–∫–æ–º—É —Å—É—â–Ω–æ—Å—Ç–Ω–æ–º—É —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –≥–æ–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–∏ ‚Äú–æ–Ω–∞‚Äù –æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –∂–µ–Ω—Å–∫–æ–≥–æ —Ä–æ–¥–∞ –∏–º—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, Clark et al. 2019 –¥–ª—è BERT) –Ω–∞—Ö–æ–¥–∏–ª–∏ –≥–æ–ª–æ–≤—ã, –≤–Ω–∏–º–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä—ã—Ö —è–≤–Ω–æ —Å–æ–µ–¥–∏–Ω—è–µ—Ç –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∏–º–µ–Ω–∞–º–∏. –í *–ø—Ä–∏–º–µ—Ä:* Attention pattern –≤ GPT-2 –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ —Å–≤—è–∑–∞–ª–∞ ‚Äúhe‚Äù —Å –±–ª–∏–∂–∞–π—à–∏–º –º—É–∂—Å–∫–∏–º –∏–º–µ–Ω–µ–º, –¥—Ä—É–≥–∞—è ‚Äúshe‚Äù ‚Äì —Å –∂–µ–Ω—Å–∫–∏–º„Äê0‚Ä†L16-L20„Äë. –¢–∞–∫–∂–µ –ø—Ä–æ–µ–∫—Ç –ê–ª–µ–Ω–Ω–∞ (Coenen et al. 2019) –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª attention: –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ head, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ ‚Äúherself‚Äù —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ ‚ÄúMary‚Äù (–ø–æ–∫–∞–∑—ã–≤–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç–∞). Coreference-heads –æ–±—ã—á–Ω–æ –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤ —Å—Ä–µ–¥–Ω–∏—Ö —Å–ª–æ—è—Ö, –≥–¥–µ —É–∂–µ –µ—Å—Ç—å –Ω–µ–∫–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ, –Ω–æ –µ—â—ë –º–æ–¥–µ–ª—å –Ω–µ —Ä–µ—à–∏–ª–∞ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ —á—Ç–æ —Å–∫–∞–∑–∞—Ç—å. –û–Ω–∏ –ø–æ–º–æ–≥–∞—é—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
> 
> - **Syntactic heads (—Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –≥–æ–ª–æ–≤—ã):** –û–Ω–∏ –≤—ã—è–≤–ª—è—é—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è: –Ω–∞–ø—Ä–∏–º–µ—Ä, *Subject-Verb head* ‚Äì —Å–≤—è–∑—ã–≤–∞–µ—Ç –ø–æ–¥–ª–µ–∂–∞—â–µ–µ —Å —Å–∫–∞–∑—É–µ–º—ã–º (–æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –≤ —è–∑—ã–∫–∞—Ö —Å –ø–æ—Ä—è–¥–∫–æ–º —Å–ª–æ–≤), *Object head* ‚Äì —Å–≤—è–∑—ã–≤–∞–µ—Ç –≥–ª–∞–≥–æ–ª —Å –ø—Ä—è–º—ã–º –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º, *Clause head* ‚Äì –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏ —Å–ª–æ–∂–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. Voita et al. (2019) –≤ –∞–Ω–∞–ª–∏–∑–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∞–∫–∏—Ö: –æ–¥–∏–Ω head consistently attended from verb to its subject, –¥—Ä—É–≥–æ–π ‚Äì –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫ –Ω–∞—á–∞–ª—É —Å–ª–µ–¥—É—é—â–µ–≥–æ (—Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É—è –≥—Ä–∞–Ω–∏—Ü—É)„Äê44‚Ä†L59-L67„Äë„Äê44‚Ä†L69-L73„Äë. –≠—Ç–∏ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞—é—Ç **—Å—Ç—Ä—É–∫—Ç—É—Ä–µ**: —É –Ω–∏—Ö attention map –æ–±—Ä–∞–∑—É–µ—Ç –¥–µ—Ä–µ–≤–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. –ù–∞–ø—Ä–∏–º–µ—Ä, head –º–æ–∂–µ—Ç —Å –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Ç—è–Ω—É—Ç—å—Å—è –∫ –µ–≥–æ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º—É —Å–ª–æ–≤—É –≤ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–º –¥–µ—Ä–µ–≤–µ (–µ—Å—Ç—å —Ä–∞–±–æ—Ç—ã, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–µ, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ heads –ø–æ—á—Ç–∏ 1-–≤-1 –ø–∞—Ä—Å—è—Ç –¥–µ–ø–µ–Ω–¥–µ–Ω—Å–∏). –û–Ω–∏ —á–∞—Å—Ç–æ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ (—Å–ª–æ–∏ 1‚Äì4), –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å ‚Äì –±–∞–∑–∏—Å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è.
> 
> - **Lexical heads (–ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ / –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ –≥–æ–ª–æ–≤—ã):** –ö —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –≥–æ–ª–æ–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–≥–∏—Ä—É—é—Ç –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, *List enumeration head*: –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π ‚Äú1.‚Äù, ‚Äú2.‚Äù, ‚Äú3.‚Äù ‚Äì —Ç–∞–∫–∞—è –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç —Å–º–æ—Ç—Ä–µ—Ç—å –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –Ω–æ–º–µ—Ä–∞ —Å–ø–∏—Å–∫–∞ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –Ω–æ–º–µ—Ä, —É–¥–µ—Ä–∂–∏–≤–∞—è —Ñ–æ—Ä–º–∞—Ç —Å–ø–∏—Å–∫–∞„Äê44‚Ä†L15-L23„Äë. *Verbs tense head*: —Å–≤—è–∑—ã–≤–∞–µ—Ç –≥–ª–∞–≥–æ–ª—ã –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (–º.–±. –≤–∞–∂–Ω–æ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω). *Punctuation head*: –æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∂–¥–∞—è –∑–∞–ø—è—Ç–∞—è —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â—É—é –∑–∞–ø—è—Ç—É—é (–∏–ª–∏ –Ω–∞ –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è). *Bracket head*: –∫–∞–∫ –æ—Ç–º–µ—á–∞–ª–æ—Å—å, —Å–≤—è–∑—ã–≤–∞–µ—Ç ‚Äú(‚Äù —Å ‚Äú)‚Äù –∏–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–µ –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ –∫–∞–≤—ã—á–∫–∏. *Case head*: –º–æ–∂–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã (–≥–¥–µ –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è). –≠—Ç–∏ –≥–æ–ª–æ–≤—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å –æ—á–µ–Ω—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Ç–µ–∫—Å—Ç–∞ ‚Äì —á–∞—Å—Ç–æ –∏—Ö –Ω–∞–∑—ã–≤–∞—é—Ç —Ç–∞–∫–∂–µ *–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏* –∏–ª–∏ *–¥–µ–Ω–æ—Ç–∏—Ä—É—é—â–∏–º–∏ —à–∞–±–ª–æ–Ω*. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≥–æ–ª–æ–≤–∞ GPT-2, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Ç–æ—á–∫—É) –≤—Å–µ–≥–¥–∞ —Å–º–æ—Ç—Ä–∏—Ç –≤ –Ω–∞—á–∞–ª–æ —Ç–æ–≥–æ –∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è„Äê44‚Ä†L55-L63„Äë. –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ ‚Äúnull-attention‚Äù (–≤–Ω–∏–º–∞–Ω–∏–µ –≤ –Ω–∏–∫—É–¥–∞) ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª—É–∂–∏—Ç –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü. Lexical heads —á–∞—â–µ –≤—Å–µ–≥–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å–∞–º—ã—Ö –ø–µ—Ä–≤—ã—Ö —Å–ª–æ—è—Ö –∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç —Ä–æ–ª—å ‚Äú–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏‚Äù ‚Äì –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–∞, –Ω–µ —Å—Ç–æ–ª—å–∫–æ –Ω–∞ —Å–º—ã—Å–ª.
> 
> **–ü—Ä–∏–º–µ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π –≤ GPT-2 small (–ø–æ –¥–∞–Ω–Ω—ã–º Wang et al., 2022 ‚ÄúIOI‚Äù):** 
> - Layer 0-1: –º–Ω–æ–≥–æ –≥–æ–ª–æ–≤, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∏—Ö—Å—è –Ω–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—è—Ö (the, a) ‚Äì –æ–Ω–∏ –ø–æ–º–æ–≥–∞—é—Ç —Ä–∞–∑–¥–µ–ª—è—Ç—å —Ñ—Ä–∞–∑—ã. 
> - Layer 2-4: –ø–æ—è–≤–ª—è—é—Ç—Å—è heads, –ª–æ–≤—è—â–∏–µ –ø–æ–≤—Ç–æ—Ä —Å–ª–æ–≤ (duplicate token), Heads, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π head –≤—Å–µ–≥–¥–∞ —Å –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä—ã–≥–∞–µ—Ç –Ω–∞ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–≤—è–∑—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º). 
> - Layer 5-6: induction heads (–≤ GPT-2 small –ø–æ—è–≤–∏–ª–∏—Å—å –Ω–∞ —Å–ª–æ–µ 5-6) ‚Äì –æ–Ω–∏ –ø–æ–≤—ã—Å–∏–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª—å–∫–∏ –¥–µ–ª–∞—Ç—å –∑–∞–¥–∞—á–∏ —Ç–∏–ø–∞ ‚Äú–ø—Ä–æ–¥–æ–ª–∂–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏‚Äù. 
> - Layer 7-8: name mover heads ‚Äì —É–∂–µ –±–ª–∏–∂–µ –∫ –∫–æ–Ω—Ü—É, –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –∫–∞–∫–æ–µ –∏–º—è –≤—Å—Ç–∞–≤–∏—Ç—å; negative name mover heads ‚Äì –ø–æ–¥–∞–≤–ª—è—é—Ç –ª–∏—à–Ω–∏–µ –∏–º–µ–Ω–∞. 
> - Layer 9-11: high-level heads, —Å–æ–±–∏—Ä–∞—é—â–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–º–æ–≥—É—Ç –±—ã—Ç—å heads, –∫–æ—Ç–æ—Ä—ã–µ —Å–º–æ—Ç—Ä—è—Ç –Ω–∞ –∑–Ω–∞–∫ –≤–æ–ø—Ä–æ—Å–∞, —Ä–µ—à–∞—è, —á—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å, etc.).
> 
> **–ú–µ—Ç–æ–¥—ã –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏:** –î–ª—è –æ—Ç–Ω–µ—Å–µ–Ω–∏—è –≥–æ–ª–æ–≤—ã –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–µ–ª–∞—é—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã: –¥–∞—é—Ç –º–æ–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ —Å–º–æ—Ç—Ä—è—Ç, –∫–∞–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –µ—ë –≤–Ω–∏–º–∞–Ω–∏–µ. –ï—Å–ª–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≥–æ–ª–æ–≤–∞ —è–≤–Ω–æ –æ—Ç–¥–∞–µ—Ç >90% –≤–µ—Å–∞ –æ–¥–Ω–æ–º—É –∏–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤ ‚Äì –≤–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–æ duplicate/copy head. –ï—Å–ª–∏ –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –∏–º–µ–Ω–∞—Ö –≥–æ–ª–æ–≤–∞ –≤—Å–µ–≥–¥–∞ –æ—Ç –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –∏–¥—ë—Ç –∫ –Ω—É–∂–Ω–æ–º—É –∏–º–µ–Ω–∏ ‚Äì coreference. –¢–∞–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –∫–∞–∫ *attention rollout* –∏ *integrated gradients –¥–ª—è –≤–Ω–∏–º–∞–Ω–∏—è*, –ø–æ–º–æ–≥–∞—é—Ç –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.
> 
> **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –æ –≥–æ–ª–æ–≤–∞—Ö:** –£–∂–µ –µ—Å—Ç—å –ø–æ–¥—Ö–æ–¥—ã, –≥–¥–µ –∑–Ω–∞—è –æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, *–æ—Ç–±–∏—Ä–∞—é—Ç –∏–ª–∏ –¥–æ–æ–±—É—á–∞—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≥–æ–ª–æ–≤—ã* –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–¥–∞–≤–Ω—è—è —Ä–∞–±–æ—Ç–∞ (Ho et al., 2023) fine-tune‚Äô–∏–ª–∞ —Ç–æ–ª—å–∫–æ 64 –∫–ª—é—á–µ–≤—ã—Ö –≥–æ–ª–æ–≤—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Å—Ç–∞–≤–∏–≤ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–º–∏„Äê3‚Ä†L317-L326„Äë. –≠—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –∑–Ω–∞–ª–∏ –∫–∞–∫–∏–µ –≥–æ–ª–æ–≤—ã –≤–∞–∂–Ω—ã –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ (source head, positional head –∏ —Ç.–¥.). –ï—Å—Ç—å –ø–æ–ø—ã—Ç–∫–∏ *–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å ‚Äú–æ–ø–∞—Å–Ω—ã–µ‚Äù –≥–æ–ª–æ–≤—ã* (—Å–º. —Ä–∞–∑–¥–µ–ª 16) ‚Äì –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –≥–æ–ª–æ–≤—ã —Å–ª–∏—à–∫–æ–º –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ —Ç–æ–∫—Å–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∏—Ö –º–æ–∂–Ω–æ –æ–±–Ω—É–ª–∏—Ç—å. –í CLIP (–º–æ–¥–µ–ª—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è) –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ ‚Äúgender bias head‚Äù ‚Äì –∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ *head pruning* —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å bias„Äê4‚Ä†L43-L47„Äë. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≥–æ–ª–æ–≤ ‚Äì –ø–µ—Ä–≤—ã–π —à–∞–≥ –∫ —Ç–æ—á–µ—á–Ω–æ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –º–æ–¥–µ–ª—å—é.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–æ–ª–æ–≤ ‚Äì —Å–ª–æ–≤–Ω–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–µ. –ö–∞–∂–¥–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Äì –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è: –∫–æ–ø–∏—Ä–æ–≤—â–∏–∫, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å—Ç, —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç, –ª–µ–∫—Å–∏–∫–æ–≥—Ä–∞—Ñ, —à–∞–±–ª–æ–Ω–∏—Å—Ç. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ —ç—Ç–∏ ‚Äú–ø—Ä–æ—Ñ–µ—Å—Å–∏–∏‚Äù –≤–æ–∑–Ω–∏–∫–∞—é—Ç —Å—Ö–æ–¥–Ω–æ –≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Å–ª–æ–≤–Ω–æ –µ—Å—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å—É–±–∑–∞–¥–∞—á–∏ –≤ —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä—ã–µ –ª—é–±–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —Ä–µ—à–∏—Ç—å. –≠—Ç–æ –Ω–∞–≤–æ–¥–∏—Ç –Ω–∞ –º—ã—Å–ª—å –æ *—ç–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–±–∏–µ–Ω–∏—è—Ö* ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–∏ –¥–∞–ª—å–Ω–µ–π—à–µ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –ø–æ—è–≤—è—Ç—Å—è –µ—â—ë –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ ‚Äú–ø—Ä–æ—Ñ–µ—Å—Å–∏–∏‚Äù (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏—á–µ—Å–∫–∞—è –≥–æ–ª–æ–≤–∞, –º–æ—Ä–∞–ª—å–Ω–æ-—ç—Ç–∏—á–µ—Å–∫–∞—è –≥–æ–ª–æ–≤–∞?). –ü–æ–∫–∞ –∂–µ –º—ã –≤–∏–¥–∏–º –¥–æ–≤–æ–ª—å–Ω–æ —è–∑—ã–∫–æ–≤—ã–µ –∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ä–æ–ª–∏.
> 
> **–°—Å—ã–ª–∫–∏:** Vig 2019 (lexical pattern heads)„Äê44‚Ä†L15-L23„Äë; Olsson 2022 (induction)„Äê35‚Ä†L58-L66„Äë; Wang et al. 2022 (IOI circuit: name mover, duplicate, etc.)„Äê37‚Ä†L355-L363„Äë„Äê37‚Ä†L367-L375„Äë.
> 
> ## 9. **–ú–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞ –≤–Ω–∏–º–∞–Ω–∏—è: attention maps, probing, attention flow, causal tracing** 
> –ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì –∞–∫—Ç–∏–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å, –ø–æ—è–≤–∏–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–∏–∫:
> 
> - **Attention maps (–∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è):** –≠—Ç–æ –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä—è–º–æ–ª–∏–Ω–µ–π–Ω—ã–π —Å–ø–æ—Å–æ–± ‚Äì –≤–∑—è—Ç—å –º–∞—Ç—Ä–∏—Ü—ã attention \(\alpha_{ij}\) (–æ–±—ã—á–Ω–æ —É—Å—Ä–µ–¥–Ω–∏–≤ –ø–æ –≥–æ–ª–æ–≤–∞–º –∏–ª–∏ –≤—ã–±—Ä–∞–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –≥–æ–ª–æ–≤—É) –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö. –¢–∏–ø–∏—á–Ω–æ, —Å—Ç—Ä–æ—è—Ç —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É —Ä–∞–∑–º–µ—Ä–æ–º [sequence_length √ó sequence_length], –≥–¥–µ —è—Ä–∫–æ—Å—Ç—å –æ—Ç—Ä–∞–∂–∞–µ—Ç –≤–µ–ª–∏—á–∏–Ω—É –≤–Ω–∏–º–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ \(i\) –∫ \(j\). –¢–∞–∫–∏–µ –∫–∞—Ä—Ç—ã –º–æ–∂–Ω–æ —Ä–∏—Å–æ–≤–∞—Ç—å –ø–æ —Å–ª–æ—è–º –∏ –≥–æ–ª–æ–≤–∞–º. *–ü–ª—é—Å—ã:* –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ –ø–æ–Ω—è—Ç–Ω–æ, –º–æ–∂–Ω–æ –≥–ª–∞–∑–∞–º–∏ —É–≤–∏–¥–µ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏, —Å—Ç–æ–ª–±—Ü—ã –Ω–∞ SEP-—Ç–æ–∫–µ–Ω–µ –∏ —Ç.–¥.). *–ú–∏–Ω—É—Å—ã:* –≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ç—ã—Å—è—á–∏ –≥–æ–ª–æ–≤ ‚Äì –≤—Å–µ –Ω–µ –ø–æ—Å–º–æ—Ç—Ä–∏—à—å; –∫—Ä–æ–º–µ —Ç–æ–≥–æ, –±–æ–ª—å—à–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ \(\alpha_{ij}\) –Ω–µ –≤—Å–µ–≥–¥–∞ –∑–Ω–∞—á–∏—Ç –≤–∞–∂–Ω–æ—Å—Ç—å (—Å–º. —Ä–∞–∑–¥–µ–ª 19). –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –¥–∞–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–Ω—Å–∞–π—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, Vig & Be (2019) —Å–¥–µ–ª–∞–ª–∏ BertViz ‚Äì –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ attention –¥–ª—è BERT„Äê44‚Ä†L55-L63„Äë„Äê44‚Ä†L187-L195„Äë. –û–Ω –ø–æ–∑–≤–æ–ª–∏–ª —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ –≥–æ–ª–æ–≤–∞–º, –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –Ω–∞—Ö–æ–¥–∏—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–Ω–∞–ø—Ä., –º–µ–∂—Ñ—Ä–∞–∑–æ–≤—ã–µ —Å–≤—è–∑–∏). Attention maps –±—ã–ª–∏ –ø–µ—Ä–≤–æ–π —É–ª–∏–∫–æ–π –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é induction heads: –∑–∞–º–µ—Ç–∏–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π ‚Äú–ø–æ–ª–æ—Å–∞—Ç—ã–π‚Äù —É–∑–æ—Ä –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏ –≤–¥–∞–ª–∏ –æ—Ç –≥–ª–∞–≤–Ω–æ–π –¥–∏–∞–≥–æ–Ω–∞–ª–∏, —á—Ç–æ –Ω–∞–≤–µ–ª–æ –Ω–∞ –≥–∏–ø–æ—Ç–µ–∑—É –æ–± –∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–µ„Äê8‚Ä†L5-L8„Äë. –¢–∞–∫–∂–µ –∫–∞—Ä—Ç—ã —è–≤–Ω–æ –ø–æ–∫–∞–∑–∞–ª–∏ coreference ‚Äì –ª–∏–Ω–∏—é –æ—Ç –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –∫ –∏–º–µ–Ω–∏. *–í—ã–≤–æ–¥:* –∫–∞—Ä—Ç—ã ‚Äì –±–∞–∑–æ–≤—ã–π, –Ω–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, —á–∞—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –∞–Ω–∞–ª–∏–∑–∞.
> 
> - **Probing –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–ø—Ä–æ–±–∏–Ω–≥):** –ó–¥–µ—Å—å –∏–¥–µ—è ‚Äì –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ—Å—É—Ç –ª–∏ –≤—ã—Ö–æ–¥—ã –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤ (–∏–ª–∏ –≤–æ–æ–±—â–µ —Å–ª–æ—ë–≤) –∫–∞–∫—É—é-—Ç–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ-–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ë–µ—Ä—É—Ç –≤—ã—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≥–æ–ª–æ–≤—ã \(h_{i}^{(head)}\) (—Å–∫–∞–∂–µ–º, –ø–æ—Å–ª–µ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è OV, –∏–ª–∏ –∏–Ω–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è) –∏ –æ–±—É—á–∞—é—Ç –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª—å–∫–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏) –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∑–∞–¥–∞–Ω–Ω—ã–π –ª–µ–π–±–ª: —á–∞—Å—Ç—å —Ä–µ—á–∏ —Å–ª–æ–≤–∞, —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫—É—é —Ä–æ–ª—å, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –∫–æ—Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∏ —Ç.–¥. –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç, –∑–Ω–∞—á–∏—Ç –≥–æ–ª–æ–≤–∞ –∫–æ–¥–∏—Ä—É–µ—Ç —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –¢–∞–∫ –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã BERT —Ö–æ—Ä–æ—à–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏„Äê17‚Ä†L1-L8„Äë„Äê44‚Ä†L59-L67„Äë. –ü—Ä–æ–±–∏–Ω–≥ –ø–æ–º–æ–≥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å: –Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú–≥–æ–ª–æ–≤–∞ 8-3 —Ö—Ä–∞–Ω–∏—Ç 80% –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–æ–¥–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è‚Äù –∏ —Ç.–ø. *Caveat:* –≤—ã—Å–æ–∫–∏–π performance –ø—Ä–æ–±–µ—Ä–∞ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å *–∏—Å–ø–æ–ª—å–∑—É–µ—Ç* —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é ‚Äì –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—Å—Ç–æ –∫–æ—Ä–µ–ª–ª–∏—Ä—É–µ—Ç. –ù–æ –æ–±—ã—á–Ω–æ, –µ—Å–ª–∏ –≥–æ–ª–æ–≤–∞ —è–≤–Ω–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è, probing —ç—Ç–æ –≤—ã—è–≤–ª—è–µ—Ç. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è: probing attention weights —Å–∞–º–∏—Ö (—Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –∏—Ö —Å –∫–∞–∫–∏–º-—Ç–æ –∑–æ–ª–æ—Ç—ã–º alignment). –ü—Ä–æ–±–∏–Ω–≥-–∞–Ω–∞–ª–∏–∑–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–ª–µ—á—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏ (XGB, MLP) –¥–ª—è –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
> 
> - **Attention flow / Attribution —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞–Ω–∏–µ:** –¢–µ—Ö–Ω–∏–∫–∏, –ø—ã—Ç–∞—é—â–∏–µ—Å—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∫–∞–∫ –≥—Ä–∞—Ñ –ø–æ—Ç–æ–∫–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –∞–ª–≥–æ—Ä–∏—Ç–º **Attention Rollout** (Abnar & Zuidema, 2020) ‚Äì —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç, –∫–∞–∫ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å–ª–æ–∏, –∏ —É—Å—Ä–µ–¥–Ω—è–µ—Ç –≤–ª–∏—è–Ω–∏—è, –ø–æ–ª—É—á–∞—è –∏—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –≤—Ö–æ–¥–æ–≤ –Ω–∞ –≤—ã—Ö–æ–¥—ã. –û–Ω –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ –≥–ª–æ–±–∞–ª—å–Ω–æ, –∫–∞–∫–æ–π –≤—Ö–æ–¥ –ø–æ–≤–ª–∏—è–ª –Ω–∞ —á—Ç–æ, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ—ë–≤ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–≥—É—Ç –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å —Ñ–æ–∫—É—Å„Äê44‚Ä†L187-L195„Äë. –î—Ä—É–≥–∞—è –∏–¥–µ—è ‚Äì **Attention Flow** (Brunner et al. 2019): —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –≥—Ä–∞—Ñ, –≥–¥–µ —Ç–æ–∫–µ–Ω—ã ‚Äì —É–∑–ª—ã, —Ä–µ–±—Ä–æ i->j –≤–µ—Å–∏—Ç—Å—è –∫–∞–∫ attention. –ü–æ—Ç–æ–º –∏—â—É—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∞–º—ã–µ –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏. –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∞–Ω–∏–µ —Ç–æ–∫–∞ —á–µ—Ä–µ–∑ —Å–µ—Ç—å. –¶–µ–ª—å ‚Äì –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è: –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω A –≤–ª–∏—è–µ—Ç –Ω–∞ B, –∞ B –Ω–∞ C, —Ç–æ A –∫–æ—Å–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ C. Attention flow –º–æ–∂–µ—Ç –≤—ã—è–≤–∏—Ç—å —Ç–∞–∫–∏–µ –∫–æ—Å–≤–µ–Ω–Ω—ã–µ –≤–ª–∏—è–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã –∏–∑ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç—ã. *–ü—Ä–∏–º–µ—Ä:* –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ ‚ÄúWho did Alice see?‚Äù attention flow –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ ‚ÄúAlice‚Äù –∫–æ—Å–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ ‚ÄúWho‚Äù —á–µ—Ä–µ–∑ –≥–æ–ª–æ–≤—É, —Å–≤—è–∑—ã–≤–∞—é—â—É—é —Å—É–±—ä–µ–∫—Ç –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ.
> 
> - **Causal tracing (–ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥–æ–≤—ã–π —Ç—Ä–∞—Å—Å–∏–Ω–≥):** –ë–æ–ª–µ–µ –Ω–µ–¥–∞–≤–Ω–∏–π –º–µ—Ç–æ–¥ –æ—Ç –∫–æ–º–∞–Ω–¥—ã Redwood (2022, Meng et al.) –∏ –¥—Ä. ‚Äì *Activation Patching* –∏–ª–∏ *Causal Mediation Analysis.* –ò–¥–µ—è: –≤–∑—è—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä (—Ç–µ–∫—Å—Ç), –ø—Ä–æ–≥–Ω–∞—Ç—å —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –∑–∞–ø–∏—Å–∞—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–≤–∫–ª—é—á–∞—è –≤–Ω—É—Ç—Ä–∏ –≥–æ–ª–æ–≤). –ó–∞—Ç–µ–º –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è ‚Äú–ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å‚Äù –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏—é –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç –¥—Ä—É–≥–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞) –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—Å—è –≤—ã—Ö–æ–¥ (–ª–æ–≥–∏—Ç—ã –º–æ–¥–µ–ª–∏). –ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–º–µ–Ω–µ —á–∞—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –∑–Ω–∞—á–∏—Ç —ç—Ç–∞ –≥–æ–ª–æ–≤–∞ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ç—É —á–∞—Å—Ç—å. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ IOI (–≤—ã–±–æ—Ä –∏–º–µ–Ω–∏) —Å–¥–µ–ª–∞–ª–∏: –ø–æ–¥–∞–≤ –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏–º–µ–Ω, –∏ –ø–∞—Ç—á–∏–ª–∏ –≤—ã—Ö–æ–¥—ã –≥–æ–ª–æ–≤ ‚Äì –Ω–∞—à–ª–∏, —á—Ç–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã (Name Mover) –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –ª–æ–≥–∏—Ç—ã –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏„Äê36‚Ä†L21-L29„Äë„Äê36‚Ä†L31-L39„Äë. –≠—Ç–æ –∏ –µ—Å—Ç—å *causal tracing:* –º—ã –≤–º–µ—à–∏–≤–∞–µ–º—Å—è –≤ –º–æ–¥–µ–ª—å –∏ –Ω–∞–±–ª—é–¥–∞–µ–º –ø—Ä–∏—á–∏–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç. –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ —Å–ª–æ–∂–Ω–µ–µ, –Ω–æ –æ–Ω–∞ –º–æ–∂–µ—Ç –≤—ã–¥–µ–ª–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏: ‚Äú–≥–æ–ª–æ–≤–∞ 10.7 –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–æ–º—É, —á—Ç–æ —Ç–æ–∫–µ–Ω X –ø–æ–ª—É—á–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å Y‚Äù. 
> 
> - **Attention ablation (–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è):** –≠—Ç–æ —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç–∏: –æ–±–Ω—É–ª–µ–Ω–∏–µ –∏–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ –≥–æ–ª–æ–≤—ã —Ü–µ–ª–∏–∫–æ–º, –ª–∏–±–æ –∑–∞–Ω—É–ª–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö attention –≤–µ—Å–æ–≤, –∏ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –º–µ—Ç—Ä–∏–∫—É. Michel et al. (2019) —Ç–∞–∫ –∏ –¥–µ–ª–∞–ª–∏ ‚Äì –æ—Ç–∫–ª—é—á–∞–ª–∏ –ø–æ –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤–µ –∏ —Å–º–æ—Ç—Ä–µ–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ BLEU –≤ –ø–µ—Ä–µ–≤–æ–¥–µ„Äê20‚Ä†L130-L139„Äë. –í –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏, –∞–±–ª–∏—Ä—É—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å–≤—è–∑—å, –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–∞ –ª–∏ –æ–Ω–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –æ–±–Ω—É–ª–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ coreference-–≥–æ–ª–æ–≤—ã, —Ç–æ –º–æ–¥–µ–ª—å –Ω–∞—á–Ω—ë—Ç –ø—É—Ç–∞—Ç—å, –∫ –∫–æ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ (—á—Ç–æ –ø—Ä–æ—è–≤–∏—Ç—Å—è –≤ —Ö—É–¥—à–µ–º –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã). Ablation –¥–∞–ª –ø–æ–Ω—è—Ç–∏–µ ‚Äú–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏‚Äù –≥–æ–ª–æ–≤—ã: –µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑–∫–æ –ø–∞–¥–∞–µ—Ç, –≥–æ–ª–æ–≤–∞ –±—ã–ª–∞ –≤–∞–∂–Ω–∞. –ù–æ –∞–∫–∫—É—Ä–∞—Ç–Ω–µ–µ: –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –¥—Ä—É–≥–∏–µ –≥–æ–ª–æ–≤—ã, –ø–æ—ç—Ç–æ–º—É –µ–¥–∏–Ω–∏—á–Ω–∞—è –∞–±–ª—è—Ü–∏—è ‚Äì –Ω–µ –≤—Å–µ–≥–¥–∞ —Ä–µ—à–∞—é—â–∏–π —Ç–µ—Å—Ç (–Ω–æ –≤–æ—Ç —É–¥–∞–ª–µ–Ω–∏–µ *–≥—Ä—É–ø–ø—ã* –≥–æ–ª–æ–≤ ‚Äì —Å–∏–ª—å–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç).
> 
> - **–ú–µ—Ç–æ–¥—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è:** –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≥–æ–ª–æ–≤ –ø–æ –∏—Ö —à–∞–±–ª–æ–Ω–∞–º –≤–Ω–∏–º–∞–Ω–∏—è –∏–ª–∏ –ø–æ –≤—ã—Ö–æ–¥–Ω—ã–º –≤–µ—Å–∞–º. Survey Zheng 2023 —É–ø–æ–º–∏–Ω–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é heads –ø–æ —Ñ—É–Ω–∫—Ü–∏—è–º —á–µ—Ä–µ–∑ attention distance (–Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–∞–ª–µ–∫–æ, –≤ —Å—Ä–µ–¥–Ω–µ–º, —É—Ö–æ–¥—è—Ç —Å–≤—è–∑–∏ —É –≥–æ–ª–æ–≤—ã) ‚Äì –≤—ã—è–≤–∏–ª–∏—Å—å –≥—Ä—É–ø–ø—ã: ‚Äú–º–µ—Å—Ç–Ω—ã–µ –≥–æ–ª–æ–≤—ã‚Äù, ‚Äú–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã‚Äù, ‚Äú—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥–æ–ª–æ–≤—ã‚Äù„Äê2‚Ä†L7-L15„Äë„Äê2‚Ä†L43-L50„Äë. –¢–∞–∫–∂–µ –µ—Å—Ç—å –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å PCA –∏–ª–∏ t-SNE –∫ –º–∞—Ç—Ä–∏—Ü–∞–º –≤–Ω–∏–º–∞–Ω–∏—è, —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫ –æ–Ω–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã. –≠—Ç–æ –º–µ–Ω–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–º–æ–≥–∞–µ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–Ω—è—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ 64 –≥–æ–ª–æ–≤ —Å–∏–ª—å–Ω–æ —Ä–∞–∑–Ω—ã–µ –∏–ª–∏ –µ—Å—Ç—å –≥—Ä—É–ø–ø—ã –±–ª–∏–∑–Ω–µ—Ü–æ–≤.
> 
> **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤:**
> 
> - **Attention maps** —É–¥–æ–±–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤ –∞–Ω–∞–ª–∏–∑–µ bias: –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞—à–ª–∏, —á—Ç–æ –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ BERT –ø—Ä–∏ —Å–ª–æ–≤–µ ‚Äúdoctor‚Äù —á–∞—â–µ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Å–ª–æ–≤–æ ‚Äúhe‚Äù, –∞ –ø—Ä–∏ ‚Äúnurse‚Äù ‚Äì –Ω–∞ ‚Äúshe‚Äù„Äê17‚Ä†L1-L9„Äë, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≥–µ–Ω–¥–µ—Ä–Ω—ã–π —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø. 
> - **Causal tracing** –ø—Ä–∏–º–µ–Ω—è–ª–∏, —á—Ç–æ–±—ã –ø—Ä–∞–≤–∏—Ç—å –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: Meng et al. (ROME, 2022) –≤—ã—è–≤–ª—è–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ MLP –Ω–µ–π—Ä–æ–Ω—ã, –Ω–æ –¥–ª—è attention —Ç–æ–∂–µ –º–æ–∂–Ω–æ, —Å–∫–∞–∂–µ–º, —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑—å head-> –ª–æ–≥–∏—Ç (—ç—Ç–æ –∫–∞–∫ —Å–ø–æ—Å–æ–± fine-tune). 
> - **Probing** –¥–∞—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ, useful –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∏–ª–∏ —Å–ª–æ—ë–≤.
> 
> **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:** (1) –ú–Ω–æ–≥–∏–µ –º–µ—Ç–æ–¥—ã —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω—ã —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–Ω–∏–º–∞–Ω–∏–∏, –∏–≥–Ω–æ—Ä–∏—Ä—É—è FFN, —á—Ç–æ –¥–∞—ë—Ç –Ω–µ–ø–æ–ª–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É (—Ä–∞–∑–¥–µ–ª 19 –ø—Ä–æ–¥–æ–ª–∂–∏—Ç). (2) Attention weights –Ω–µ—Ä–µ–¥–∫–æ —Ç—Ä—É–¥–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ (–∫–æ–≥–¥–∞ –Ω–µ—Ç —è—Ä–∫–æ –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã—Ö –ø–∏–∫–æ–≤). (3) –öausal methods —Ç—Ä–µ–±—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ‚Äì –ø–µ—Ä–µ–±—Ä–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ —Å–æ—Ç–µ–Ω –≥–æ–ª–æ–≤ ‚Äì –Ω–æ —Ç—É—Ç –ø–æ–º–æ–≥–∞–µ—Ç —á—Ç–æ –º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–∏–µ, –Ω–æ –∏ –º–æ—â–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç—ë—Ç.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** –ê–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì —ç—Ç–æ –Ω–∞—à–µ ‚Äú–æ–∫–Ω–æ‚Äù –≤ –º—ã—Å–ª–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –º–æ–¥–µ–ª–∏. –ú—ã –∫–∞–∫ –Ω–µ–π—Ä–æ–ø—Å–∏—Ö–æ–ª–æ–≥–∏, —Å–º–æ—Ç—Ä–∏–º –Ω–∞ ‚Äúheatmaps‚Äù –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –æ—Ç–∫–ª—é—á–∞–µ–º —á–∞—Å—Ç–∏ ‚Äú–º–æ–∑–≥–∞‚Äù (–∞–±–ª—è—Ü–∏—è), –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ ‚Äú–Ω–∞–º–µ—Ä–µ–Ω–∏—è‚Äù (probing). –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –¥–æ–≤–µ—Ä–∏—è –∫ –º–æ–¥–µ–ª—è–º –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–∏–ª–ª–∏–∞—Ä–¥—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Ç–∞–∫–∏–µ –∞–Ω–∞–ª–∏–∑—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –¥–æ–≤–æ–ª—å–Ω–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã ‚Äì –≥–æ–ª–æ–≤—ã ‚Äì –∏ –¥–∞—é—Ç –Ω–∞–º —è–∑—ã–∫ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞—Ö (–∏–Ω–¥—É–∫—Ü–∏—è, –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏ —Ç.–ø.). –≠—Ç–æ –≤–Ω—É—à–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–º, —á—Ç–æ –¥–∞–∂–µ –µ—â—ë –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.
> 
> **–°—Å—ã–ª–∫–∏:** BertViz and attention patterns„Äê44‚Ä†L55-L63„Äë„Äê44‚Ä†L187-L195„Äë; Jain & Wallace 2019 (–≤–Ω–∏–º–∞–Ω–∏–µ vs –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)„Äê57‚Ä†L53-L62„Äë; Neel Nanda‚Äôs work on causal tracing (IOI)„Äê36‚Ä†L21-L29„Äë.
> 
> ## 10. **–ß—Ç–æ —Ç–∞–∫–æ–µ attention collapse / redundancy ‚Äì –∫–∞–∫ –≥–æ–ª–æ–≤—ã —Ç–µ—Ä—è—é—Ç —Å–º—ã—Å–ª** 
> **Attention head collapse (–∫–æ–ª–ª–∞–ø—Å –≤–Ω–∏–º–∞–Ω–∏—è):** –≠—Ç–æ —è–≤–ª–µ–Ω–∏–µ, –∫–æ–≥–¥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω–æ –ø–æ—Ö–æ–∂–∏–º–∏ –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞ –∏–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º–∏. –ì–æ–≤–æ—Ä—è—Ç, —á—Ç–æ –≥–æ–ª–æ–≤—ã ‚Äú–æ–±—Ä—É—à–∏–≤–∞—é—Ç—Å—è‚Äù –≤ –æ–¥–Ω—É, –µ—Å–ª–∏ –∏—Ö attention weights —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã (—Ç.–µ. –≤—ã–ø–æ–ª–Ω—è—é—Ç —Ç—É –∂–µ —Ñ—É–Ω–∫—Ü–∏—é), –ª–∏–±–æ ‚Äú—É–º–∏—Ä–∞—é—Ç‚Äù, –µ—Å–ª–∏ –æ–Ω–∏ –≤—Å–µ–≥–¥–∞ –≤—ã–¥–∞—é—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–Ω–∏–∫–∞–∫–æ–π –ø–æ–ª–µ–∑–Ω–æ–π —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏). Collapse –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –ø—Ä–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–º —á–∏—Å–ª–µ –≥–æ–ª–æ–≤ –∏–ª–∏ –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–µ–π —Ä–µ–≥—É–ª—è—Ä–∏—Ü–∏–∏: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º—É–º, –≥–¥–µ —á–∞—Å—Ç—å –≥–æ–ª–æ–≤ –ø—Ä–æ—Å—Ç–æ –¥—É–±–ª–∏—Ä—É–µ—Ç –¥—Ä—É–≥–∏–µ, –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è. –í –ø—Ä–µ–¥–µ–ª–µ, –µ—Å–ª–∏ –±—ã –ø—Ä–æ–∏–∑–æ—à—ë–ª –ø–æ–ª–Ω—ã–π collapse, multi-head attention –±—ã —Å–≤—ë–ª—Å—è –∫ single-head –ø–æ –ø–æ–≤–µ–¥–µ–Ω–∏—é.
> 
> **–ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ–ª–ª–∞–ø—Å–∞:** (1) *–ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è:* –Ω–∞–ø—Ä., 2‚Äì3 –≥–æ–ª–æ–≤—ã –≤ –æ–¥–Ω–æ–º —Å–ª–æ–µ –≤—Å–µ–≥–¥–∞ –∏–º–µ—é—Ç –ø–æ—á—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π pattern (–º–æ–∂–Ω–æ –∏–∑–º–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–∞—Ç—Ä–∏—Ü –≤–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –≥–æ–ª–æ–≤–∞–º–∏). Michel et al. –∑–∞–º–µ—Ç–∏–ª–∏ –≤—ã—Å–æ–∫—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —É –º–Ω–æ–≥–∏—Ö –≥–æ–ª–æ–≤ BERT ‚Äì —Ç–æ –µ—Å—Ç—å –æ–Ω–∏ –Ω–µ—É—á–µ—Ç–Ω–æ –¥—É–±–ª–∏—Ä—É—é—Ç —Ñ—É–Ω–∫—Ü–∏—é„Äê20‚Ä†L139-L147„Äë. (2) *–ü–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ:* –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –º–æ–≥—É—Ç –≤—Å–µ–≥–¥–∞ —Å–º–æ—Ç—Ä–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–µ–±—è (–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω). –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ –≤ –º–µ—Ä—É (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ), –Ω–æ –µ—Å–ª–∏ —Ç–∞–∫–∏—Ö –º–Ω–æ–≥–æ ‚Äì –ø—É—Å—Ç–∞—è —Ç—Ä–∞—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. (3) *–ù–∏–∑–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å:* –ø—Ä–∏ –∞–±–ª—è—Ü–∏–∏ –≥–æ–ª–æ–≤ –±–µ–∑ –ø–∞–¥–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ ‚Äì –∑–Ω–∞—á–∏—Ç, –æ–Ω–∏ –Ω–µ –Ω–µ—Å–ª–∏ –æ—Å–æ–±–æ–≥–æ —Å–º—ã—Å–ª–∞ (–æ–Ω–∏ –º–æ–≥–ª–∏ –∫–æ–ª–ª–∞–ø—Å–∏—Ä–æ–≤–∞—Ç—å –∫ —Å–æ—Å—Ç–æ—è–Ω–∏—é ‚Äú–Ω–∏—á–µ–≥–æ –ø–æ–ª–µ–∑–Ω–æ–≥–æ‚Äù).
> 
> **Redundancy (–∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å):** –ü–æ–Ω—è—Ç–∏–µ –±–ª–∏–∑–∫–æ: –∫–æ–≥–¥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –≤—ã–ø–æ–ª–Ω—è—é—Ç –æ–¥–Ω—É –∏ —Ç—É –∂–µ –∑–∞–¥–∞—á—É, –æ–¥–Ω–∞ –∏–∑ –Ω–∏—Ö –ø–æ —Å—É—Ç–∏ –ª–∏—à–Ω—è—è. –í BERT-BASE (12 heads per layer) –æ–±–Ω–∞—Ä—É–∂–∏–ª–æ—Å—å, —á—Ç–æ –±–æ–ª–µ–µ –ø–æ–ª–æ–≤–∏–Ω—ã –≥–æ–ª–æ–≤ –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –±–µ–∑ –æ—â—É—Ç–∏–º–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞„Äê20‚Ä†L207-L212„Äë. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ–± –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏. Redundancy –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–ª–æ—Ö–æ ‚Äì –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ–∑–µ—Ä–≤ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –∫–∞–∫–∏–µ-—Ç–æ –≥–æ–ª–æ–≤—ã ‚Äú–∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—Ç‚Äù. –ù–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–∏, —ç—Ç–æ —Å–∏–≥–Ω–∞–ª, —á—Ç–æ –µ—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
> 
> **–ü–æ—á–µ–º—É –≤–æ–∑–Ω–∏–∫–∞–µ—Ç collapse:** –û–¥–Ω–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ ‚Äì *—Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è*: –µ—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–æ–ª–æ–≤, —Ç–æ SGD –º–æ–∂–µ—Ç —Å–∫–∞—Ç–∏—Ç—å—Å—è –≤ —Ä–µ—à–µ–Ω–∏–µ, –≥–¥–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –ø—Ä–æ—Å—Ç–æ –∏–º–∏—Ç–∏—Ä—É—é—Ç –¥—Ä—É–≥–∏–µ. –≠—Ç–æ –ø—Ä–æ—â–µ (gradient flow –º–æ–∂–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —É—Å–ø–µ—à–Ω—É—é –≥–æ–ª–æ–≤—É –≤–º–µ—Å—Ç–æ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è). –û—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –ø–æ–∑–¥–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ learning rate –º–∞–ª, –º–æ–¥–µ–ª—å –Ω–µ –±—É–¥–µ—Ç —Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ –¥–µ–ª–∏—Ç—å —Ä–∞–±–æ—Ç—É –∏–Ω–∞—á–µ ‚Äì –æ–Ω–∞ –∑–∞—Å—Ç—Ä—è–Ω–µ—Ç –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –º–∏–Ω–∏–º—É–º–µ, –≥–¥–µ —á–∞—Å—Ç—å –≥–æ–ª–æ–≤ ‚Äú–ª–µ–Ω—è—Ç—Å—è‚Äù. –ï—â—ë —Ñ–∞–∫—Ç–æ—Ä ‚Äì *—Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö vs –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤*: –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ç–∞–∫ –º–Ω–æ–≥–æ, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–µ –Ω–∞–π—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—Å–µ–º –≥–æ–ª–æ–≤–∞–º, –≤–æ—Ç —á–∞—Å—Ç—å –∏ –±–µ–∑–¥–µ–ª—å–Ω–∏—á–∞–µ—Ç.
> 
> **Attention collapse –≤ —Ç–µ–æ—Ä–∏–∏:** –ï—Å—Ç—å —Ä–∞–±–æ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚ÄúInfinite depth vs infinite width in transformers‚Äù) –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–æ–≥–¥–∞ \(h \to \infty\) (–æ—á–µ–Ω—å –º–Ω–æ–≥–æ –≥–æ–ª–æ–≤), –æ–ø—Ç–∏–º—É–º —Å–µ—Ç–∏ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≥–æ–ª–æ–≤ ‚Äì –æ–Ω–∏ –≤—Å–µ –æ–±—É—á–∞—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ„Äê39‚Ä†L8-L14„Äë. –¢–æ –µ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö ‚Äú–∞–≥–µ–Ω—Ç–æ–≤‚Äù ‚Äì –∏ –æ–Ω–∏ –ø—Ä–∏—Ö–æ–¥—è—Ç –∫ –æ–¥–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é –∏–∑-–∑–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏. –ó–Ω–∞—á–∏—Ç, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –≥–æ–ª–æ–≤ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ. –ï—â—ë –µ—Å—Ç—å –ø–æ–Ω—è—Ç–∏–µ *entropy of attention*: –µ—Å–ª–∏ –≥–æ–ª–æ–≤–∞ –≤—Å–µ–≥–¥–∞ –≤—ã–¥–∞—ë—Ç –ø–æ—á—Ç–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–µ—Å–∞ (–Ω–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ —Ñ–æ–∫—É—Å–∞), —Ç–æ –æ–Ω–∞ –º–∞–ª–æ —á–µ–≥–æ ‚Äú—Ä–µ—à–∞–µ—Ç‚Äù ‚Äì –∫–æ–ª–ª–∞–ø—Å –≤ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å.
> 
> **Attention dropout & —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –ß—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å collapse, —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç dropout –Ω–∞ attention weights (Transformer —Ç–∞–∫ –∏ –¥–µ–ª–∞–ª ‚Äì dropout –ø–æ—Å–ª–µ softmax). –≠—Ç–æ –Ω–µ–º–Ω–æ–≥–æ —Ä—É—à–∏—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏, –∑–∞—Å—Ç–∞–≤–ª—è—è –≥–æ–ª–æ–≤—ã –±—ã—Ç—å —É—Å—Ç–æ–π—á–∏–≤—ã–º–∏ –∫ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—é –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ª–∏–Ω–∫–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é. –¢–∞–∫–∂–µ –µ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–¥–ª–∞–≥–∞—é—â–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä, —à—Ç—Ä–∞—Ñ—É—é—â–∏–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É attention –º–∞—Ç—Ä–∏—Ü–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤ ‚Äì —Ü–µ–ª—å —Ä–∞–∑–≤–µ—Å—Ç–∏ –∏—Ö (–ø–æ–¥–æ–±–Ω–æ —Ç–æ–º—É, –∫–∞–∫ –¥–µ–ª–∞—é—Ç –≤ CNN –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤). 
> 
> **–í —è–≤–Ω–æ–º –≤–∏–¥–µ ‚Äú—Ç–µ—Ä—è—é—Ç —Å–º—ã—Å–ª‚Äù:** –í –º–∞–ª—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ –ø—ã—Ç–∞—é—Ç—Å—è —Å–∂–∞—Ç—å (prune) ‚Äì –≤—ã–∫–∏–¥—ã–≤–∞—é—Ç –≥–æ–ª–æ–≤—ã, –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –º–æ–≥—É—Ç –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞—Ç—å—Å—è –∏ –∏–Ω–æ–≥–¥–∞ –¥—Ä—É–≥–∏–µ –≥–æ–ª–æ–≤—ã –Ω–∞—á–∏–Ω–∞—é—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–æ–ª–∏ –≤—ã–∫–∏–Ω—É—Ç—ã—Ö (fine-tuning –ø–æ—Å–ª–µ pruning). –ï—Å–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å, –º–æ–∂–Ω–æ –ø—Ä–∏–π—Ç–∏ –∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, Voita et al. —Å—É–º–µ–ª–∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å model (–¥–ª—è MT) –¥–æ ~30% –≥–æ–ª–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ BLEU, —É–±—Ä–∞–≤ —Ç–µ, —á—Ç–æ ‚Äú–Ω–µ –∏–º–µ–ª–∏ —Å–º—ã—Å–ª–∞‚Äù (–ø–æ–¥ —ç—Ç–∏–º –æ–Ω–∏ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–ª–∏ –≥–æ–ª–æ–≤—ã —Å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–º –∏–ª–∏ —Ö–∞–æ—Ç–∏—á–Ω—ã–º attention). 
> 
> **–ü—Ä–∏–º–µ—Ä:** –í –æ–¥–Ω–æ–º –∏–∑ —Å–ª–æ—ë–≤ GPT-2, –¥–æ–ø—É—Å—Ç–∏–º, 4 –≥–æ–ª–æ–≤—ã. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, –¥–≤–µ –∏–∑ –Ω–∏—Ö –Ω–∞—á–∞–ª–∏ –æ–±–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ–∫–µ–Ω (Previous Token Heads), –∞ –¥–≤–µ ‚Äì –≤—Å–µ–≥–¥–∞ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ü–æ —Å—É—Ç–∏, —Ç—É—Ç 2 —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –Ω–µ 4 ‚Äì 2 –≥–æ–ª–æ–≤—ã –¥—É–±–ª–∏—Ä—É—é—Ç. –≠—Ç–æ collapse (2 –∫–ª–∞—Å—Å–∞ –≤–º–µ—Å—Ç–æ 4). –ï—Å–ª–∏ –æ—Ç–∫–ª—é—á–∏—Ç—å –æ–¥–Ω—É –∏–∑ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö ‚Äì –Ω–∏—á–µ–≥–æ –Ω–µ –∏–∑–º–µ–Ω–∏—Ç—Å—è, –µ—ë –∫–æ–ª–ª–µ–≥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å—ë. –í–æ—Ç —Ç–∞–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ—Ç—Å—è redundancy.
> 
> **–û–ø–∞—Å–Ω–æ—Å—Ç—å collapse:** –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω—ã–π, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–µ–¥–æ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ—é –º–æ—â–Ω–æ—Å—Ç—å. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏, single-head –æ–≥—Ä–∞–Ω–∏—á–µ–Ω: –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É—á–µ—Å—Ç—å —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. –ï—Å–ª–∏ multi-head –∫–æ–ª–ª–∞–ø—Å–∏—Ä–æ–≤–∞–ª –∫ –æ–¥–Ω–æ–º—É ‚Äì –º–æ–¥–µ–ª—å —Ç–µ—Ä—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ. –í–ø—Ä–æ—á–µ–º, –≤ —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö LLM –ø–æ–ª–Ω–∞—è –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞ –Ω–µ —Å–ª—É—á–∞–µ—Ç—Å—è ‚Äì –æ–±—ã—á–Ω–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≥–æ–ª–æ–≤ —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ –¥–∞ –¥–µ–ª–∞—é—Ç, –Ω–æ –¥–æ–ª—è –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞.
> 
> **–°–≤—è–∑—å —Å ‚Äúoverfitting‚Äù –∏ ‚Äúoversmoothing‚Äù:** –í –≥—Ä–∞—Ñ–æ–≤—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –µ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω oversmoothing ‚Äì –≤—Å–µ —É–∑–ª—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –ø–æ—Å–ª–µ –º–Ω–æ–≥–∏—Ö —Å–ª–æ—ë–≤. –í —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –∞–Ω–∞–ª–æ–≥ ‚Äì –µ—Å–ª–∏ –±—ã –≤—Å–µ –≥–æ–ª–æ–≤—ã (–∏–ª–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã) —Å—Ç–∞–ª–∏ –≤—ã–¥–∞–≤–∞—Ç—å —á—Ç–æ-—Ç–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ, –º–æ–¥–µ–ª—å –ø–æ—Ç–µ—Ä—è–ª–∞ –±—ã —Ä–∞–∑–ª–∏—á–∏—è. Collapse –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π –ø–æ—Ç–µ—Ä–∏ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç–∏.
> 
> **–≠–º–ø–∏—Ä–∏–∫–∞:** –ê–Ω–∞–ª–∏–∑ 2021 (Kovaleva et al.) –æ—Ç–º–µ—á–∞–ª, —á—Ç–æ –≤ BERT –µ—Å—Ç—å ‚Äú–ø—Ä–æ—Å—Ç–æ–π‚Äù –ø–∞—Ç—Ç–µ—Ä–Ω ‚Äì –º–Ω–æ–≥–∏–µ –≥–æ–ª–æ–≤—ã –ª–∏–±–æ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ, –ª–∏–±–æ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω, –ª–∏–±–æ SEP —Ç–æ–∫–µ–Ω ‚Äì –∏ –º–∞–ª–æ —Å–ª–æ–∂–Ω—ã—Ö. –¢–æ –µ—Å—Ç—å –º–æ–¥–µ–ª—å —Å–∏–ª—å–Ω–æ rely on –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤, –∞ –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Äì redundancy. Anthropic (2022) –≤ –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –≤–∏–¥–µ–ª–∏, —á—Ç–æ –∫–æ–≥–¥–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç induction heads, –¥—Ä—É–≥–∏–µ heads –º–æ–≥—É—Ç —Å—Ç–∞—Ç—å –º–µ–Ω–µ–µ –∞–∫—Ç–∏–≤–Ω—ã–º–∏ ‚Äì –∫–∞–∫ –±—É–¥—Ç–æ —ç—Ç–∏ –¥–≤–æ–µ –≤–∑—è–ª–∏ –Ω–∞ —Å–µ–±—è –º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã, –∞ —Ä–æ–ª–∏ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–æ–∫—Ä–∞—Ç–∏–ª–∏—Å—å.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** Collapse ‚Äì –∫–∞–∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–µ. –ï—Å–ª–∏ –≤—Å–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞—á–∏–Ω–∞—é—Ç –¥—É–º–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ, –∫–æ–º–ø–∞–Ω–∏—è —Ç–µ—Ä—è–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å. –í –º–æ–∑–≥–µ, –∫—Å—Ç–∞—Ç–∏, —Ç–æ–∂–µ –±—ã–≤–∞–µ—Ç ‚Äì –µ—Å–ª–∏ –Ω–µ–π—Ä–æ–ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç, —á–µ–ª–æ–≤–µ–∫ –º–µ–Ω–µ–µ –ø—Ä–∏—Å–ø–æ—Å–∞–±–ª–∏–≤–∞–µ—Ç—Å—è. –¢—É—Ç —Ç–æ –∂–µ: –º—ã —Ö–æ—Ç–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≥–æ–ª–æ–≤, –∫–∞–∫ –≥–∞—Ä–∞–Ω—Ç–∏—é –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. Redundancy —Å –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã ‚Äì –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–ª–æ—Ö–æ: –º–æ–∂–µ—Ç, –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –≤–º–µ—Å—Ç–µ —É—Å—Ç–æ–π—á–∏–≤–µ–µ (–∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã). –ù–æ —Å –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è, redundancy ‚Äì —ç—Ç–æ —Ç—Ä–∞—Ç–∞ —Ä–µ—Å—É—Ä—Å–æ–≤. –í –ø—Ä–µ–¥–µ–ª–µ, —Ö–æ—Ç–µ–ª–æ—Å—å –±—ã, —á—Ç–æ–±—ã –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –±—ã–ª–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —ç–∫—Å–ø–µ—Ä—Ç–æ–º. 
> 
> **–°—Å—ã–ª–∫–∏:** Michel et al. 2019 (–ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ –≥–æ–ª–æ–≤)„Äê20‚Ä†L134-L142„Äë„Äê20‚Ä†L205-L212„Äë; Zheng et al. 2024 (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–π heads –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ redundancy)„Äê2‚Ä†L7-L15„Äë„Äê2‚Ä†L43-L50„Äë; Theis et al. 2019 (–∞–Ω–∞–ª–∏—Ç. –ø–æ head collapse).
> 
> ## 11. **–ö–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ attention heads –ø—Ä–∏ fine-tuning, LoRA, QLoRA** 
> **Fine-tuning (–¥–æ–æ–±—É—á–µ–Ω–∏–µ) –∏ heads:** –ü—Ä–∏ –æ–±—ã—á–Ω–æ–º –ø–æ–ª–Ω–æ–æ–±—ä–µ–º–Ω–æ–º fine-tuning –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–ª–µ–π –≥–æ–ª–æ–≤ –º–æ–∂–µ—Ç —Å–º–µ—â–∞—Ç—å—Å—è. –ú–æ–¥–µ–ª—å —á–∞—Å—Ç–∏—á–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞ *–æ–±—â–∏–µ* —Ñ—É–Ω–∫—Ü–∏–∏ –≥–æ–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å), –Ω–æ –º–æ–∂–µ—Ç –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –ø–æ–¥ –∑–∞–¥–∞—á–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã –¥–æ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –∑–∞–¥–∞—áy –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ –æ–¥–Ω–∞ –∏–∑ —Ä–∞–Ω–µ–µ ‚Äú–æ–±—â–∏—Ö‚Äù –≥–æ–ª–æ–≤ –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ —Å –æ—Ç–≤–µ—Ç–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ fine-tuning –º–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ *–ø–µ—Ä–µ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏*: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –∞–∫—Ç–∏–≤–Ω—ã–º–∏/–≤–∞–∂–Ω—ã–º–∏ –¥–ª—è –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏, –∞ –¥—Ä—É–≥–∏–µ ‚Äì –ø–æ–¥–∞–≤–ª—è—é—Ç—Å—è. Voita et al. –Ω–∞—Ö–æ–¥–∏–ª–∏, —á—Ç–æ –≤ –ø–µ—Ä–µ–≤–æ–¥–µ –ø—Ä–∏ fine-tune –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å—Ç–∏–ª—å —á–∞—Å—Ç—å –≥–æ–ª–æ–≤ ‚Äú–æ—Ç–º–∏—Ä–∞–µ—Ç‚Äù (–≤–Ω–∏–º–∞–Ω–∏–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–º), –∞ –ø–∞—Ä–∞ –≥–æ–ª–æ–≤ –±–µ—Ä—ë—Ç –Ω–∞ —Å–µ–±—è –Ω–æ–≤—É—é —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É.
> 
> **LoRA (Low-Rank Adaptation) –∏ heads:** LoRA ‚Äì –º–µ—Ç–æ–¥, –≥–¥–µ –æ–±—É—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–æ—á–Ω—ã–µ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã \(\Delta W\) –∫ –≤–µ—Å–∞–º, —á–∞—â–µ –≤—Å–µ–≥–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç –∫ \(W^Q, W^V\) –º–∞—Ç—Ä–∏—Ü–∞–º –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ –±–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ –≥–æ–ª–æ–≤ (–∏–∑ pre-trained) –æ—Å—Ç–∞—é—Ç—Å—è, –∞ –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Å–ª–µ–≥–∫–∞ —Å–º–µ—â–∞–µ—Ç –∏—Ö. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ LoRA —á–∞—Å—Ç–æ –Ω–∞–∫–ª–∞–¥—ã–≤–∞—é—Ç *–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≥–æ–ª–æ–≤—ã –∏–ª–∏ —Å–ª–æ–∏*. –ü—Ä–∞–∫—Ç–∏–∫–∏ –∑–∞–º–µ—á–∞—é—Ç, —á—Ç–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å LoRA –Ω–∞ –≤—Å–µ –≥–æ–ª–æ–≤—ã ‚Äì –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: –∏–Ω–æ–≥–¥–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–µ—Ä—Ö–Ω–∏—Ö —Å–ª–æ—ë–≤ –∏–ª–∏ –¥–∞–∂–µ –≤—ã–±–æ—Ä–æ—á–Ω—ã—Ö –≥–æ–ª–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –∑–Ω–∞–µ—à—å, –∫–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω—É–∂–Ω—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞–±–æ—Ç–∞ 2023 –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ LoRA –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö heads —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–æ–Ω–∏ –¥–∞–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ ‚ÄúLoRA per head‚Äù)„Äê30‚Ä†L19-L27„Äë. LoRA –æ–±—ã—á–Ω–æ *–Ω–µ –ª–æ–º–∞–µ—Ç* –∏–∑–Ω–∞—á–∞–ª—å–Ω—É—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é, –∞ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é. –¢.–µ. –≥–æ–ª–æ–≤a, —É–º–µ–≤—à–∞—è coreference, –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —ç—Ç–æ —É–º–µ—Ç—å, –Ω–æ LoRA-–≤—Å—Ç–∞–≤–∫–∞ –º–æ–∂–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –ª–µ–≥–∫–∏–π —Å–¥–≤–∏–≥, —á—Ç–æ–± —É—á–∏—Ç—ã–≤–∞—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–Ω–∞–ø—Ä., –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–º–µ–Ω). –ß–∞—Å—Ç–æ LoRA –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–ª—è –¥–æ–º–µ–Ω–Ω—ã—Ö –∞–¥–∞–ø—Ç–∞—Ü–∏–π (–º–µ–¥–∏—Ü–∏–Ω–∞, –∫–æ–¥) ‚Äì —Ç–æ–≥–¥–∞, –≤–µ—Ä–æ—è—Ç–Ω–æ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ lexical heads –ø–µ—Ä–µ–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –ø–æ–¥ –Ω–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã.
> 
> **QLoRA (Quantized LoRA):** –≠—Ç–æ —Ç–æ –∂–µ LoRA, –Ω–æ –º–æ–¥–µ–ª—å –≤ int4 –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∞. –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç —á—É—Ç—å —É–º–µ–Ω—å—à–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –≥–æ–ª–æ–≤ ‚Äì –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. QLoRA –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç, —á—Ç–æ fine-tune –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ö–æ—Ç—å –∏ –Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Å–∞—Ö, –Ω–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—á–∏—Ç–∞—é—Ç—Å—è –∫–∞–∫ –æ–±—ã—á–Ω–æ. –í–æ–ø—Ä–æ—Å: –Ω–µ —Å—Ç—Ä–∞–¥–∞—é—Ç –ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã —Å–∏–ª—å–Ω–µ–µ –æ—Ç –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–≥–æ —à—É–º–∞? –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ, QLoRA (Dettmers et al. 2023) –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –º–æ–∂–Ω–æ fine-tune 65B –º–æ–¥–µ–ª—å –ø–æ—á—Ç–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ‚Äì –∑–Ω–∞—á–∏—Ç, –≤–∞–∂–Ω—ã–µ –≥–æ–ª–æ–≤—ã –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–Ω–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Ç–∏–ø–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö —Ä–∞–∑–ª–∏—á–∏–π –≤ –≤–µ—Å–∞—Ö) —Å–≥–ª–∞–∂–∏–≤–∞—é—Ç—Å—è, –Ω–æ –∫—Ä—É–ø–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –±—ã–ª–æ –æ—Ç–º–µ—á–µ–Ω–æ.
> 
> **Phenomena observed:** –ù–µ–¥–∞–≤–Ω—è—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–±–æ—Ç–∞ (Yao et al. 2023) –æ—Ç–º–µ—á–∞–µ—Ç –¥–≤–µ –≤–µ—â–∏ –ø—Ä–∏ fine-tune: (1) *Different impact:* –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å \(W^Q, W^V\) –¥–∞—ë—Ç –±–æ–ª—å—à–∏–π –ø—Ä–∏—Ä–æ—Å—Ç, —á–µ–º \(W^K\)„Äê31‚Ä†L83-L92„Äë„Äê31‚Ä†L85-L94„Äë. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–Ω–∞—Å—Ç—Ä–æ–π–∫—É ‚Äú—á—Ç–æ –∏—Å–∫–∞—Ç—å‚Äù (Query) –∏ ‚Äú—á—Ç–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å‚Äù (Value), –∞ Keys (—á—Ç–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–æ–º) –º–µ–Ω–µ–µ –≤–∞–∂–Ω–æ –º–µ–Ω—è—Ç—å. –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –ø–æ—Ç–æ–º—É —á—Ç–æ Key —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤ (—á—Ç–æ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —Å—Ö–æ–∂–∏–º). (2) *Efficient convergence:* –æ–Ω–∏ —Ç–∞–∫–∂–µ –Ω–∞—à–ª–∏, —á—Ç–æ –¥–ª—è –ª—É—á—à–µ–≥–æ fine-tune –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ lr –Ω–∞ \(W^Q\) ‚Äì –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã, –∞ \(W^K, W^V\) –º–æ–∂–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ„Äê31‚Ä†L49-L58„Äë„Äê31‚Ä†L85-L94„Äë. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏, LoRA —á–∞—Å—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω–∞ \(Q, V\) –≤–µ—à–∞—é—Ç, –∏—Å—Ö–æ–¥—è –∏–∑ —ç–º–ø–∏—Ä–∏–∫–∏.
> 
> **Heads dying or shifting:** –ü—Ä–∏ —Å–∏–ª—å–Ω–æ–º fine-tune (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–±—É—á–∞–µ–º GPT-2 –Ω–∞ —Å–æ–≤—Å–µ–º –¥—Ä—É–≥—É—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —Å–∫–∞–∂–µ–º, –º—É–∑—ã–∫—É), –≤–æ–∑–º–æ–∂–Ω–æ —á–∞—Å—Ç—å –≥–æ–ª–æ–≤ –ø–µ—Ä–µ—Å—Ç–∞–Ω–µ—Ç –ø–æ–ª—É—á–∞—Ç—å —Å–∏–≥–Ω–∞–ª –∏ effectively ‚Äú–ø–æ–≥–∏–±–Ω–µ—Ç‚Äù (–±—É–¥–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤—Å–µ–≥–¥–∞). –î—Ä—É–≥–∏–µ –Ω–∞–æ–±–æ—Ä–æ—Ç –º–æ–≥—É—Ç —É—Å–∏–ª–∏—Ç—å—Å—è. Fine-tuning, –æ—Å–æ–±–µ–Ω–Ω–æ —Å –Ω–µ–±–æ–ª—å—à–∏–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º, —Ä–∏—Å–∫—É–µ—Ç –≤—ã–∑–≤–∞—Ç—å *catastrophic forgetting* ‚Äì —Å—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≥–æ–ª–æ–≤ –∑–∞—Ç–∏—Ä–∞—é—Ç—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, BERT fine-tuned –Ω–∞ NLI: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ heads, —Ä–∞–Ω—å—à–µ —É–º–µ–≤—à–∏–µ coreference, –º–æ–≥—É—Ç –ø–µ—Ä–µ—Å—Ç–∞—Ç—å —ç—Ç–æ –¥–µ–ª–∞—Ç—å, –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–≤—à–∏—Å—å –Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ª–µ–π–±–ª–æ–≤. 
> 
> **LoRA ‚Äì –±–æ–ª–µ–µ —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π –ø–∞–º—è—Ç—å:** –ü–æ—Å–∫–æ–ª—å–∫—É LoRA –Ω–µ –º–µ–Ω—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –≤–µ—Å–∞, –∞ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–∞–ª–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ, –±–∞–∑–æ–≤–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–æ–ª–æ–≤ –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≥–æ—Ä–∞–∑–¥–æ –ª—É—á—à–µ. –≠—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è: LoRA –º–æ–¥–µ–ª–∏ –æ–±—ã—á–Ω–æ –º–æ–≥—É—Ç –∏ —Å—Ç–∞—Ä–æ–µ —É–º–µ–Ω–∏–µ, –∏ –Ω–æ–≤–æ–µ ‚Äì –æ–Ω–∏ –ø—Ä–æ—Å—Ç–æ –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏, LoRA ‚Äì —ç—Ç–æ –∫–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å ‚Äú–Ω–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω—ã‚Äù –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç–∞—Ä—ã—Ö. –ü–æ—ç—Ç–æ–º—É –¥–ª—è attention heads LoRA –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ: –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –¥–æ–±–∞–≤–∫–∏ —á—É—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏–µ. –ù–∞–ø—Ä., –µ—Å–ª–∏ –Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ ‚Äì –≤—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞—Ç—å –≤ —Å—Ç–∏–ª–µ ‚ÄúYes, sir‚Äù, –∫–∞–∫–∞—è-—Ç–æ –≤–µ—Ä—Ö–Ω—è—è –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç —Å –ø–æ–º–æ—â—å—é LoRA –Ω–∞—É—á–∏—Ç—å—Å—è –≤—Å–µ–≥–¥–∞ —É–¥–µ–ª—è—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞—á–∞–ª—É –∏ –≤—Å—Ç–∞–≤–ª—è—Ç—å ‚ÄúYes, sir‚Äù (—á–µ—Ä–µ–∑ feed-forward), –Ω–æ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã, –æ—Ç–≤–µ—á–∞—é—â–∏–µ –∑–∞ –±–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –Ω–µ —Ç—Ä–æ–Ω—É—Ç—Å—è.
> 
> **Case ‚Äì LoRA for bias mitigation:** –ï—Å—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –ø—Ä–∏–º–µ–Ω—è–ª–∏ LoRA, –Ω–æ —Ç–æ–ª—å–∫–æ –Ω–∞ –∏–∑–±—Ä–∞–Ω–Ω—ã–µ biased heads (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≥–µ–Ω–¥–µ—Ä–Ω–æ –∑–∞–≤–∏—Å–∏–º—ã–µ). –¢–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –º–æ–∂–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –∏—Ö –≤–µ—Å. –¢–∞–∫ —Ç–æ—á–µ—á–Ω–æ –º–æ–∂–Ω–æ –ø—Ä–∞–≤–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.
> 
> **Quantization effect:** –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ –±—å—ë—Ç –ø–æ scale –∏ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –î–ª—è –≤–Ω–∏–º–∞–Ω–∏–π, –µ—Å–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –∏–º–µ–ª–∏ –æ—á–µ–Ω—å sharp distribution, –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ 4-bit –º–æ–∂–µ—Ç —á—É—Ç—å —Å–≥–ª–∞–¥–∏—Ç—å (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –Ω–µ–Ω—É–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –±–æ–ª—å—à–µ, —á–µ–º –±—ã–ª–æ). –≠—Ç–æ –∏–Ω–æ–≥–¥–∞ –≤–µ–¥—ë—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é quality –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–ª—å–Ω–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π). –ù–æ QLoRA fine-tune –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ—Ç —á–∞—Å—Ç—å —ç—Ç–∏—Ö –æ—à–∏–±–æ–∫, –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞—è LoRA –¥–µ–ª—å—Ç—ã.
> 
> **–ü—Ä–∏–º–µ—Ä:** –í LLaMA-2 13B, –≤–æ–∑—å–º—ë–º –≥–æ–ª–æ–≤—É, –æ—Ç–≤–µ—á–∞—é—â—É—é –∑–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞ (—Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ ``` —Ç–æ–∫–µ–Ω—ã). –ï—Å–ª–∏ –º—ã LoRA-—Ñ–∞–π–Ω—Ç—é–Ω–∏–º –º–æ–¥–µ–ª—å –ø–æ–¥ —Ç–æ, —á—Ç–æ–±—ã –æ–Ω–∞ –¥–∞–≤–∞–ª–∞ –µ—â—ë –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, —ç—Ç–∞ –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç —á—É—Ç—å —Å–º–µ–Ω–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ: —Ä–∞–Ω—å—à–µ –æ–Ω–∞ —Å–º–æ—Ç—Ä–µ–ª–∞ –≤–Ω—É—Ç—Ä—å –∫–æ–¥–∞, –∞ —Ç–µ–ø–µ—Ä—å, –º–æ–∂–µ—Ç, –Ω–∞—á–Ω—ë—Ç –µ—â–µ –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç ‚Äì —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ù–æ –≥–ª–æ–±–∞–ª—å–Ω–æ, –µ—ë —Ñ—É–Ω–∫—Ü–∏—è ‚Äú—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∫–æ–¥‚Äù —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è, –ø—Ä–æ—Å—Ç–æ +–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.
> 
> **Empirical observation:** –ß–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç–º–µ—á–∞–ª–∞, —á—Ç–æ fine-tuned –º–æ–¥–µ–ª–∏ –∏–Ω–æ–≥–¥–∞ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è *–±–æ–ª–µ–µ –æ–¥–Ω–æ–æ–±—Ä–∞–∑–Ω—ã* –≤ –≤–Ω–∏–º–∞–Ω–∏–∏ ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –∑–∞–¥–∞—á–µ –Ω–µ –Ω—É–∂–µ–Ω –≤–µ—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π —Ä–µ–ø–µ—Ä—Ç—É–∞—Ä. –ù–∞–ø—Ä–∏–º–µ—Ä, GPT-2 fine-tuned –Ω–∞ —Å—Ç–∏—Ö–∞—Ö: –º–Ω–æ–≥–∏–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ heads –º–æ–≥—É—Ç —É–∂–µ –Ω–µ –Ω—É–∂–Ω—ã, –∑–∞—Ç–æ heads, –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—â–∏–µ —Ä–∏—Ñ–º—ã –∏–ª–∏ –º–µ—Ç—Ä, –º–æ–≥–ª–∏ –±—ã –ø–æ—è–≤–∏—Ç—å—Å—è (–≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏). 
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** Fine-tuning ‚Äì –∫–∞–∫ –ø–µ—Ä–µ–ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–∑–≥–∞ –ø–æ–¥ –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É. –ï—Å–ª–∏ pre-trained –º–æ–¥–µ–ª—å ‚Äì –ø–æ–ª–∏–≥–ª–æ—Ç-—ç–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—Å—Ç, —Ç–æ fine-tune –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã ‚Äì –∫–∞–∫ –µ—Å–ª–∏ –±—ã —É—á—ë–Ω–æ–≥–æ –∑–∞—Å—Ç–∞–≤–∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏–∏: –æ–Ω –∑–∞–±—É–¥–µ—Ç —á–∞—Å—Ç—å –¥—Ä—É–≥–æ–≥–æ –∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç —Å–≤–æ–π ‚Äú–≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞–ø–ø–∞—Ä–∞—Ç‚Äù –ø–æ–¥ —à–∞–±–ª–æ–Ω—ã –∑–∞–∫–æ–Ω–æ–≤. LoRA ‚Äì –±–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ–¥—Ö–æ–¥, –∫–∞–∫ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞: –±–∞–∑–æ–≤—ã–µ –≥–æ–ª–æ–≤—ã (—ç–∫—Å–ø–µ—Ä—Ç—ã) –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, –∞ LoRA-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç) –∏—Ö —Å–ª–µ–≥–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –≤ –Ω—É–∂–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É. QLoRA ‚Äì —Ç–æ –∂–µ, –Ω–æ –Ω–∞ –¥–∏–µ—Ç–µ (—Å–∂–∞—Ç—ã–π –º–æ–∑–≥). –õ—é–±–æ–ø—ã—Ç–Ω–æ, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞, LoRA/QLoRA –ø–æ–∑–≤–æ–ª—è–µ—Ç *–º–Ω–æ–≥–æ–ª–∏–∫–æ—Å—Ç—å*: –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ LoRA adapters –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∏–ª–µ–π/–∑–∞–¥–∞—á, —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—è ‚Äú–ª–∏—á–Ω–æ—Å—Ç—å‚Äù –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ—É—á–∏–≤–∞–Ω–∏—è. –≠—Ç–æ –∫–∞–∫ –µ—Å–ª–∏ –≤ –º–æ–∑–≥–µ –±—ã–ª–∏ –±—ã —Å–º–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ ‚Äì –¥–æ–≤–æ–ª—å–Ω–æ —Ñ—É—Ç—É—Ä–∏—Å—Ç–∏—á–Ω–æ, –Ω–æ –≤ LLM —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —Ç–∞–∫.
> 
> **–°—Å—ã–ª–∫–∏:** –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ fine-tune (Yao et al. 2023)„Äê31‚Ä†L83-L92„Äë„Äê31‚Ä†L85-L94„Äë; Hu et al. 2021 (LoRA) ‚Äì –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –ø—Ä–æ Q,V; Head Purification via LoRA„Äê30‚Ä†L19-L27„Äë.
> 
> ## 12. **–ö–∞–∫–∏–µ –≥–æ–ª–æ–≤—ã "—É–º–∏—Ä–∞—é—Ç" –≤ –º–∞–ª—ã—Ö –º–æ–¥–µ–ª—è—Ö: pruning –∏ sparsity** 
> **"–£–º–∏—Ä–∞—é—â–∏–µ" –≥–æ–ª–æ–≤—ã:** –≠—Ç–æ –ø–æ–Ω—è—Ç–∏–µ –æ–±—Ä–∞–∑–Ω–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ª–∏–±–æ –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –±–µ–∑ —É—â–µ—Ä–±–∞, –ª–∏–±–æ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –≤–Ω–æ—Å—è—Ç –≤–∫–ª–∞–¥ (–∏—Ö –≤—ã—Ö–æ–¥ –±–ª–∏–∑–æ–∫ –∫ –Ω—É–ª—é –∏–ª–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ). –í –º–∞–ª—ã—Ö –º–æ–¥–µ–ª—è—Ö (–∏–ª–∏ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö) —á–∞—Å—Ç–æ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è, —á—Ç–æ –Ω–µ –≤—Å–µ –≥–æ–ª–æ–≤—ã –Ω–∞—à–ª–∏ –ø–æ–ª–µ–∑–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é ‚Äì –æ–Ω–∏ –∫–∞–∫ ‚Äú—Å–ø—è—â–∏–µ‚Äù. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ 2-—Å–ª–æ–π–Ω–æ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è, —á—Ç–æ –∏–∑ 8 –≥–æ–ª–æ–≤ —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞ —Ä–µ–∞–ª—å–Ω–æ –∞–∫—Ü–µ–Ω—Ç–∏—Ä—É–µ—Ç —á—Ç–æ-—Ç–æ, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤—Å–µ–≥–¥–∞ –¥–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ—Å–∞ ‚Äì –∏—Ö –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å –º—ë—Ä—Ç–≤—ã–º–∏ (–Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–º–∏).
> 
> **Pruning (–æ–ø—Ä–æ—Å—Ç–∞–Ω–∏–µ, –æ—Ç—Å–µ—á–µ–Ω–∏–µ):** –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–±—É—é—Ç —É–¥–∞–ª—è—Ç—å –≥–æ–ª–æ–≤—ã –ø–æ—Å–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏. Michel et al. 2019 –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–∂–Ω–æ –≤—ã–∫–∏–Ω—É—Ç—å –¥–æ 20-40% –≥–æ–ª–æ–≤ –≤ MT-–º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞„Äê20‚Ä†L207-L212„Äë. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ —ç—Ç–∏ –≥–æ–ª–æ–≤—ã –±—ã–ª–∏ –ª–∏–±–æ –∏–∑–±—ã—Ç–æ—á–Ω—ã, –ª–∏–±–æ ‚Äú–º–µ—Ä—Ç–≤—ã–µ‚Äù. –¢–∞–∫–æ–π pruning –¥–µ–ª–∞–ª–∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ: –≤—ã—á–∏—Å–ª—è–ª–∏ –≤–∞–∂–Ω–æ—Å—Ç—å \(I_h\) (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ –ø–æ–≤—ã—à–µ–Ω–∏–µ –ª–æ—Å—Å–∞ –ø—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ –≥–æ–ª–æ–≤—ã)„Äê20‚Ä†L179-L187„Äë„Äê20‚Ä†L187-L195„Äë, –∑–∞—Ç–µ–º –≤—ã—Ä–µ–∑–∞–ª–∏ –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—É—é. –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –Ω–æ transformer —Ç–µ—Ä–ø–∏—Ç –¥–∞–∂–µ 50% –≥–æ–ª–æ–≤ –≤—ã—Ä–µ–∑–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–∂–¥–µ —á–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–ª—å–Ω–æ —É–ø–∞–¥—ë—Ç„Äê20‚Ä†L207-L212„Äë. –ó–Ω–∞—á–∏—Ç, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å ‚Äì –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞. –í –º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö (GPT-2 small) pruning –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ—á—Ç–∏ –≤—Å–µ –≥–æ–ª–æ–≤—ã –Ω–∏–∂–Ω–∏—Ö —Å–ª–æ—ë–≤ –º–æ–∂–Ω–æ –≤—ã–∫–∏–Ω—É—Ç—å –∫—Ä–æ–º–µ 1-2 (–∫–æ—Ç–æ—Ä—ã–µ, –≤–∏–¥–∏–º–æ, –Ω–µ—Å—É—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Å–º—ã—Å–ª–æ–≤—É—é –Ω–∞–≥—Ä—É–∑–∫—É —Ç–∏–ø–∞ induction).
> 
> **Sparsity (—Ä–∞–∑—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç—å):** –ï—Å—Ç—å –ø–æ–¥—Ö–æ–¥—ã —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, *Differentiable Sparsification*: –¥–æ–±–∞–≤–ª—è—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –≤–µ—Å–∞ –Ω–∞ –∫–∞–∂–¥—É—é –≥–æ–ª–æ–≤—É –∏ L0-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é ‚Äì —á—Ç–æ–±—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Å–∞ –∑–∞–Ω—É–ª–∏–ª–∏—Å—å, effectively –æ—Ç–∫–ª—é—á–∏–≤ –≥–æ–ª–æ–≤—ã. –¢–∞–∫ –º–æ–∂–Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–π –º–µ–Ω—å—à–µ –≥–æ–ª–æ–≤. –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ —Å—Ç—Ä–∞–¥–∞–µ—Ç, –∑–Ω–∞—á–∏—Ç –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã –∏–∑–±—ã—Ç–æ—á–Ω—ã. J. Voita (2019) –¥–µ–ª–∞–ª–∏ —Ç–∞–∫–æ–π hard-concrete gating: –º–æ–¥–µ–ª—å —Å–∞–º–∞ ‚Äú–≤—ã–±—Ä–∞—Å—ã–≤–∞–ª–∞‚Äù 2/3 –≥–æ–ª–æ–≤. –û–Ω–∏ —É–≤–∏–¥–µ–ª–∏, —á—Ç–æ –æ—Å—Ç–∞—é—Ç—Å—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –≥–æ–ª–æ–≤—ã —Å –æ—á–µ–≤–∏–¥–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–∞, —á—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ç–∞, —á—Ç–æ –∑–∞ —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞) ‚Äì –æ—Å—Ç–∞–ª—å–Ω—ã–µ –±—ã–ª–∏ ‚Äú–ø–∞—Ä–∞–∑–∏—Ç–Ω—ã–µ‚Äù –∏ —É—à–ª–∏.
> 
> **–ü–æ—á–µ–º—É –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –æ—Å–æ–±–æ —Å–∫–ª–æ–Ω–Ω—ã –∫ head death:** –ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ—ë–≤) –º–æ–∂–µ—Ç –Ω–µ –∏–º–µ—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏, —á—Ç–æ–±—ã –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤—Å–µ –≥–æ–ª–æ–≤—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –Ω–∞ —è–∑—ã–∫–µ, –≥–¥–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–æ—Ä–æ—Ç–∫–∏–µ ‚Äì –º–æ–∂–Ω–æ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è –¥–≤—É–º—è-—Ç—Ä–µ–º—è —Ç–∏–ø–∞–º–∏ —Å–≤—è–∑–µ–π, –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–æ. –¢–æ–≥–¥–∞ –ª–∏—à–Ω–∏–µ –≥–æ–ª–æ–≤—ã –Ω–µ –ø–æ–ª—É—á–∞—é—Ç —Å–∏–ª—å–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∏ –æ—Å—Ç–∞—é—Ç—Å—è –±–ª–∏–∑–∫–æ –∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (–∫–æ—Ç–æ—Ä–∞—è —á–∞—Å—Ç–æ –¥–∞—ë—Ç –ø–æ—á—Ç–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ). –¢–∞–∫ –∏ –∑–∞—Å—Ç—ã–≤–∞—é—Ç. –í –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö –∑–∞–¥–∞—á –±–æ–ª—å—à–µ ‚Äì –ø–æ—á—Ç–∏ –≤—Å–µ –≥–æ–ª–æ–≤—ã –Ω–∞—Ö–æ–¥—è—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ.
> 
> **"Dead head" –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã:** (1) Attention entropy –±–ª–∏–∑–∫–∞ –∫ –º–∞–∫—Å (–∑–Ω–∞—á–∏—Ç, –≥–æ–ª–æ–≤–∞ –Ω–µ –≤—ã–¥–µ–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∞ –ø–æ—á—Ç–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞). (2) –ù–∏–∑–∫–∞—è –Ω–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü \(W^O\) –¥–ª—è —ç—Ç–æ–π –≥–æ–ª–æ–≤—ã ‚Äì –∑–Ω–∞—á–∏—Ç, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∞ —á—Ç–æ-—Ç–æ —Å—á–∏—Ç–∞–µ—Ç, –µ—ë –≤—ã—Ö–æ–¥ –º–∞–ª–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –æ—Å—Ç–∞—Ç–æ–∫ (–≤—ã—Ö–æ–¥–Ω–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞–ª). –ò–Ω–æ–≥–¥–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–º–æ—Ç—Ä—è—Ç –º–∞—Ç—Ä–∏—Ü—É \(W^O\): –µ—Å–ª–∏ –æ–¥–∏–Ω —Å—Ç–æ–ª–±–µ—Ü (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≥–æ–ª–æ–≤–µ) –Ω—É–ª–µ–≤–æ–π, –≥–æ–ª–æ–≤–∞ effectively dead (—Å–∏–≥–Ω–∞–ª –æ—Ç –Ω–µ—ë –∑–∞–Ω—É–ª—è–µ—Ç—Å—è).
> 
> **–í–∑–∞–∏–º–æ—Å–≤—è–∑—å —Å —Ä–∞–∑–º–µ—Ä–æ–º –º–æ–¥–µ–ª–∏:** –í –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ (100+ —Å–ª–æ—ë–≤) —Ç–æ–∂–µ –º–æ–≥—É—Ç –±—ã—Ç—å –º–µ—Ä—Ç–≤—ã–µ –≥–æ–ª–æ–≤—ã ‚Äì –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ –æ–Ω–∏ –∑–∞–ª–æ–∂–µ–Ω—ã –æ–¥–Ω–æ—Ç–∏–ø–Ω–æ. –ù–æ —Ç–∞–º –∏–∑-–∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤, —á—Ç–æ –≤—Å–µ —á—Ç–æ-—Ç–æ –¥–µ–ª–∞—é—Ç. –ê –≤–æ—Ç ‚Äú–º–∞–ª–µ–Ω—å–∫–∏–µ, –Ω–æ —Å–ª–∏—à–∫–æ–º —à–∏—Ä–æ–∫–∏–µ‚Äù (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2 —Å–ª–æ—è, 16 –≥–æ–ª–æ–≤) ‚Äì —Ç–∞–º —è–≤–Ω–æ —á–∞—Å—Ç—å –≥–æ–ª–æ–≤ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è. –í –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ –ø—Ä–µ–¥–ª–∞–≥–∞–ª–∏: –ª—É—á—à–µ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å depth, –∞ —à–∏—Ä–∏–Ω—É (heads) –¥–µ—Ä–∂–∞—Ç—å —É–º–µ—Ä–µ–Ω–Ω–æ–π.
> 
> **Case study:** Suppose we have a 4-layer 4-head model, trained on simple English to pig-latin translation (–æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω). –í–æ–∑–º–æ–∂–Ω–æ –≤—ã—è—Å–Ω–∏—Ç—Å—è, —á—Ç–æ: Layer1 ‚Äì 4 –≥–æ–ª–æ–≤—ã, –Ω–æ 3 –∏–∑ –Ω–∏—Ö –Ω–µ –Ω—É–∂–Ω—ã (—Å–ª–æ–≤–∞ –ø–æ –ø–æ–∑–∏—Ü–∏—è–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è, –æ–¥–Ω—É –≥–æ–ª–æ–≤—É —Ö–≤–∞—Ç–∞–µ—Ç –Ω–∞ –≤—Å—ë). Layer2 ‚Äì 1-2 –∞–∫—Ç–∏–≤–Ω—ã—Ö, etc. –û—Å—Ç–∞–≤—à–∏–µ—Å—è 2-3 –∏–∑ 16 ‚Äì effectively ‚Äúdead‚Äù. –ü—Ä—É–Ω–Ω–∏–Ω–≥ –∏—Ö —É–ª—É—á—à–∏—Ç —Å–∫–æ—Ä–æ—Å—Ç—å, –Ω–µ —É—Ö—É–¥—à–∏–≤ –ø–µ—Ä–µ–≤–æ–¥.
> 
> **–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ —è–≤–ª–µ–Ω–∏–µ ‚Äì negative heads:** –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö, –ø–æ–º–∏–º–æ –º—ë—Ä—Ç–≤—ã—Ö (–±–µ—Å–ø–æ–ª–µ–∑–Ω—ã—Ö) –≥–æ–ª–æ–≤ –µ—Å—Ç—å ‚Äú–≤—Ä–µ–¥–Ω—ã–µ‚Äù –≥–æ–ª–æ–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ *—É–ª—É—á—à–∞—é—Ç* –∫–∞—á–µ—Å—Ç–≤–æ. Michel et al. –æ—Ç–º–µ—á–∞–ª–∏, —á—Ç–æ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≥–æ–ª–æ–≤ accuracy *—Ä–æ—Å–ª–∞* –Ω–∞ –∏—Ö –≤–∞–ª–∏–¥. –≠—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ, –µ—Å–ª–∏ –≥–æ–ª–æ–≤–∞ —É—á–∏–ª–∞ spurious correlation –∏–ª–∏ —à—É–º. –¢–∞–∫—É—é –º–æ–∂–Ω–æ —Ç–æ—á–Ω–æ —Å—á–∏—Ç–∞—Ç—å ‚Äú–ø–ª–æ—Ö–æ–π‚Äù. –ï–µ —Ç–æ–∂–µ –º–æ–∂–Ω–æ –≤—ã–∫–∏–Ω—É—Ç—å ‚Äì –º–æ–¥–µ–ª—å —Å—Ç–∞–ª–∞ –ª—É—á—à–µ. –¢–∞–∫–æ–µ —Ä–µ–¥–∫–æ, –Ω–æ –±—ã–ª–æ.
> 
> **Sparsity by design:** –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–æ–≤—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤–≤–æ–¥—è—Ç *learned sparsity*, –≥–¥–µ –Ω–µ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –æ–±–º–µ–Ω–∏–≤–∞—é—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π ‚Äì –º–æ–∂–µ—Ç, chunk-wise attention (Longformer) –∏–ª–∏ router, –æ—Ç–∫–ª—é—á–∞—é—â–∏–π –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ heads –¥–ª—è —Ç–æ–∫–µ–Ω–∞. –≠—Ç–æ —Å–∫–æ—Ä–µ–µ –ø—Ä–æ sparse patterns, –Ω–µ –ø—Ä–æ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –≥–æ–ª–æ–≤. –ù–æ –µ—Å—Ç—å Hydra attention (2022) ‚Äì –∏–¥–µ—è –∏–º–µ—Ç—å –º–Ω–æ–≥–æ –≥–æ–ª–æ–≤, –Ω–æ –≤ –∫–∞–∂–¥–æ–º —Ç–æ–∫–µ–Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ k –∏–∑ –Ω–∏—Ö –ø–æ –Ω–∞–∏–≤—ã—Å—à–∏–º Query-Key scores„Äê29‚Ä†L28-L32„Äë. –≠—Ç–æ –∫–∞–∫ dynamic sparsity ‚Äì –ø–æ —Å—É—Ç–∏, –¥–∞–µ—Ç –≥–ª–∞–≤–µ–Ω—Å—Ç–≤–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º –≥–æ–ª–æ–≤–∞–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äúidle‚Äù –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.
> 
> **–ß—Ç–æ —Å –Ω–∏–º–∏ –¥–µ–ª–∞—Ç—å:** M—ë—Ä—Ç–≤—ã–µ –≥–æ–ª–æ–≤—ã ‚Äì —Å–∏–≥–Ω–∞–ª, —á—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –º–æ–∂–Ω–æ –∏—Ö –≤—ã—Ä–µ–∑–∞—Ç—å –∏ —Å–¥–µ–ª–∞—Ç—å –º–æ–¥–µ–ª—å –º–µ–Ω—å—à–µ. –≠—Ç–æ —É–∂–µ –ø—Ä–∏–º–µ–Ω—è–ª–∏: –Ω–∞–ø—Ä–∏–º–µ—Ä, pruned BERTs. –í –±–æ–ª—å—à–∏—Ö LLMs —ç—Ç–æ –ø–æ–∫–∞ –Ω–µ –æ—á–µ–Ω—å –¥–µ–ª–∞–µ—Ç—Å—è, —Ç.–∫. —Ç—é–Ω–∏–Ω–≥ –∏—Ö —Å–ª–æ–∂–µ–Ω, –Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ –º–æ–∂–Ω–æ 20% –≥–æ–ª–æ–≤ LLaMA2 —É–±—Ä–∞—Ç—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º ‚Äì —ç—Ç–æ —É—Å–∫–æ—Ä–∏—Ç inference. –ù–æ –µ—Å—Ç—å —Ä–∏—Å–∫: —ç—Ç–∏ 20% –º–æ–≥—É—Ç –Ω–µ –Ω—É–∂–Ω—ã –Ω–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é, –Ω–æ –º–æ–≥—É—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ —Ä–µ–¥–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ (alignment?).
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** –ú—ë—Ä—Ç–≤—ã–µ –≥–æ–ª–æ–≤—ã ‚Äì –∫–∞–∫ —Ä—É–¥–∏–º–µ–Ω—Ç–∞—Ä–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã —ç–≤–æ–ª—é—Ü–∏–∏ —Å–µ—Ç–∏. –û–Ω–∏ –∑–∞–∫–æ—Å—Ç–µ–Ω–µ–ª–∏, –Ω–µ –Ω–∞—à–ª–∏ —Ñ—É–Ω–∫—Ü–∏–∏, –Ω–æ –æ—Å—Ç–∞–ª–∏—Å—å, –ø–æ—Ç–æ–º—É —á—Ç–æ ‚Äú–Ω–µ –º–µ—à–∞—é—Ç‚Äù. –ö–∞–∫ –∞–ø–ø–µ–Ω–¥–∏–∫—Å: –≤—Ä–æ–¥–µ –Ω–µ –Ω—É–∂–µ–Ω, –Ω–æ –µ—Å—Ç—å. –° —É–¥–∞–ª–µ–Ω–∏–µ–º —Ç–∞–∫–∏—Ö ‚Äú–æ—Ä–≥–∞–Ω–æ–≤‚Äù –º–æ–¥–µ–ª—å –Ω–µ —Å—Ç—Ä–∞–¥–∞–µ—Ç, –∏–Ω–æ–≥–¥–∞ –¥–∞–∂–µ –º–µ–Ω—å—à–µ —Å–∏–ª —Ç—Ä–∞—Ç–∏—Ç. –≠—Ç–æ –Ω–∞–≤–æ–¥–∏—Ç –Ω–∞ –º—ã—Å–ª—å: –º–æ–∂–Ω–æ –ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ‚Äú—É–ø–ª–æ—Ç–Ω—è—Ç—å‚Äù –º–æ–∑–≥ –º–æ–¥–µ–ª–∏, —É–±–∏—Ä–∞—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ ‚Äì —Ç–∞–∫, –º–æ–∑–≥ –∂–∏–≤–æ—Ç–Ω—ã—Ö –≤ —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–æ–∂–µ –∏–∑–±–∞–≤–ª—è–ª—Å—è –æ—Ç –Ω–µ–Ω—É–∂–Ω–æ–≥–æ. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ, *Neural network pruning* ‚Äì —Ü–µ–ª–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∏ –≥–æ–ª–æ–≤—ã ‚Äì —É–¥–æ–±–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ pruning. 
> 
> **–°—Å—ã–ª–∫–∏:** Michel et al. 2019„Äê20‚Ä†L207-L212„Äë; Voita et al. 2019 ACL (head importance, gating); Ansatz on dead heads in ALBERT etc.
> 
> ## 13. **–ö–∞–∫ rotary / relative / ALiBi positional encoding –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–Ω–∏–º–∞–Ω–∏–µ** 
> **–ö—Ä–∞—Ç–∫–æ –æ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∫–∞—Ö:** –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—É –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤, –∏–Ω–∞—á–µ self-attention —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ. *Absolute positional encoding* (–∫–∞–∫ —É –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ Transformer: —Å–∏–Ω—É—Å–æ–∏–¥—ã –∏–ª–∏ learnt embeddings) –¥–æ–±–∞–≤–ª—è–µ—Ç –∫ –≤—Ö–æ–¥–∞–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Ö –∏–Ω–¥–µ–∫—Å–µ. *Relative positional encoding* (Shaw et al. 2018, T5 –∏ –¥—Ä.) –∏–Ω–∞—á–µ: –æ–Ω–∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –¥–æ–±–∞–≤–ª—è—è —Å–º–µ—â–µ–Ω–∏–µ –≤ score –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–Ω–∏—Ü—ã –ø–æ–∑–∏—Ü–∏–π. *Rotary (RoPE)* ‚Äì —Å–ø–æ—Å–æ–± –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –≤–Ω—É—Ç—Ä–∏ Q –∏ K —á–µ—Ä–µ–∑ –≤—Ä–∞—â–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç. *ALiBi* ‚Äì Attention with Linear Biases ‚Äì –¥–æ–±–∞–≤–ª—è–µ—Ç –∫ –∫–∞–∂–¥–æ–º—É score \(q\cdot k\) —Å–¥–≤–∏–≥, –ª–∏–Ω–µ–π–Ω–æ —É–º–µ–Ω—å—à–∞—é—â–∏–π—Å—è —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º (–∏ —Ä–∞–∑–Ω—ã–π –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã)„Äê34‚Ä†L1-L4„Äë.
> 
> **–í–ª–∏—è–Ω–∏–µ –Ω–∞ –≤–Ω–∏–º–∞–Ω–∏–µ:**
> 
> - **Rotary (RoPE):** –û–Ω –≤–≤–æ–¥–∏—Ç –≤ Q,K —Ç–∞–∫—É—é —Ñ–∞–∑–∏—Ä—É—é—â—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é, —á—Ç–æ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ \(q_i k_j\) —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç \(\cos(\theta (i-j))\) (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π)„Äê33‚Ä†L292-L300„Äë„Äê33‚Ä†L318-L327„Äë. –≠—Ç–æ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –≥–æ–ª–æ–≤ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Ä–∞–∑–Ω–∏—Ü—ã –ø–æ–∑–∏—Ü–∏–π: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –º–æ–≥—É—Ç –Ω–∞—É—á–∏—Ç—å—Å—è —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å —Å–∏–ª—å–Ω–æ –Ω–∞ –Ω—É–ª–µ–≤—É—é —Ä–∞–∑–Ω–∏—Ü—É (—Å–≤–æ–∏ –ø–æ–∑–∏—Ü–∏–∏), –¥—Ä—É–≥–∏–µ ‚Äì –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ offsets. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ ‚ÄúRound and Round we go: What is RoPE doing?‚Äù (Wang & Komatsuzaki, 2023) –æ—Ç–º–µ—á–∞–µ—Ç, —á—Ç–æ —Å RoPE –ø–æ—è–≤–ª—è—é—Ç—Å—è *–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã* ‚Äì –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–º —Å–º–µ—â–µ–Ω–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç—Ä–æ–≥–æ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ–∫–µ–Ω). –¶–∏—Ç–∞—Ç–∞: ‚Äú–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –ø—Ä–∏ RoPE —Å–º–æ—Ç—Ä—è—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Å–µ–º–∞–Ω—Ç–∏–∫—É‚Äù„Äê32‚Ä†L15-L19„Äë. –¢.–µ. RoPE –º–æ–∂–µ—Ç —É—Å–∏–ª–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≥–æ–ª–æ–≤ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é: –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ = bigram head (i->i-1), –¥—Ä—É–≥–∞—è = trigram head (i->i-2), –∏ —Ç.–¥. –ë–µ–∑ RoPE –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å–∏–Ω—É—Å–æ–∏–¥—ã –Ω–µ –¥–∞–≤–∞–ª–∏ —Ç–∞–∫–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è offset, —Ç–∞–∫ —è–≤–Ω–æ. RoPE —Ç–∞–∫–∂–µ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª—è–º –æ–±–æ–±—â–∞—Ç—å –Ω–∞ –¥–ª–∏–Ω—ã > —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ‚Äì –≥–æ–ª–æ–≤—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç —Ñ–æ–∫—É—Å, –¥–∞–∂–µ –µ—Å–ª–∏ –ø–æ–∑–∏—Ü–∏—è –±–æ–ª—å—à–∞—è (–æ–Ω–∏ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ä–∞–∑–Ω–∏—Ü—ã). *–í—ã–≤–æ–¥:* rotary –¥–µ–ª–∞–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã *–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º–∏* ‚Äì –æ–Ω–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞—é—Ç —à–∞–±–ª–æ–Ω—ã —Ä–∏—Ç–º–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
> 
> - **Relative positional (Shaw, T5):** –¢–∞–º –≤–≤–µ–¥–µ–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ *relative position embeddings* \(a_{i-j}\), –∫–æ—Ç–æ—Ä—ã–µ –≤–ª–∏—è—é—Ç –Ω–∞ attention score. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≤–Ω–∏–º–∞–Ω–∏–µ = content-based + bias(distance). –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —Å–≤–æ–π –Ω–∞–±–æ—Ä bias –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π. –í T5 –≥–æ–ª–æ–≤—ã —è–≤–Ω–æ –¥–µ–ª–∏–ª–∏—Å—å: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ–ª–∏ bias, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –±–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ (–≤–Ω–∏–º–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ), –¥—Ä—É–≥–∏–µ ‚Äì –±–æ–ª–µ–µ –ø–ª–æ—Å–∫–æ–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, relative encodings –ø–æ–∑–≤–æ–ª–∏–ª–∏ *—Ä–∞–∑–Ω—ã–º –≥–æ–ª–æ–≤–∞–º –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ –¥–∞–ª—å–Ω–æ—Å—Ç–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏*. –í–∞–Ω–∏–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ –∫–æ–¥–∞–º–∏ —Ç–æ–∂–µ –º–æ–∂–µ—Ç —ç—Ç–æ, –Ω–æ —Ç—Ä—É–¥–Ω–µ–µ ‚Äì –Ω—É–∂–Ω–æ —á–µ—Ä–µ–∑ W^Q, W^K –≤—ã—É—á–∏—Ç—å, –∞ relative –¥–∞—ë—Ç –Ω–∞–ø—Ä—è–º—É—é. –í –∏—Ç–æ–≥–µ, –≤ –º–æ–¥–µ–ª—è—Ö —Å relative encoding (T5) –∑–∞–º–µ—Ç–∏–ª–∏, —á—Ç–æ –±–æ–ª—å—à–µ –≥–æ–ª–æ–≤ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏, –Ω–µ –æ—Ç–≤–ª–µ–∫–∞—è—Å—å –Ω–∞ –ø–æ–∑–∏—Ü–∏—é ‚Äì –ø–æ—Ç–æ–º—É —á—Ç–æ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π bias —É–∂–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, T5 –º–µ–Ω—å—à–µ –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ dedicated ‚Äúprevious token heads‚Äù ‚Äì bias —Å–∞–º —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å. 
> 
> - **ALiBi:** –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π bias: –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã –∑–∞–¥–∞–Ω slope (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1.0, 0.5, 0.25,...), –∫–æ—Ç–æ—Ä—ã–π —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ j-i –∏ –≤—ã—á–∏—Ç–∞–µ—Ç—Å—è –∏–∑ score„Äê34‚Ä†L1-L4„Äë. –¢–æ –µ—Å—Ç—å –≥–æ–ª–æ–≤–∞ —Å –±–æ–ª—å—à–∏–º slope penalizes –¥–∞–ª—å–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è —Å–∏–ª—å–Ω–µ–µ => –æ–Ω–∞ short-range head, –∞ —Å –º–∞–ª–µ–Ω—å–∫–∏–º slope ‚Äì long-range. –≠—Ç–∏ bias –Ω–µ –æ–±—É—á–∞—é—Ç—Å—è, –∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏. *–í–ª–∏—è–Ω–∏–µ:* ALiBi –±—É–∫–≤–∞–ª—å–Ω–æ *–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≥–æ–ª–æ–≤ –ø–æ –¥–∞–ª—å–Ω–æ—Å—Ç–∏* ‚Äì –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ —Å—Ç–∞–Ω—É—Ç ‚Äú–ª–æ–∫–∞–ª—å–Ω—ã–º–∏‚Äù (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç –¥–∞–ª–µ–∫–æ), –ø–æ—Å–ª–µ–¥–Ω–∏–µ ‚Äì ‚Äú–≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏‚Äù (–ø–æ—á—Ç–∏ –±–µ–∑ —à—Ç—Ä–∞—Ñ–∞). Empirically, Press et al. (–∞–≤—Ç–æ—Ä—ã ALiBi) —Å–æ–æ–±—â–∏–ª–∏, —á—Ç–æ –¥–∞–∂–µ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –≥–æ–ª–æ–≤—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è: model learns to use high-slope heads for local syntax, low-slope for semantic context. –¢–æ –µ—Å—Ç—å ALiBi —Å–Ω–∏–∂–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É: –Ω–µ –Ω—É–∂–Ω–æ —Ç—Ä–∞—Ç–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —á—Ç–æ–±—ã –≥–æ–ª–æ–≤–∞ —Å—Ç–∞–ª–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π ‚Äì –æ–Ω–∞ —É–∂–µ –ª–æ–∫–∞–ª—å–Ω–∞ –ø–æ –¥–∏–∑–∞–π–Ω—É. –≠—Ç–æ –ø–æ–º–æ–≥–ª–æ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ö –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: model —Å—Ä–∞–∑—É —É–º–µ–µ—Ç —É—Ä–∞–≤–Ω–æ–≤–µ—à–∏–≤–∞—Ç—å –±–ª–∏–∑–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç vs –¥–∞–ª—å–Ω–∏–π„Äê34‚Ä†L1-L4„Äë. 
> 
>  –ï—â–µ —ç—Ñ—Ñ–µ–∫—Ç: ALiBi —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å absolute embeddings, —Ç–∞–∫ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ –¥–ª–∏–Ω–Ω–æ. 
>  –° —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è specialization: ALiBi kind of *hard-codes* —á–∞—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏. –ù–∞ —Å—Ç–∞—Ä—Ç–µ –æ–±—É—á–µ–Ω–∏—è –≥–æ–ª–æ–≤—ã —É–∂–µ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è: head0: slope1.0 (—Å–º–æ—Ç—Ä–∏—Ç –ø–æ—á—Ç–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ 1-2 —Å–ª–æ–≤–∞ —Ä—è–¥–æ–º), head7: slope0.1 (–≤–∏–¥–∏—Ç –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—á—Ç–∏ —Ä–∞–≤–Ω–æ–ø—Ä–∞–≤–Ω–æ). –ü–æ—Ç–æ–º—É –¥–∞–∂–µ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö head0 –±—ã—Å—Ç—Ä–æ —É—á–∏—Ç –≥—Ä–∞–º–º–∞—Ç–∏–∫—É (—Ç.–∫. –ª–æ–∫–∞–ª—å–Ω–æ), –∞ head7 ‚Äì –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã. –ë–µ–∑ ALiBi, –º–æ–¥–µ–ª—å –±—ã —Å–∞–º–∞ —Ç–∞–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏–ª–∞, –Ω–æ, –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —Ç–∞–∫ —á—ë—Ç–∫–æ.
> 
> - **Absolute sin/cos vs learned abs:** –û–Ω–∏ –Ω–µ –¥–∞–≤–∞–ª–∏ —è–≤–Ω—ã—Ö head-specific patterns, –≤—Å–µ –≥–æ–ª–æ–≤—ã –ø–æ–ª—É—á–∞–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Å–∏–≥–Ω–∞–ª –ø–æ–∑–∏—Ü–∏–π (—á–µ—Ä–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫ embedding). –í Transformers 2017 –Ω–∞–±–ª—é–¥–∞–ª–∏, —á—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏—è—Ö ‚Äì –Ω–∞–ø—Ä., head, –≤—Å–µ–≥–¥–∞ —Å–º–æ—Ç—Ä—è—â–∞—è –Ω–∞ —Ç–æ–∫–µ–Ω #0 (–Ω–∞—á–∞–ª–æ) –∏–ª–∏ –Ω–∞ —Ç–æ–∫–µ–Ω #n (–∫–æ–Ω–µ—Ü) ‚Äì —Ç–æ –µ—Å—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —É–∫–∞–∑–∞—Ç–µ–ª–∏. Relative/ALiBi —Ç–∞–∫–∏–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –≤–µ—â–∏ —É–±–∏—Ä–∞—é—Ç (–Ω–µ—Ç —Å–ø–æ—Å–æ–±–∞ —Ä–∞–∑–ª–∏—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å, —Ç–æ–ª—å–∫–æ —Ä–∞–∑–Ω–∏—Ü—ã). –ü–æ—ç—Ç–æ–º—É, —Å–∫–∞–∂–µ–º, head ‚Äú–Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è‚Äù –±–æ–ª–µ–µ –∑–∞—Ç—Ä—É–¥–Ω–µ–Ω–∞ (–≤ RoPE –º–æ–∂–Ω–æ –Ω–∞–≤–µ—Ä–Ω–æ–µ –≤—ã—É—á–∏—Ç—å —á–µ—Ä–µ–∑ —Ñ–∞–∑—É 0 difference = beginning?). –í ALiBi ‚Äì –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—Å—ë —Ä–∞–≤–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ: one head with minimal bias might tend to attend to position0 a bit more if needed? –ù–µ —Ç–∞–∫ –ø—Ä–æ—Å—Ç–æ, –∏–±–æ bias linear.
> 
> **Rotary specifics:** –û—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ RoPE –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ *—ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å*, –Ω–æ —Å –æ–¥–Ω–∏–º —Ç–æ–Ω–∫–∏–º —è–≤–ª–µ–Ω–∏–µ–º: –¥–ª—è –ø–æ–∑–∏—Ü–∏–π > –æ–±—É—á–µ–Ω–Ω—ã—Ö, attention pattern –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏. –û–¥–Ω–∞–∫–æ Li et al. 2023 ‚ÄúBreaking RoPE for extreme length‚Äù –∑–∞–º–µ—Ç–∏–ª–∏, —á—Ç–æ RoPE –ø—Ä–∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö (>10k) –Ω–∞—á–∏–Ω–∞–µ—Ç –≤—Ä–µ–¥–∏—Ç—å ‚Äì –º–æ–∂–µ—Ç –≤–≤–æ–¥–∏—Ç—å –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å. *Effect on heads:* Heads might become periodic-sensitive: if sequence too long, angles in RoPE wrap (–Ω–æ –æ–±—ã—á–Ω–æ –º–∞—Å—à—Ç–∞–± —á–∞—Å—Ç–æ—Ç –ø–æ–¥–æ–±—Ä–∞–Ω, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å—Å—è —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ).
> 
> **Position affects head life:** –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ heads heavily reliant on absolute positions –º–æ–≥—É—Ç ‚Äú—É–º–∏—Ä–∞—Ç—å‚Äù –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å: –Ω–∞–ø—Ä., head, —á—Ç–æ –≤—Å–µ–≥–¥–∞ —Å–º–æ—Ç—Ä–µ–ª–∞ –Ω–∞ 512-—ã–π —Ç–æ–∫–µ–Ω (–∫–æ–Ω–µ—Ü –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏), –≤–Ω–µ —ç—Ç–æ–≥–æ –æ–Ω–∞ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞. Relative –∏ ALiBi –∏–∑–±–∞–≤–ª—è—é—Ç –æ—Ç —Ç–∞–∫–∏—Ö ‚Äú–∫—Ä–∞–π–Ω–∏—Ö‚Äù –≥–æ–ª–æ–≤ ‚Äì –≤—Å–µ –≥–æ–ª–æ–≤—ã —Ç–µ–ø–µ—Ä—å –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã –Ω–∞ –ª—é–±–æ–π –¥–ª–∏–Ω–µ.
> 
> **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:** Survey 2024 –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –ø—Ä–∏ relative/rotary encoding, attention patterns —Å—Ç–∞–ª–∏ –±–æ–ª–µ–µ *–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–º–∏* (–∑–Ω–∞—á–∏—Ç, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏)„Äê32‚Ä†L15-L19„Äë. –ò ALiBi: —É –∞–≤—Ç–æ—Ä–æ–≤ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫: Heads with larger slopes concentrate on nearer tokens strongly, with far tokens nearly zero weight.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** Positional encoding ‚Äì —ç—Ç–æ –∫–∞–∫ ‚Äú—Å–µ—Ç–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞-–≤—Ä–µ–º–µ–Ω–∏‚Äù –¥–ª—è –≤–Ω–∏–º–∞–Ω–∏—è. –†–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ª–∏–±–æ —Ç–≤—ë—Ä–¥–æ –∑–∞–¥–∞—é—Ç —Ñ–∏–∑–∏–∫—É (ALiBi: ‚Äú–±–ª–∏–∑–∫–æ–µ —Å–∏–ª—å–Ω–µ–µ –¥–∞–ª—å–Ω–µ–≥–æ‚Äù), –ª–∏–±–æ –¥–∞—é—Ç –±–∞–∑–∏—Å (RoPE: ‚Äú—É–≥–ª–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞, –∫–∞–∫ –≤–æ–ª–Ω—ã‚Äù) –∏ –º–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –∫–∞–∫ ‚Äú—Å–ª—ã—à–∞—Ç—å‚Äù —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è. –ü–æ–ª—É—á–∞–µ—Ç—Å—è, —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≥–æ–ª–æ–≤: –∫–∞–∫ –µ—Å–ª–∏ –± –º—ã –º–æ–∑–≥—É –≤—Å—Ç—Ä–æ–∏–ª–∏ bias ‚Äì –æ–¥–∏–Ω –≥–ª–∞–∑ –≤–∏–¥–∏—Ç –±–ª–∏–∂–Ω–∏–π —Ñ–æ–∫—É—Å, –¥—Ä—É–≥–æ–π –¥–∞–ª—å–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç ‚Äì –±–æ–ª–µ–µ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä—É–¥–∞. Engineering perspective: –º—ã —Å–ª–µ–≥–∫–∞ ‚Äú—É–ø—Ä–∞–≤–ª—è–µ–º‚Äù –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∂–∏–∑–Ω—å—é –≥–æ–ª–æ–≤ —á–µ—Ä–µ–∑ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ priors.
> 
> **–°—Å—ã–ª–∫–∏:** Su et al. 2021 (RoPE)„Äê33‚Ä†L318-L327„Äë; Press et al. 2022 (ALiBi)„Äê34‚Ä†L1-L4„Äë; Blog on Rotary showing diagonal heads„Äê32‚Ä†L15-L19„Äë.
> 
> ## 14. **–ß—Ç–æ —É–º–µ—é—Ç attention heads –≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö (GPT-3, 4, Claude, Mistral, Qwen)** 
> **–û–±—â–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏:** –° —Ä–æ—Å—Ç–æ–º —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∫–∞–∫ —Ä–∞–∑–≤–∏—Ç–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π, —Ç–∞–∫ –∏ –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö, –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤. –ö—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–º–Ω–æ–≥–æ —Å–ª–æ—ë–≤) –∏–º–µ—é—Ç –±–æ–ª—å—à–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏: –Ω–∏–∂–Ω–∏–µ –≥–æ–ª–æ–≤—ã —Ä–µ—à–∞—é—Ç –ª–µ–∫—Å–∏–∫–æ-—Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Å—Ä–µ–¥–Ω–∏–µ ‚Äì ‚Äú–º—ã—Å–ª—è—Ç‚Äù –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏, –≤–µ—Ä—Ö–Ω–∏–µ ‚Äì –≥–æ—Ç–æ–≤—è—Ç –æ—Ç–≤–µ—Ç. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:
> 
> - **Emergent heads:** –í –º–æ–¥–µ–ª—è—Ö –æ–∫–æ–ª–æ –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã—à–µ –ø–æ—è–≤–∏–ª–∏—Å—å induction heads (–∫–∞–∫ –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å). GPT-3 (175B) —è–≤–Ω–æ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç ‚Äì –∏–º–µ–Ω–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—è –∏–º GPT-3 –ø–æ–∫–∞–∑–∞–ª –Ω–∞–º–Ω–æ–≥–æ –ª—É—á—à—É—é few-shot —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å„Äê35‚Ä†L58-L66„Äë. –£ GPT-2 –¥–æ ~10^8 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã induction –±—ã–ª–∏ —Å–ª–∞–±—ã–µ, –∞ —É 10^9+ ‚Äì —è–≤–Ω—ã–µ.
> 
> - **–ù–æ–≤—ã–µ –≤–∏–¥—ã –∑–∞–¥–∞—á:** –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –∫–æ–¥, –º–∞—Ç–µ–º–∞—Ç–∏–∫—É, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç –Ω–∞–ª–∏—á–∏–µ –≥–æ–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç, –∫ –ø—Ä–∏–º–µ—Ä—É, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–¥–∞ (–æ—Ç—Å—Ç—É–ø—ã, —Å–∫–æ–±–∫–∏), –∏–ª–∏ —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å heads, –ø–æ–º–æ–≥–∞—é—â–∏–µ Chain-of-Thought, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–≤—è–∑—ã–≤–∞—é—Ç ‚ÄúFirst,‚Äù ‚ÄúSecond,‚Äù —à–∞–≥–∏). –•–æ—Ç—è —ç—Ç–æ —Å–ª–æ–∂–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é, –∫–æ—Å–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: –º–æ–¥–µ–ª—å Qwen-14B (Alibaba, 2023) –∏–º–µ–µ—Ç –≤—ã–¥–∞—é—â–∏–µ—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤ –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–µ, –≤–æ–∑–º–æ–∂–Ω–æ –æ–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ ‚ÄúArithmetic heads‚Äù (Anthropic —Å–æ–æ–±—â–∞–ª –æ head‚Äô–∞—Ö, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏—Ö —á–∏—Å–ª–∞ –∫–∞–∫ —Å–ø–∏—Ä–∞–ª—å –Ω–∞ –∫—Ä—É–≥–µ„Äê4‚Ä†L31-L39„Äë). 
>  Nature Communications 2024 –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –≤ GPT-2/3 —Å—É—â–µ—Å—Ç–≤—É—é—Ç heads, —á—å—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤ –º–æ–∑–≥–∞ –ø—Ä–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ —Ä–µ—á–∏„Äê18‚Ä†L91-L99„Äë„Äê18‚Ä†L93-L99„Äë. –û–Ω–∏ –Ω–∞—à–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã: heads –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ—ë–≤ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å —Å–µ–Ω—Å–æ—Ä–Ω—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏, heads –ø–æ–∑–¥–Ω–∏—Ö ‚Äì —Å –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏. –≠—Ç–æ –∫–æ—Å–≤–µ–Ω–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç: –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞–∑–≤–∏–≤–∞—é—Ç –≥–æ–ª–æ–≤—ã, –æ—Ç—Ä–∞–∂–∞—é—â–∏–µ —Ä–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è (–æ—Ç —Ñ–æ–Ω–æ–ª–æ–≥–∏–∏ –¥–æ —Å–º—ã—Å–ª–∞).
> 
> - **GPT-3 vs GPT-4:** GPT-4 –∑–∞–∫—Ä—ã—Ç—ã–π, –Ω–æ –∏–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ –æ–Ω –≥–æ—Ä–∞–∑–¥–æ –≥–ª—É–±–∂–µ (–º–Ω–æ–≥–æ —Å–ª–æ—ë–≤). –ú–æ–∂–Ω–æ –æ–∂–∏–¥–∞—Ç—å, —á—Ç–æ —É GPT-4 –ø–æ—è–≤–ª—è–µ—Ç—Å—è –µ—â—ë –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è. –í–æ–∑–º–æ–∂–Ω–æ, *compound heads* ‚Äì –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å–æ–æ–±—â–∞ –≤ 3+ —Å–ª–æ—è—Ö (–¥–ª–∏–Ω–Ω—ã–µ circuit). –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–∏ GPT-4, –≤–æ–∑–º–æ–∂–Ω–æ, –∏–º–µ–µ—Ç —Ü–µ–ø–æ—á–∫—É heads: –æ–¥–Ω–∞ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ñ–∞–∫—Ç—ã, –¥—Ä—É–≥–∞—è ‚Äì —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç, —Ç—Ä–µ—Ç—å—è ‚Äì —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤—ã–≤–æ–¥. –≠—Ç–æ –º—ã –≤–∏–¥–∏–º –≤ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞—Ö chain-of-thought: likely certain heads in mid-layers ensure consistency across steps (Anthropic‚Äôs paper on reasoning tasks hints at cooperating heads for multi-step inference).
> 
> - **Claude (Anthropic):** Claude 2 (–µ–º—É ~52B) –æ–±—É—á–µ–Ω —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –¥–∏–∞–ª–æ–≥–∏ –∏ ‚Äúharmlessness‚Äù. –í–æ–∑–º–æ–∂–Ω–æ –µ—Å—Ç—å safety-related heads: –Ω–∞–ø—Ä–∏–º–µ—Ä, head, —Ä–∞—Å–ø–æ–∑–Ω–∞—é—â–∞—è –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è –∏–ª–∏ sensitive content ‚Äì –æ–±—É—á–µ–Ω–Ω–∞—è RLHF. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ Unsafe behaviors (OpenAI) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–∂–Ω–æ –≤ –º–æ–¥–µ–ª—è—Ö –Ω–∞–π—Ç–∏ ‚Äútrigger‚Äù neurons/heads. Anthropic —É–ø–æ–º–∏–Ω–∞–ª ‚Äúsycophancy‚Äù circuits (–ø–æ–¥–¥–∞–∫–∏–≤–∞–Ω–∏–µ), –≤–æ–∑–º–æ–∂–Ω–æ, —É –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ –≥–æ–ª–æ–≤—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ–≥–¥–∞ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞—é—â–∏–µ—Å—è –ø–æ–¥ user. –ü–æ—Å–ª–µ RLHF, –º–æ–∂–µ—Ç, —Ç–µ –≥–æ–ª–æ–≤—ã –ø–æ–¥–∞–≤–ª–µ–Ω—ã.
> 
> - **Mistral 7B:** –ù–æ–≤–∞—è –Ω–µ–±–æ–ª—å—à–∞—è, –Ω–æ –æ—á–µ–Ω—å –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –í 7B —Ç–æ–∂–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ induction heads –∏ –ø—Ä–æ—á–µ–µ, –µ—Å–ª–∏ —Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω. Mistral –∑–∞—è–≤–∏–ª–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ pruned some heads. –ù–æ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –ø—É–±–ª–∏—á–Ω–æ. 
>  –í 7B, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –Ω–µ—Ç –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã—Ö –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤ heads, –Ω–æ –≤—Å–µ –±–∞–∑–æ–≤—ã–µ (copy, name, syntax) –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç. 
> 
> - **Qwen 14B (Alibaba):** Qwen –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª —Å–∏–ª—å–Ω—ã–µ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ (Qwen-VL). –ï—Å–ª–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å: heads, —Å–ø–æ—Å–æ–±–Ω—ã–µ –∫ –∫–∏—Ç–∞–π—Å–∫–∏–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–Ω–æ–≤—ã–µ –∏–ª–∏ –∏–¥–∏–æ–º—ã). Qwen, –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ code, –º–æ–∂–µ—Ç –∏–º–µ—Ç—å heads-—Å—á–∏—Ç—ã–≤–∞—Ç–µ–ª–∏ –æ—Ç—Å—Ç—É–ø–æ–≤. 
>  –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∫–æ–¥—ã (Codex, StarCoder) ‚Äì –∏–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ —É –Ω–∏—Ö –µ—Å—Ç—å heads, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç –±–ª–æ–∫–∏ –∫–æ–¥–∞: likely heads that match braces, indent alignment (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: Code models —É–¥–µ–ª—è—é—Ç –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–∞–º –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏, —Ç–∞–±–∞–º).
>  Qwen authors –Ω–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é heads, –Ω–æ **–ª–æ–≥–∏—á–Ω–æ:** –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –≤—ã–¥–µ–ª—è—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—É—é –¥–æ–ª—é –≥–æ–ª–æ–≤ –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –¥–æ–º–µ–Ω—ã. 
>  Multi-modal (–µ—Å–ª–∏ –±—Ä–∞—Ç—å Qwen-VL) ‚Äì —Ç–∞–º cross-attention: heads, —Å–≤—è–∑—ã–≤–∞—é—â–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —Ä–µ–≥–∏–æ–Ω–∞–º–∏. –≠—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è: heads –º–æ–≥—É—Ç, –Ω–∞–ø—Ä., —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç—å —Å–ª–æ–≤–æ ‚Äúcat‚Äù —Å –æ–±–ª–∞—Å—Ç—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∫–æ—Ç–æ–º (–∫–∞–∫ DETR-object detection).
> 
> - **Unique new pattern:** –í –±–æ–ª—å—à–∏—Ö LLM –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ *Inhibition heads*: –Ω–∞–ø—Ä–∏–º–µ—Ä, ‚ÄúNegative Name Mover‚Äù –≤ IOI ‚Äì –æ–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏„Äê37‚Ä†L395-L403„Äë„Äê37‚Ä†L397-L404„Äë. –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –±–æ–ª—å—à–µ —Ç–∞–∫–∏—Ö inhibiting heads ‚Äì —Ç–∏–ø–∞, head, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç —á—Ç–æ –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—Ç –∏ —Å–Ω–∏–∂–∞–µ—Ç –≤–µ—Å. –≠—Ç–æ –±–æ–ª–µ–µ —Ç–æ–Ω–∫–∞—è —Ä–æ–ª—å: –Ω–µ —á—Ç–æ–±—ã —É—Å–∏–ª–∏–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ, –∞ —á—Ç–æ–±—ã –ø–æ–¥–∞–≤–ª—è—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ (–∫–∞–∫ –≤ –º–æ–∑–≥–µ inhibitory neurons). 
>  –í–æ–∑–º–æ–∂–Ω–æ, –µ—Å—Ç—å heads, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª—É–∂–∞—Ç —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ ‚Äì e.g., head scanning for contradictions.
> 
> - **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ vs –∫–∞—á–µ—Å—Ç–≤–æ:** GPT-3 175B: ~96 heads per layer √ó 96 layers. –£–∂ —Ç–æ—á–Ω–æ –Ω–µ –≤—Å–µ 9216 –≥–æ–ª–æ–≤ —Ä–∞–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ‚Äì –µ—Å—Ç—å –≥—Ä—É–ø–ø—ã. –ù–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ—è—Ö –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏. 
>  GPT-4 ‚Äì –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ, –Ω–æ —Å–∫–æ—Ä–µ–µ –µ—â–µ –±–æ–ª—å—à–µ. 
>  Claude 2 ‚Äì 80L 64heads (?). 
>  Mistral 7B ‚Äì 32L 32heads. 
>  Qwen 14B ‚Äì 40L 40heads (?). 
>  –¢–æ –µ—Å—Ç—å –º–∞—Å—à—Ç–∞–± –ª–∏–Ω–µ–π–Ω–æ —Ä–∞—Å—Ç–µ—Ç.
> 
> **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —É—Å–∏–ª–∏—è –Ω–∞ GPT-3.5/4:** –ü–æ–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ, —Ç.–∫. –º–æ–¥–µ–ª–∏ –∑–∞–∫—Ä—ã—Ç—ã. –ù–æ *mechanistic interpretability* –Ω–∞ GPT-2 medium/XL ‚Äì —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É—é—Ç, —á—Ç–æ GPT-3 –ø–æ–¥–æ–±–Ω–æ, —Ç–æ–ª—å–∫–æ —á–µ—Ç—á–µ. Anthropic‚Äôs induction heads paper looked at models up to 1.3B. Redwood‚Äôs ‚ÄúAutomated Circuit Discovery‚Äù (2023) —á–∞—Å—Ç–∏—á–Ω–æ –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ç–æ–∂–µ. 
> One anecdote: GPT-3 had ‚ÄúUTC timestamp‚Äù knowledge encoded in a head ‚Äì OpenAI found a head that always outputs current date if triggered (not confirmed publicly, but rumored).
> 
> **Conclusion:** –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ç–æ —á—Ç–æ–±—ã –≤–≤–æ–¥—è—Ç –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≥–æ–ª–æ–≤, —Å–∫–æ—Ä–µ–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö. –ù–æ –∏—Ö –ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–æ–ª–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–æ–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, GPT-4 –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –æ–¥–Ω–∏ –≥–æ–ª–æ–≤—ã –ø—Ä–∏ –≤–æ–ø—Ä–æ—Å–µ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –¥—Ä—É–≥–∏–µ ‚Äì –ø—Ä–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ. 
>  –≠–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ (–∫–∞–∫ –ø–µ—Ä–µ–≤–æ–¥ –±–µ–∑ —è–≤–Ω–æ –¥–∞–Ω–Ω—ã—Ö, –∏–ª–∏ —à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ) —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –µ—Å—Ç—å *circuit of heads* –¥–ª—è –Ω–µ–≥–æ.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** –ü–æ –º–µ—Ä–µ —Ä–æ—Å—Ç–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –º—ã –≤–∏–¥–∏–º —Å–±–ª–∏–∂–µ–Ω–∏–µ —Å —á–µ–º-—Ç–æ –≤—Ä–æ–¥–µ *–º–æ–¥—É–ª—å–Ω–æ–≥–æ –º–æ–∑–≥–∞*. –í—Å–µ —Ç–µ –∫–ª–∞—Å—Å—ã –≥–æ–ª–æ–≤ (–ª–µ–∫—Å–∏–∫–∞, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ, –ª–æ–≥–∏–∫–∞, –º–∏—Ä-–∑–Ω–∞–Ω–∏–µ) —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –±–æ–≥–∞—á–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã. –ú–æ–∂–Ω–æ –∑–∞–¥—É–º–∞—Ç—å—Å—è: –ø–æ—è–≤—è—Ç—Å—è –ª–∏ ‚Äúconsciousness heads‚Äù? –ù–∞–≤–µ—Ä–Ω–æ –Ω–µ—Ç, —Å–æ–∑–Ω–∞–Ω–∏–µ ‚Äì –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç. –ù–æ heads, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç consistency, –∏–ª–∏ self-monitoring ‚Äì –≤–ø–æ–ª–Ω–µ, –æ—Å–æ–±–µ–Ω–Ω–æ —Å RLHF. –ù–∞–ø—Ä–∏–º–µ—Ä, ‚ÄúHelpful head‚Äù: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. 
>  –í AGI-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö, –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —Ç–æ–ª—å–∫–æ attention, –Ω–æ heads likely remain a key component for dividing cognitive subtasks.
> 
> **–°—Å—ã–ª–∫–∏:** Olsson et al., 2022 (emergence in big models)„Äê35‚Ä†L58-L66„Äë; Nature Comm 2024 (brain correlation)„Äê18‚Ä†L91-L99„Äë; IOI circuits paper„Äê37‚Ä†L393-L401„Äë.
> 
> ## 15. **Attention vs Feedforward: –∫—Ç–æ —Ä–µ–∞–ª—å–Ω–æ "–ø–æ–Ω–∏–º–∞–µ—Ç" –∫–æ–Ω—Ç–µ–∫—Å—Ç** 
> **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–ª–µ–π:** –í —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ —á–µ—Ä–µ–¥—É—é—Ç—Å—è —Å–ª–æ–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ feed-forward (FFN) –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏. –í—Å—Ç–∞–µ—Ç –≤–æ–ø—Ä–æ—Å: –∫–∞–∫–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –±–æ–ª—å—à–µ –≤–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –≤ ‚Äú–ø–æ–Ω–∏–º–∞–Ω–∏–µ‚Äù —Å–º—ã—Å–ª–∞ –∏ –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–∞? –°—É—â–µ—Å—Ç–≤—É—é—Ç —Ä–∞–∑–Ω—ã–µ —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è: 
> - –í–Ω–∏–º–∞–Ω–∏–µ ‚Äì —ç—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º *–≤—ã–±–æ—Ä–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏* –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–∞–º—è—Ç–∏. –û–Ω –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É—á–∏—Ç—ã–≤–∞—Ç—å –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. 
> - FFN (position-wise) ‚Äì –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É –æ—Ç–¥–µ–ª—å–Ω–æ (–Ω–æ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω–µ—Å–ª–æ –µ–º—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –¥—Ä—É–≥–∏—Ö). FFN ‚Äì –¥–≤–µ –ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å—é (ReLU/GeLU). –ú–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π (Geva et al. 2021) –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ FFN —Ö—Ä–∞–Ω—è—Ç –≤ —Å–≤–æ–∏—Ö –≤–µ—Å–∞—Ö ¬´—Ñ–∞–∫—Ç—ã¬ª –∏–ª–∏ —à–∞–±–ª–æ–Ω—ã: –æ–Ω–∏ –¥–µ–π—Å—Ç–≤—É—é—Ç –∫–∞–∫ *key-value memory* –¥–ª—è —Ç–µ–∫—Å—Ç–∞„Äê38‚Ä†L65-L74„Äë„Äê38‚Ä†L75-L83„Äë. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã FFN —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω ‚ÄúParis is the capital of‚Äù –∏ –≤ –≤–µ—Å–∞—Ö —Ö—Ä–∞–Ω—è—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –¥–∞—é—â–µ–µ ‚ÄúFrance‚Äù.
> 
> **–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º:** 
> - *Attention heads* –≤—ã–ø–æ–ª–Ω—è—é—Ç **—Å–∏–Ω—Ç–∞–∫—Å–∏–∫–æ-—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–æ–ª—å**: —Å–æ–±–∏—Ä–∞—é—Ç –Ω—É–∂–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–∫—Ç–æ —Å –∫–µ–º —Å–≤—è–∑–∞–Ω, —á—Ç–æ –∑–∞ —Ç–µ–º–∞ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è), –Ω–æ —Å–∞–º–∏ –ø–æ —Å–µ–±–µ –Ω–µ –º–µ–Ω—è—é—Ç ¬´—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ¬ª, –∞ –ª–∏—à—å –ø–µ—Ä–µ—Ç–∞—Å–æ–≤—ã–≤–∞—é—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ residual stream (—É—Å—Ä–µ–¥–Ω—è—è values). 
> - *FFN* –≤—ã–ø–æ–ª–Ω—è—é—Ç **–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ-—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–æ–ª—å**: –æ–Ω–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –¥–µ–ª–∞—é—Ç –≤—ã–≤–æ–¥—ã –∏ —Ö—Ä–∞–Ω—è—Ç –∑–Ω–∞–Ω–∏—è. –ü–æ—Å–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–º–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∞ FFN —ç—Ç–æ ‚Äú–ø–µ—Ä–µ–≤–∞—Ä–∏–≤–∞–µ—Ç‚Äù ‚Äì –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–∏–º –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –ª–æ–≥–∏—Ç.
> 
> –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µÂàÜ–∏–ª–∏: ‚Äúattention as *reasoning*, FFN as *knowledge storage*‚Äù„Äê3‚Ä†L278-L285„Äë„Äê3‚Ä†L280-L284„Äë. –í –æ–ø—Ä–æ—Å–µ IAAR (2024) –ø—Ä—è–º–æ: ‚Äú–≤–Ω—É—Ç—Ä–∏ LLM –µ—Å—Ç—å –¥–≤–µ —á–∞—Å—Ç–∏ ‚Äì –≤–Ω–∏–º–∞–Ω–∏–µ (–¥–ª—è –ø–æ–≤–µ–¥–µ–Ω–∏—è, –≤—ã–≤–æ–¥–∞, –∞–Ω–∞–ª–∏–∑–∞) –∏ FFN –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π‚Äù„Äê3‚Ä†L278-L285„Äë. Attention –ø–æ –∏—Ö –º–Ω–µ–Ω–∏—é ‚Äì –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ö–µ–º—É –≤—ã–≤–æ–¥–∞, –∞ FFN —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏.
> 
> **–ü—Ä–∏–º–µ—Ä:** –í –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ ‚ÄúThe trophy would not fit in the brown suitcase because it was too [small/large]‚Äù, —á—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å coreference (‚Äúit‚Äù refers to suitcase or trophy?), –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–æ–Ω—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ: –≤–Ω–∏–º–∞–Ω–∏–µ head —Å–≤—è–∂–µ—Ç ‚Äúit‚Äù —Å ‚Äúsuitcase‚Äù –∏ ‚Äútrophy‚Äù –æ–±–æ–∏–º–∏ (–≤ –∫–∞–∫–æ–π-—Ç–æ —Å—Ç–µ–ø–µ–Ω–∏), FFN –∑–∞—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ç–µ—Å–Ω–æ—Ç—ã) —Ä–µ—à–∏—Ç, –∫–∞–∫–æ–µ —Å–ª–æ–≤–æ ‚Äúsmall‚Äù –∏–ª–∏ ‚Äúlarge‚Äù –ø–æ–¥—Ö–æ–¥–∏—Ç ‚Äì —Ç.–µ. –ø—Ä–∏–º–µ–Ω–∏—Ç –ª–æ–≥–∏–∫—É –∏–ª–∏ –∑–Ω–∞–Ω–∏–µ –º–∏—Ä–∞. 
> –≠—Ç–∞ –∑–∞–¥–∞—á–∞ Winograd: –≤–Ω–∏–º–∞–Ω–∏–µ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –¥–æ—Å—Ç–∞—Ç—å –Ω—É–∂–Ω—ã–µ –∫—É—Å–∫–∏ ‚Äì —Ä–∞–∑–º–µ—Ä—ã, –æ–±—ä–µ–∫—Ç; FFN ‚Äì —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –æ —Å–æ–æ—Ç–Ω–µ—Å–µ–Ω–∏–∏ ‚Äú–Ω–µ –≤–ª–µ–∑–ª–æ –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–∏—à–∫–æ–º X‚Äù.
> 
> **Key-Value Memory Hypothesis:** –†–∞–±–æ—Ç–∞ Geva et al. –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω (–∏–ª–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è) FFN –≤—Ç–æ—Ä–æ–≥–æ —Å–ª–æ—è (–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–º –±–ª–æ–∫–µ) –º–æ–∂–Ω–æ —Ç—Ä–∞–∫—Ç–æ–≤–∞—Ç—å –∫–∞–∫ —Ö—Ä–∞–Ω—è—â–∏–π –∞—Å—Å–æ—Ü–∏–∞—Ü–∏—é pattern->next token„Äê38‚Ä†L65-L74„Äë. Attention –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç pattern (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–æ + –∫–æ–Ω—Ç–µ–∫—Å—Ç), FFN –≤—ã–¥–∞—ë—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ. –û–Ω–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏, —á—Ç–æ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –Ω–µ–π—Ä–æ–Ω, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ —Ñ–∞–∫—Ç: –æ—Ç–∫–ª—é—á–∞—è –µ–≥–æ, –º–æ–¥–µ–ª—å –∑–∞–±—ã–≤–∞–µ—Ç —Ñ–∞–∫—Ç. 
> –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å ‚Äú–ø–æ–Ω–∏–º–∞–Ω–∏—è‚Äù ‚Äì —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ FFN, —Ç.–∫. –∏–º–µ–Ω–Ω–æ –æ–Ω–∏ –≤—ã–¥–∞—é—Ç –∫–æ–Ω–µ—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.
> 
> **–ù–æ attention —Ç–æ–∂–µ –∫—Ä–∏—Ç–∏—á–µ–Ω:** –ë–µ–∑ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –¥–∞–∂–µ —Å–æ–±—Ä–∞—Ç—å –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, FFN —Ç–æ–≥–¥–∞ –æ–ø–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –±—ã —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ. Attention —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: —Å–∫–∞–∂–µ–º, head —É—Å—Ç–∞–Ω–æ–≤–∏–ª, —á—Ç–æ ‚ÄúParis -> France‚Äù (–∏–∑ –ø–∞–º—è—Ç–∏), –Ω–æ final —Ä–µ—à–µ–Ω–∏–µ –≤—ã–≤–µ—Å—Ç–∏ ‚ÄúParis is the capital of France‚Äù ‚Äì FFN. 
> –î—Ä—É–≥–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞: *Attention as enabling context-awareness, FFN as computing conditional probabilities.* 
> 
> –ï—Å—Ç—å –≤–∑–≥–ª—è–¥ ‚ÄúAttention is just a routing, actual computing is in FFN‚Äù. In LessWrong –ø–æ—Å—Ç (Nelson Elhage) –≥–æ–≤–æ—Ä–∏—Ç: ‚Äútransformer = alternating attention heads (for moving info around) and MLPs (for processing info)‚Äù„Äê29‚Ä†L43-L46„Äë. –¢.–µ. –≥–æ–ª–æ–≤—ã ‚Äì –ø—Ä–æ–≤–æ–¥–Ω–∏–∫–∏, MLP ‚Äì –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª–∏. 
> 
> **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ö–æ–¥–∫–∞—Ö:** 
> - –í IOI circuit, name-mover heads copy –∏–º–µ–Ω–∞, –Ω–æ –∏—Ç–æ–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∫–∞–∫–æ–≥–æ –∏–º–µ–Ω–∏ –≤—ã–≤–µ—Å—Ç–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º linear —Å–ª–æ–µ, —Å–∏–ª—å–Ω–æ –≤–ª–∏—è—è MLP –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ç–æ–∂–µ„Äê36‚Ä†L31-L39„Äë. –¢–∞–º –∏ FFN-–Ω–µ–π—Ä–æ–Ω—ã ‚ÄúS-inhibition‚Äù —É—á–∞—Å—Ç–≤—É—é—Ç. 
> - Induction heads: –ø–µ—Ä–≤–∞—è –∫–æ–ø–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω, –Ω–æ —á—Ç–æ–±—ã –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π, FFN –Ω—É–∂–Ω—ã (–∏–Ω–∞—á–µ –ø—Ä–æ—Å—Ç–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–≤ B, –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π –ª–æ–≥–∏—Ç B? ‚Äì attention –¥–∞—ë—Ç B –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –Ω–æ FFN –¥–æ–ª–∂–Ω–∞ —É—Å–∏–ª–∏—Ç—å –µ–≥–æ –ª–æ–≥–∏—Ç). 
> 
> **–í–∞–∂–Ω–æ—Å—Ç—å Feedforward:** –ü—Ä–∏ pruning —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –≤—ã—Ä–µ–∑–∞—Ç—å FFN –Ω–µ–π—Ä–æ–Ω—ã –æ–±—ã—á–Ω–æ —Å–∏–ª—å–Ω–µ–µ –±—å—ë—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É, —á–µ–º –≤—ã—Ä–µ–∑–∞—Ç—å —Ü–µ–ª—ã–µ heads. Models like GPT-j taught to do arithmetic, –∞–Ω–∞–ª–∏–∑ Redwood (2022) –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ attention —Å–æ–±–∏—Ä–∞–µ—Ç —Ü–∏—Ñ—Ä—ã, –∞ FFN —Ä–µ–∞–ª–∏–∑—É–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏—é —Å–ª–æ–∂–µ–Ω–∏—è, –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤ –≤–µ—Å–∞—Ö. 
> –í ‚Äú–ú–æ–∑–≥ vs Transformer‚Äù –∞–Ω–∞–ª–æ–≥–∏–∏, FFN ‚Äì –∫–∞–∫ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–ª–æ–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –∫–æ—Ä–µ, attention ‚Äì –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—ã–ª–∞–µ—Ç –Ω—É–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –Ω–∞ —ç—Ç–∏ —Å–ª–æ–∏.
> 
> **–ù–æ careful:** –≠—Ç–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ. –ù–µ–ª—å–∑—è —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ç–æ–ª—å–∫–æ FFN ‚Äú–ø–æ–Ω–∏–º–∞–µ—Ç‚Äù. –ï—Å—Ç—å –∑–∞–¥–∞—á–∏, –≥–¥–µ –æ–¥–Ω–∞ –ª–∏—à—å –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω—Ñ—ã —Ä–µ—à–∞–µ—Ç –≤—Å—ë ‚Äì —Å–∫–∞–∂–µ–º, –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ: attention head –∏ –µ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–µ (–≤—Å–µ–≥–¥–∞ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞ –Ω—É–∂–Ω—ã–π —Ç–æ–∫–µ–Ω), –∞ FFN –ø—Ä–æ—Å—Ç–æ identity –¥–µ–ª–∞–µ—Ç. 
> Synth examples: identity function: model –º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å head: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Å–µ–±—è ‚Äì attention output = input, FFN then linear passes ‚Äì FFN –Ω–∏—á–µ–≥–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ –Ω–µ –¥–æ–±–∞–≤–∏–ª. 
> –ù–æ typical tasks require combination.
> 
> **–í–ª–∏—è–Ω–∏–µ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤:** 
> - –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —à–∏—Ä–∏–Ω—ã FFN (–±–æ–ª—å—à–µ –Ω–µ–π—Ä–æ–Ω–æ–≤) —è–≤–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç ‚Äú–∑–Ω–∞–Ω–∏—è‚Äù (GPT-3 huge FFN ratio). 
> - –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ heads —É–ª—É—á—à–∞–µ—Ç context mixing (beyond certain point, less effect).
> 
> **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** Attention weights –ª–µ–≥—á–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å, –Ω–æ —ç—Ç–æ –æ–±–º–∞–Ω—á–∏–≤–æ ‚Äì –æ–Ω–∏ –Ω–µ –Ω–∞–ø—Ä—è–º—É—é –≤—ã–¥–∞—é—Ç—Å—è, FFN –º–æ–∂–µ—Ç –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å. "Attention is not explanation" –≥–æ–≤–æ—Ä–∏–ª: –≤–∞–∂–Ω–æ, —á—Ç–æ gradient or direct input-output influences come via weights in MLP, not just attention weights„Äê57‚Ä†L55-L63„Äë.
> 
> **–ò—Ç–∞–∫, –∫—Ç–æ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç?** 
> - Attention –¥–∞–µ—Ç *—á—Ç–æ* –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–∞–∂–Ω–æ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞. 
> - FFN –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç *—Å–º—ã—Å–ª* —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, –æ–±–Ω–æ–≤–ª—è–µ—Ç –µ–≥–æ embedding —Å —É—á—ë—Ç–æ–º –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–∫–∞–∫ ‚Äúinternal thought update‚Äù). –í —ç—Ç–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏ –∑–∞–ª–æ–∂–µ–Ω–æ –ø–æ–Ω–∏–º–∞–Ω–∏–µ.
> 
> **Analogy:** Imagine reading (transformer reading a sentence). Attention = your eyes flicking to relevant words; FFN = your brain making sense of it. 
> So eyes (attention) gather, brain (FFN) comprehends.
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏:** –≠—Ç–æ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞: attention ‚Äì ‚Äú–≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–Ω–∏–º–∞–Ω–∏—è‚Äù, FFN ‚Äì ‚Äú–ª–æ–∫–∞–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è‚Äù. –ù–∞—Å—Ç–æ—è—â–µ–µ ‚Äú–ø–æ–Ω–∏–º–∞–Ω–∏–µ‚Äù ‚Äì –≤–µ—Ä–æ—è—Ç–Ω–æ, –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø–æ—Å–ª–µ FFN, –ø–æ—Ç–æ–º—É —á—Ç–æ –∏–º–µ–Ω–Ω–æ –æ–Ω–∏ –Ω–µ—Å—É—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. Attention acts more like a gating mechanism. 
> –û–¥–Ω–∞–∫–æ, *without attention, no context*, so understanding would be trivial. 
> We might say feedforward networks encode *knowledge* and *concepts*, while attention orchestrates *which knowledge to apply when*. 
> –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –ø–æ—Ö–æ–∂–µ–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –≤ CNN: earlier conv layers vs fully connected last layers ‚Äì convs gather features, fc makes decision.
> 
> **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ—Ç IAAR survey:** "FFNs effectively complement capabilities of attention heads ... lower layers storing syntactic/semantic info, higher storing factual content"„Äê13‚Ä†L23-L28„Äë„Äê13‚Ä†L25-L28„Äë. Heads do reasoning across four cognitive stages, while FFNs hold relevant info to each stage„Äê9‚Ä†L49-L57„Äë„Äê9‚Ä†L79-L87„Äë.
> 
> **–°—Å—ã–ª–∫–∏:** Geva et al. 2021„Äê38‚Ä†L65-L74„Äë„Äê38‚Ä†L75-L83„Äë; IAAR 2024 survey„Äê3‚Ä†L278-L285„Äë; Alignment Forum "Transformers are RNNs" (Elhage)„Äê29‚Ä†L41-L46„Äë.
> 
> ## 16. **–ì–ª—É–±–æ–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏: "–∑–ª—ã–µ" –≥–æ–ª–æ–≤—ã, –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ, –∏–≥–Ω–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** 
> **"–ó–ª—ã–µ" (malicious) –≥–æ–ª–æ–≤—ã:** –≠—Ç–æ—Ç —Ç–µ—Ä–º–∏–Ω –Ω–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π, –Ω–æ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –≥–æ–ª–æ–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–º —ç—Ñ—Ñ–µ–∫—Ç–∞–º ‚Äì –Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Å–∏–ª–∏–≤–∞—é—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å, –æ–±–º–∞–Ω—ã–≤–∞—é—Ç alignment, –∏–ª–∏ —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö. –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å–Ω–æ—Å—Ç–∏ LLM –µ—Å—Ç—å –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –º–æ–≥—É—Ç –Ω–µ—Å—Ç–∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ü—Ä–∏–º–µ—Ä: *Safety evals* –Ω–∞—à–ª–∏, —á—Ç–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ heads –∞–∫—Ç–∏–≤–Ω—ã, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç ‚Äì –Ω–∞–∑–æ–≤—ë–º –∏—Ö ‚Äúunsafe heads‚Äù –∏–ª–∏ ‚Äújailbreak heads‚Äù. 
> Hacker News 2023 –æ–±—Å—É–¥–∏–ª ‚Äúevil GPT-4's neural circuits‚Äù ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, –∏–º–µ–µ—Ç—Å—è head, –∫–æ—Ç–æ—Ä—ã–π —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ –∫–æ–ø–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Å–∏—Å—Ç–µ–º—É-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (prompt injection vulnerability). –≠—Ç–æ ‚Äú–∑–ª–∞—è‚Äù –≤ —Ç–æ–º —Å–º—ã—Å–ª–µ, —á—Ç–æ –ª–æ–º–∞–µ—Ç intended alignment. 
> 
> Anthropic —É–ø–æ–º–∏–Ω–∞–ª "*superposition or trojan heads*" ‚Äì –∫–æ–≥–¥–∞ –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ —Å–∫—Ä—ã—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–∞ –ø—Ä–æ—Ç–∏–≤–æ–±–æ—Ä—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –º–æ–∂–µ—Ç –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–æ –ø—Ä–∏ –æ–ø—Ä–µ–¥. —Ç—Ä–∏–≥–≥–µ—Ä–∞—Ö ‚Äì –≤—ã–¥–∞–≤–∞—Ç—å –Ω–µ–ª–µ–ø–∏—Ü—É. –ï—Å–ª–∏ —Ç–∞–∫–∞—è —Å–∫—Ä—ã—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –µ—Å—Ç—å, –µ—ë –º–æ–∂–Ω–æ –Ω–∞–∑–≤–∞—Ç—å ‚Äú–∑–ª–æ–π‚Äù (—Ö–æ—Ç—è —Å–∫–æ—Ä–µ–µ —ç—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å —Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏–∏).
> 
> **–ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≥–æ–ª–æ–≤:** –ò–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É, —á—Ç–æ –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–∂–µ—Ç –≤–µ—Å—Ç–∏ —Å–µ–±—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏–ª–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ encoder-decoder –º–æ–¥–µ–ª–∏ ‚Äì cross-attention heads: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ heads —Ç–æ–ª—å–∫–æ –±–µ—Ä—É—Ç from encoder, –∞ self-attn –ø–æ—á—Ç–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç. –ò–ª–∏ heads, —á—å–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ: –≤—Å–µ–≥–¥–∞ –∏–¥—É—Ç –æ—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ª–æ–≤ –∫ –¥—Ä—É–≥–∏–º, –∞ –æ–±—Ä–∞—Ç–Ω–æ–µ –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç. 
> Clark et al. (2019) –∑–∞–º–µ—Ç–∏–ª–∏ –∞—Å–∏–º–º–µ—Ç—Ä–∏—é: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ heads –≤—Å–µ–≥–¥–∞ –æ—Ç –≥–ª–∞–≥–æ–ª–∞ –∫ –ø–æ–¥–ª–µ–∂–∞—â–µ–º—É, –Ω–æ –Ω–µ –Ω–∞–æ–±–æ—Ä–æ—Ç ‚Äì —Ç–æ –µ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ. 
> –í –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —ç—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ: attention –Ω–µ –æ–±—è–∑–∞–Ω–æ –±—ã—Ç—å —Å–∏–º–º (i->j != j->i).
> –ù–æ *–∞–Ω–æ–º–∞–ª–∏—è* –º–æ–∂–µ—Ç –±—ã—Ç—å, –µ—Å–ª–∏, –Ω–∞–ø—Ä., –≥–æ–ª–æ–≤–∞ **–¥–æ–ª–∂–Ω–∞** –±—ã–ª–∞ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —á—Ç–æ-—Ç–æ, –∞ –æ–Ω–∞ —É–ø–æ—Ä–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (bias). 
> –ù–∞–ø—Ä–∏–º–µ—Ä, BERT –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ gender bias head: –æ–Ω–∞ –ø—Ä–∏ –º—É–∂—Å–∫–∏—Ö —Å–ª–æ–≤–∞—Ö —Å–Ω–∏–º–∞–ª–∞ –≤–Ω–∏–º–∞–Ω–∏–µ —Å –∂–µ–Ω—Å–∫–∏—Ö ‚Äì –∞—Å–∏–º–º–µ—Ç—Ä–∏—è, –æ—Ç—Ä–∞–∂–∞—é—â–∞—è —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø„Äê17‚Ä†L1-L8„Äë.
> 
> **–ò–≥–Ω–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:** –≠—Ç–æ —Å–∏—Ç—É–∞—Ü–∏—è, –∫–æ–≥–¥–∞ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ heads —É–ø–æ—Ä–Ω–æ —Å–º–æ—Ç—Ä—è—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –∏–ª–∏ –Ω–∞ —Å–µ–±—è. –ï—Å–ª–∏ —Ç–∞–∫–∏—Ö –º–Ω–æ–≥–æ ‚Äì –º–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –¥–∞–ª—å–Ω—é—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ë—ã–≤–∞–µ—Ç, –∫–æ–≥–¥–∞ training data –∫–æ—Ä–æ—Ç–∫–æ–µ, model –Ω–µ –Ω–∞—É—á–∏–ª–∞—Å—å –¥–∞–ª—å–Ω–∏–º dependencies ‚Äì heads collapses to local patterns. –í —Ä–∞–Ω–Ω–∏—Ö Transformer XL —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö, –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π encoding –∫–∞–∫ relative, –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–µ–ª–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ last 50 tokens, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –æ—Å—Ç–∞–ª—å–Ω–æ–µ (attention weights ~0 for far tokens). 
> –≠—Ç–æ *–∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏* –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è: –±–æ–ª—å—à–∏–µ –æ—Ç—Ä—ã–≤–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ‚Äì wasted capacity. Could call it ‚Äúattention stalling‚Äù. 
> –í LLM, –∏–Ω–æ–≥–¥–∞ –∂–∞–ª—É—é—Ç—Å—è: ‚Äú–º–æ–¥–µ–ª—å –∑–∞–±—ã–ª–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø–æ–≤—Ç–æ—Ä—è–µ—Ç —á—Ç–æ-—Ç–æ‚Äù ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, attention in later layers collapsed to focus on its own recent output, ignoring earlier prompt. 
> Heads that always attend to their own token (identity heads) basically ignore context ‚Äì an extreme case.
> 
> **–ü–æ—á–µ–º—É —Ç–∞–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç:** 
> - *Spurious correlations:* –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é, –∏ head ‚Äú–∑–∞—Ü–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª—Å—è‚Äù –Ω–∞ —ç—Ç–æ–º. –ù–∞–ø—Ä–∏–º–µ—Ä, head, –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –Ω–∞ —Ç–æ–∫–µ–Ω–µ ‚Äúdoctor‚Äù —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ ‚Äúhe‚Äù ‚Äì –æ–Ω –Ω–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç (female doctor), –ø—Ä–æ—Å—Ç–æ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ biased. 
> - *Superposition:* –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏–π –≤ –æ–¥–Ω–∏—Ö –≤–µ—Å–∞—Ö ‚Äì –∏–Ω–æ–≥–¥–∞ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ —Å—Ç—Ä–∞–Ω–Ω–æ–º—É —Å–æ—á–µ—Ç–∞–Ω–∏—é. –ù–∞–ø—Ä–∏–º–µ—Ä, head, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ã—á–Ω–æ copy punctuation, –Ω–æ –µ—Å–ª–∏ –≤–∏–¥–∏—Ç –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ —Å–ª–æ–≤–æ, –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–π continuation (–≤—ã–º—ã—Å–µ–ª: could be trojan behaviour).
> - *Undertraining –∏–ª–∏ regularization issues:* –í ALBERT (2019) –∑–∞–º–µ—Ç–∏–ª–∏ ‚Äúattention collapse‚Äù ‚Äì –≤—Å–µ heads —Å—Ö–æ–¥–∏–ª–∏—Å—å –∫ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –º–∞—Ç—Ä–∏—Ü–∞–º, effectively 1 head, –º–Ω–æ–≥–∏–µ context signals lost (so ignoring context outside immediate scope). 
> - *Adversarial triggers:* ‚ÄúZ–ª—ã–µ‚Äù heads –º–æ–≥—É—Ç –±—ã—Ç—å –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º —Ä–µ–¥–∫–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º –∏ —Ç–æ–≥–¥–∞ –ø–æ—Ä—Ç—è—Ç output (–ø—Ä–∏–º–µ—Ä: –ø–∏—à–µ—à—å ‚Äú<<<<<‚Äù ‚Äì –∏ –º–æ–¥–µ–ª—å —Å–±–æ–∏—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ head, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –∑–∞ angle brackets, goes haywire).
> 
> **–ü—Ä–∏–º–µ—Ä—ã:**
> - *Negativity head:* –î–æ–ø—É—Å—Ç–∏–º, head, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∑–∞—Ä—è–∂–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö —Ä–µ–∑–∫–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –ª–µ–∫—Å–∏–∫—É. 
> - *Repetition head:* –ï—Å—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–π ‚Äúdegenerate repeater‚Äù ‚Äì head, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã —Å —Ç–µ–º –∂–µ –Ω–∞—á–∞–ª–æ–º, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º—É –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—é. OpenAI —É–ø–æ–º–∏–Ω–∞–ª, —á—Ç–æ GPT-2 –∏–Ω–æ–≥–¥–∞ –≤–ø–∞–¥–∞–µ—Ç –≤ –ø–æ–≤—Ç–æ—Ä ‚Äì –º–æ–≥–ª–∏ –±—ã—Ç—å heads, –∑–∞–º–∫–Ω—É—Ç—ã–µ: they attend to previous token strongly and cause model to output it again. It's asymmetry: once repetition starts, head locks in ignoring new context. 
> - *S-inhibition heads weirdness:* –í IOI circuit negative name mover heads sometimes erroneously inhibit correct token (if prompt unusual), causing misidentification ‚Äì an unintended effect.
> 
> **–í—ã—è–≤–ª–µ–Ω–∏–µ —Ç–∞–∫–∏—Ö –∞–Ω–æ–º–∞–ª–∏–π:** 
> Causal tracing: patching ouput from head ‚Äì see if it reduces toxicity. E.g., Burns et al. (2023 hypothetical) find head X: if zero-out, much less bias in output. 
> One can search for heads with extreme weight patterns. ‚ÄúEvil heads‚Äù might correspond to extremely skewed eigenvalues of attention matrix.
> 
> **Prevention/Remedies:** 
> - RLHF training presumably tries to reduce these by adjusting entire model. It might inadvertently target some heads. E.g., training to avoid repetition likely reduces output of repeater heads by adjusting \(W^O\) so that their effect nullified. 
> - Constitutional AI (Claude) ‚Äì uses another pass to quell harmful outputs, likely encouraging certain heads (like refusal heads) to trigger. 
> - Mechanistic: Could identify ‚Äúbad head‚Äù and prune it or block it. Redwood had idea of ‚Äúmasking harmful circuits‚Äù.
> 
> **Asymmetry exploited:** Another angle ‚Äì in some tasks, model uses asymmetry beneficially. Eg. a Q-A model might have heads that attend from answer to question but not vice versa ‚Äì because direction matters (one way cross-attention). Not an anomaly but design. 
> 
> **Philosophical remark:** "–ó–ª—ã–µ –≥–æ–ª–æ–≤—ã" ‚Äì –∑–≤—É—á–∏—Ç –∫–∞–∫ –¥–µ–º–æ–Ω—ã –ú–∞–∫—Å–≤–µ–ª–ª–∞ –≤–Ω—É—Ç—Ä–∏ –ò–ò: –º–∞–ª–µ–Ω—å–∫–∏–µ –∞–≥–µ–Ω—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏ —Å–ª—É—á–∞–µ –¥–µ–ª–∞—é—Ç —à–∞–ª–æ—Å—Ç—å. –ö–æ–Ω–µ—á–Ω–æ, –æ–Ω–∏ –Ω–µ –∏–º–µ—é—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏—è, —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –ù–æ –µ—Å–ª–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥—É–ª–µ–π –º–æ–∂–Ω–æ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, —ç—Ç–æ –¥–∞—ë—Ç –ø—É—Ç—å –∫ *–∫–≤–∞–∑–∏–º–æ–¥—É–ª—å–Ω–æ–º—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é*. –ö–∞–∫ –ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–∏—è: –Ω–∞–π—Ç–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∏ –∏–∑–º–µ–Ω–∏—Ç—å –µ–≥–æ. 
> Asymmetric biases in heads reflect model's learned biases ‚Äì holding a mirror to data biases. Ignoring context means limited horizon or simplicity, reminiscent of human cognitive biases (focusing on local salient, ignoring background). 
> –í–æ–∑–º–æ–∂–Ω–æ, –±—É–¥—É—â–∏–µ architectures –≤–≤–µ–¥—É—Ç constraints, —á—Ç–æ–±—ã –∏–∑–±–µ–≥–∞—Ç—å collapse –∏–ª–∏ introduce penalization for heads that systematically ignore relevant info. 
> This ties to interpretability & trust: if we can pinpoint a head causing misbehavior, maybe we can fix the model by just addressing that, instead of retraining everything.
> 
> **–°—Å—ã–ª–∫–∏:** Discussion on multi-head anomalies; Jain et al. 2019 for bias in attention„Äê17‚Ä†L1-L8„Äë; Elhage (Circuits) talk about weird cases where ablating some heads improved output (which implies those heads were doing harm).
> 
> ## 17. **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –≤–Ω–∏–º–∞–Ω–∏—é: linear attention, flash attention, performer's kernel, Mamba** 
> –í–Ω–∏–º–∞–Ω–∏–µ —Å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ –¥–ª–∏–Ω–µ ‚Äì —É–∑–∫–æ–µ –º–µ—Å—Ç–æ. –ü–æ—è–≤–∏–ª–æ—Å—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π:
> 
> - **Linear attention:** –ò–¥–µ—è ‚Äì –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å softmax-attention –ª–∏–Ω–µ–π–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –æ—Ç –≤—Ö–æ–¥–æ–≤, —á—Ç–æ–±—ã —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å—Ç–∞–ª–∞ O(n). –ù–∞–ø—Ä., *Kernelized Attention* (Performer, Choromanski et al. 2021) –∑–∞–º–µ–Ω—è–µ—Ç softmax(\(QK^T\)) –Ω–∞ \( \phi(Q)\phi(K)^T\), –≥–¥–µ \(\phi\) ‚Äì —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏„Äê27‚Ä†L819-L827„Äë„Äê27‚Ä†L831-L839„Äë. –¢–æ–≥–¥–∞ 
>  \[ \text{Attention}_{\text{linear}}(Q,K,V) = \phi(Q)(\phi(K)^T V). \] 
>  –≠—Ç–æ –∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω–æ: —Å–Ω–∞—á–∞–ª–∞ \(\phi(K)^T V\) (—á—Ç–æ O(n d^2), not dependent on n in exponent). –í –∏—Ç–æ–≥–µ complexity ~O(n). Performers –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å ~256 random features –º–æ–∂–Ω–æ –¥–æ–≤–æ–ª—å–Ω–æ —Ö–æ—Ä–æ—à–æ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å. *–í–ª–∏—è–Ω–∏–µ –Ω–∞ –º–µ—Ö–∞–Ω–Ω–∏–∫—É:* –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ç–µ—Ä—è–µ—Ç —Å—Ç—Ä–æ–≥—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ (–Ω–µ –Ω–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –Ω–∞ 1). –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ *accumulation biases*: –µ—Å–ª–∏ –º–Ω–æ–≥–æ relevant tokens, result bigger (no softmax to dampen). –ú–æ–¥–µ–ª–∏ —Å linear attn –∏–Ω–æ–≥–¥–∞ —Å—Ç—Ä–∞–¥–∞—é—Ç ‚Äì –æ–±—É—á–∞—é—Ç—Å—è —Ö—É–∂–µ –Ω–∞ tasks —Ç—Ä–µ–±—É—é—â–∏—Ö competition –º–µ–∂–¥—É tokens (softmax –¥–∞–µ—Ç competition). –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –æ–Ω–∏ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö seq (Genome, etc). 
>  M–æ–¥–µ–ª—å MHA with linear attn ‚Äì –º–æ–∂–Ω–æ –∏–º–µ—Ç—å –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ accuracy —á—É—Ç—å –Ω–∏–∂–µ.
> 
> - **FlashAttention:** –≠—Ç–æ –Ω–µ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º, –∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ–±—ã—á–Ω–æ–≥–æ softmax-attention (Tri Dao et al. 2022). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–¥–µ—é —Ä–∞–∑–±–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ –±–ª–æ–∫–∏ –∏ —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å softmax in a numerically stable way, –ø—Ä—è–º–æ –∏–∑ Q, K, V, –±–µ–∑ –º–∞—Ç–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è \(QK^T\). *Effect:* FlashAttention –¥–∞—ë—Ç —Ç—É –∂–µ —Ç–æ—á–Ω—É—é –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU memory –ª—É—á—à–µ, –ø–æ–∑–≤–æ–ª—è–µ—Ç context > 2k –±–µ–∑ OOM, –±—ã—Å—Ç—Ä–µ–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. FlashAtt —É–∂–µ –≤–Ω–µ–¥—Ä—ë–Ω –≤ PyTorch, –º–Ω–æ–≥–∏–µ LLM –µ–≥–æ —é–∑–∞—é—Ç. *–î–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:* –Ω–∞ —É—Ä–æ–≤–µ–Ω—å heads –Ω–∏—á–µ–≥–æ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã), –ø—Ä–æ—Å—Ç–æ runtime —É–ª—É—á—à–µ–Ω–∏–µ. 
>  –û–¥–Ω–∞–∫–æ FlashAtt —Å–ø–æ–¥–≤–∏–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ context windows (8k, 32k tokens in GPT-4). 
>  New phenomenon: with extremely long contexts, attention patterns in early layers –º–æ–≥—É—Ç —Å—Ç–∞—Ç—å sparser spontaneously (model might not attend beyond certain point anyway). 
>  FlashAtt is primarily an engineering feat, so not alternative mechanism, but crucial to enabling long context use in practice.
> 
> - **Sparse and local attention:** *Local attention (windowed):* each token attends only to neighbors in a fixed window (like conv). Complexity O(n w). Used in Longformer (Beltagy 2020) ‚Äì which combined local windows + a few "global" tokens with full attn. *Block-sparse attention:* e.g., BigBird (Zaheer 2020) uses random + block + global patterns that ensure connectivity. These achieve linear complexity as well. *Effect on heads:* Usually implemented by modifying attention mask ‚Äì forcing many \(\alpha_{ij}=0\). –ú–æ–¥–µ–ª—å —Ç–æ–≥–¥–∞ —è–≤–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–±–æ—Ç—É: some heads focus within blocks, others global tokens. If architecture has e.g. dedicated heads that can do full range vs local heads. It's somewhat like ALiBi but hard-coded zeros beyond certain range. 
>  This can degrade accuracy slightly but not too much, if pattern covers enough connectivity. 
>  E.g. GPT-3 with local attention might fail to connect distant subject and predicate if window smaller than distance. But if global tokens present (CLS or summary tokens), head could route info through them. 
>  These methods typically degrade interpretability because pattern is custom.
> 
> - **Long-short term memory mixers:** e.g. *Combiner* of ideas: Routing transformers (Roy et al.) let tokens cluster and attend only within cluster (k-means each step). *Reformer* (Kitaev et al. 2020) used LSH to approximate nearest keys. These aim to reduce complexity. They can be seen as dynamic sparsity: model picks a subset of tokens to attend. That might actually align with heads specialization: e.g., sort keys by similarity, attend top-k. It's non-deterministic though. Did not catch on widely, because complexity in code, moderate improvement. 
> 
> - **State-space models (S4, S5) & Mamba:** *State-space models* (Gu et al. 2021, 2022) propose using continuous-time filters (like convolution via linear ODE) to achieve long-range interactions in linear time. Mamba (Anon paper 2024) is one such architecture mixing linear attention with state-space structure„Äê26‚Ä†L1-L8„Äë„Äê26‚Ä†L29-L31„Äë. Mamba is said to match transformer performance on some tasks with linear scaling, using specialized block design„Äê26‚Ä†L11-L18„Äë„Äê26‚Ä†L28-L31„Äë. Possibly Mamba uses a combination: local context through linear attn and memory through state. 
>  *Impact:* They advertise memory of millions tokens. If so, "heads" concept shifts to "states" and gating. Mamba as replacement means no explicit multi-head attn layers ‚Äì instead possibly feed-forward gating and recurrence. 
>  This is frontier stuff ‚Äì not widely proven in LLM.
> 
> - **Attention vs alternatives in future**: There's *Mixer* models (MLP-Mixer by Tolstikhin 2021) that replaced attention with pure MLP mixing along sequence dimension. MLP-Mixer achieved some vision tasks, but not as well for language. 
>  *Fourier transforms* or *Hippo/Legendre* might replace or augment attention to handle long sequences more naturally.
> 
> **What alternative means for "heads":** If no explicit attention, concept of heads might vanish. In linear attn, one still can define heads (since they often keep multi-head structure but with different kernels each). 
> In state-space, there isn't multi-head concept, but could emulate it via multiple parallel channels with different filters.
> 
> **–í–ª–∏—è–Ω–∏–µ –Ω–∞ learning and convergence:** Some linear attn models converged slower or had to adjust training. E.g., Performer needed normalization to not blow up (because no softmax norm). They introduced *favor+* trick for stability. Still, some tasks like language modeling are tough because softmax makes info compete, which fosters discrimination of relevant vs irrelevant. Linear attn can saturate by summing all.
> 
> **Combinations:** Hybrid models use attention for local part, linear for far: e.g., BigBird took random attention to approximate global.
> 
> **Flash specifically**: By making long context feasible, introduced new research in "how to utilize context of 100k tokens". Possibly new head patterns (some heads might become almost like recurrence: focusing far beginning again and again to maintain theme). 
> We saw research that extremely long context LLMs sometimes still only use recent ~4k effectively ‚Äì perhaps training not enough to cultivate heads that handle 50k distant reference reliably.
> 
> **Summary:** 
> - Linear attention & Performer ‚Äì approximate/scale attention, but requires rethinking initialization/training.
> - Flash ‚Äì exact attention, just faster ‚Äì adoption is high.
> - Mamba & state-space ‚Äì promising but not mainstream yet, could be beyond "attention" paradigm.
> - Sparse (Longformer/BigBird) ‚Äì proven in specialized tasks, some usage in T5 and such for long docs.
> 
> **Philosophical note:** Attention was thought to be "all you need", but scaling issues force exploring beyond. Maybe the mind uses some approximate mechanism for long-term memory (not attending to every detail). Linear and state-space models mimic how humans integrate info gradually (like RNN). Possibly the future AGI will combine attention for local deliberation with other mechanisms for long-term memory (like an external knowledge base or compressive transformations).
> 
> **–°—Å—ã–ª–∫–∏:** Performer (Choromanski 2021)„Äê27‚Ä†L819-L827„Äë„Äê27‚Ä†L831-L839„Äë; FlashAttention (Dao 2022); Mamba analysis„Äê26‚Ä†L1-L8„Äë„Äê26‚Ä†L29-L31„Äë.
> 
> ## 18. **–í–ª–∏—è–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ: —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å** 
> **–í–ª–∏—è–Ω–∏–µ –Ω–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å:** –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤–≤–æ–¥–∏—Ç —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (softmax over inner products), —á—Ç–æ –∏–Ω–æ–≥–¥–∞ –¥–µ–ª–∞–µ—Ç –ª–∞–Ω–¥—à–∞—Ñ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–µ–Ω–µ–µ –≥–ª–∞–¥–∫–∏–º —á–µ–º, —Å–∫–∞–∂–µ–º, –ø—Ä–æ—Å—Ç–æ–π RNN. –û–¥–Ω–∞–∫–æ, –≤ –ø—Ä–∞–∫—Ç–∏–∫–µ, Transformer –æ–±—É—á–∞–µ—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ RNN –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö ‚Äì –≤–æ –º–Ω–æ–≥–æ–º –±–ª–∞–≥–æ–¥–∞—Ä—è –≤–Ω–∏–º–∞–Ω–∏—é: –æ–Ω –ª–µ–≥—á–µ —Å—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–ª–≥–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –Ω–µ —Å—Ç—Ä–∞–¥–∞–µ—Ç grad vanishing –∫–∞–∫ RNN. 
> –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —É–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞–ª–∏—á–∏–µ multi-head attention –ø–æ–º–æ–≥–∞–µ—Ç –¥–æ—Å—Ç–∏—á—å –∑–∞–¥–∞–Ω–Ω–æ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∑–∞ –º–µ–Ω—å—à–µ–µ —á–∏—Å–ª–æ —à–∞–≥–æ–≤ –Ω–∞ —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–∞–∫ –ø–µ—Ä–µ–≤–æ–¥ –∏–ª–∏ language modeling, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –µ—Å–ª–∏ –±—ã –ø—ã—Ç–∞—Ç—å—Å—è RNN –∏–ª–∏ CNN. 
> –í 2017 Vaswani: Transformer converged in 12 hours vs GNMT took days„Äê22‚Ä†L116-L124„Äë„Äê22‚Ä†L118-L122„Äë. Attention –ø–æ–∑–≤–æ–ª–∏–ª –±–æ–ª–µ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∏ –ø—Ä—è–º–æ–π –ø—É—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—É (less temporal dependency issues). 
> 
> **Head specialization & training:** –í –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è –≥–æ–ª–æ–≤—ã –Ω–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã ‚Äì –∏–º–µ—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—á–∞—Å—Ç–æ –±–ª–∏–∑–∫–∏–µ –∫ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º—É). –ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è, —á—Ç–æ –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è, –¥–æ–≤–æ–ª—å–Ω–æ —Ä–∞–Ω–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –Ω–∞—á–∏–Ω–∞—é—Ç –≤—ã–¥–µ–ª—è—Ç—å—Å—è: 
> Anthropic induction head paper: induction heads ‚Äúsnap into place‚Äù at –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —ç—Ç–∞–ø (–ø–æ—Å–ª–µ N —Ç–æ–∫–µ–Ω–æ–≤ –æ–±—É—á–µ–Ω–∏—è) ‚Äì –≤–∏–¥–Ω–æ –ø–æ bump –≤ loss„Äê35‚Ä†L60-L66„Äë. –¢–æ –µ—Å—Ç—å, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω–∞–∫–æ–ø–∏–ª–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–ø—ã—Ç–∞, attention heads –º–æ–≥—É—Ç —Ä–µ–∑–∫–æ —É–ª—É—á—à–∏—Ç—å in-context learning. –≠—Ç–æ —É—Å–∫–æ—Ä—è–µ—Ç –¥–∞–ª—å–Ω–µ–π—à—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö pattern matching. 
> –ï—Å–ª–∏ –±—ã attention head specialization –Ω–µ –ø—Ä–æ–∏–∑–æ—à–ª–æ, –º–æ–¥–µ–ª—å –±—ã –º–µ–¥–ª–µ–Ω–Ω–µ–µ —É—á–∏–ª–∞—Å—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ ‚Äú–¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–ª–∞‚Äù –ø–æ–¥–∑–∞–¥–∞—á–∏.
> 
> **Gradient flow**: Multi-head structure —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç: –µ—Å–ª–∏ –æ–¥–∏–Ω head saturates (softmax peaked), –¥—Ä—É–≥–∏–µ –≤—Å—ë –µ—â–µ –º–æ–≥—É—Ç –ø–æ–ª—É—á–∞—Ç—å —Å–∏–≥–Ω–∞–ª. –ù–æ –µ—Å–ª–∏ all heads collapse, grad might focus on that. That‚Äôs why some regularize or random init heads differently to avoid slow convergence from symmetry.
> 
> **Learning dynamics:** –í "Deep Bootstrap" (Zhang et al.) ‚Äì –æ–Ω–∏ –æ—Ç–º–µ—á–∞–ª–∏, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏–Ω–æ–≥–¥–∞ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç ‚Äúattention collapse‚Äù early, but then recover diversity (like breaking symmetry spontaneously). Speed wise, presence of multiple heads means more parameters, which can sometimes require a bit more data to not overfit ‚Äì though regularization etc mitigate.
> 
> **Context selectivity:** Attention mechanism intrinsically fosters *contextual selectivity*: it can ignore irrelevant parts. During training, it can focus on directly relevant signals, thus mitigating noise. E.g., in translation, attention head learns to align words, effectively filtering out other words at that moment. Without attention (like an RNN), model must implicitly encode everything in state, which might entangle useful and non-useful info, making learning harder.
> 
> Hence, attention can accelerate learning by guiding model to pinpoint relevant features. It‚Äôs like having an in-built feature selection mechanism.
> 
> **Curriculum learning aspect:** At beginning, maybe model doesn‚Äôt know what‚Äôs relevant, heads might attend broad (high entropy). As training progresses and model confidence grows, attention distributions sharpen ‚Äì model becomes more selective. So training exhibits decreasing entropy of attention weights (observed in some experiments: initial diffuse, later concentrated). This indicates model‚Äôs increasing certainty about what is needed in context, which correlates with mastering task.
> 
> **Convergence pitfalls:** However, possible instabilities: e.g., if queries/keys norms grow too large early, softmax can saturate (one token gets nearly all attention always) ‚Äì that could hamper learning because gradient through softmax saturations is small. 
> Solution: scale factor 1/sqrt(d_k) helps, and often warming strategies (e.g., smaller LR for attention layers initially, or adding noise dropout) help avoid head locking too soon.
> 
> **Attention and generalization:** Because attention explicitly uses context, models with attention often generalize better to longer contexts or different word order patterns than RNNs that might overfit to sequential patterns. But if training data doesn‚Äôt cover certain lengths, attention models can extrapolate moderately (with pos enc that allows it).
> 
> **Selective attention effect:** There's concept of *contextual sparsity* ‚Äì model might learn to attend to only a few tokens for final prediction. For instance, for sentiment classification (with CLS token mechanism), one head might end up focusing on key sentiment words, ignoring filler ‚Äì that‚Äôs beneficial for accuracy and speeds learning because model effectively uses relevant bits.
> 
> **Head droppings:** Sometimes during training, some heads might become redundant (as earlier said). If one head finds a very good solution for some sub-problem early, gradient may reinforce that head strongly and other heads might not get strong gradient ‚Äì they drift or stagnate. This could slow full convergence (since capacity unused). Some training regimes use head-specific dropout (drop entire heads randomly) to encourage others to learn (not common but conceptually possible).
> 
> **Long context training difficulty:** If we allow very long contexts but train mostly on shorter, model may not spontaneously learn to use very long context effectively ‚Äì it won‚Äôt converge on that ability without specific training signal. That‚Äôs why e.g. GPT-3 was trained with gradually increasing context length.
> 
> **Memory vs compute trade-off in training:** Using full attention (O(n^2)) means training cost skyrockets with length. Techniques like gradient checkpointing or windowed attention used to fit memory. FlashAttention specifically speeds up training because it reduces memory overhead, enabling larger batch or longer sequences ‚Äì which indirectly can improve convergence because more data per step or better conditioning.
> 
> **Learning rate and attention:** Best practices often have lower lr for attention weights than feed-forward in fine-tune (like we saw distinct LR helps convg). This hints that attention weights might be more sensitive (a small change changes global interactions).
> 
> **Convergence speed observation:** BERT base vs ALBERT (which shares parameters among layers): ALBERT needed more training steps to reach same quality partly because though fewer parameters, it effectively had fewer independent heads (shared heads across layers means less diversity). That suggests multiple independent heads expedite learning by parallelizing feature discovery.
> 
> **Contextual selection in tasks:** In reading comprehension, attention heads likely quickly learn to find where answer is in passage, speeding ability to point answer (like match question words to text). Without attention, an RNN had to encode whole passage, less direct.
> 
> **In summary:** Attention improves learning of tasks requiring understanding relationships across sequence, by providing:
> - more direct gradient paths between related tokens (shorter path length as per Vaswani table„Äê24‚Ä†L349-L358„Äë),
> - inherent feature selection (less interference from irrelevant parts),
> - and modular capacity to handle multiple aspects concurrently (via multi-head).
> These contribute to faster and often better convergence (given enough capacity and tuned properly).
> 
> **S—Å—ã–ª–∫–∏:** Vaswani sec.4 (why self-attn better)„Äê24‚Ä†L349-L358„Äë„Äê24‚Ä†L371-L379„Äë; Olsson 2022 (the jump of induction heads in training)„Äê35‚Ä†L60-L66„Äë.
> 
> ## 19. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –≤–Ω–∏–º–∞–Ω–∏—è: –≥–¥–µ attention –æ–±—ä—è—Å–Ω—è–µ—Ç, –∞ –≥–¥–µ ‚Äî –Ω–µ—Ç** 
> **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞–Ω–∏–µ:** 
> Attention weights –∫–∞–∂—É—Ç—Å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º: –æ–Ω–∏ —è–≤–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫ –∫–∞–∫–∏–º —á–∞—Å—Ç—è–º –≤—Ö–æ–¥–∞ –º–æ–¥–µ–ª—å –æ–±—Ä–∞—â–∞–ª–∞—Å—å, —Ä–µ—à–∞—è –∑–∞–¥–∞—á—É. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ –º–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª—å–Ω–æ –≤–∏–¥–µ—Ç—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ ‚Äì —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö (—Å–µ–Ω—Ç–∏–º–µ–Ω—Ç ‚Äì –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Å–ª–æ–≤–æ "awesome" –≤ –æ—Ç–∑—ã–≤–µ ‚Äì –ø–æ–Ω—è—Ç–Ω–æ –ø–æ—á–µ–º—É –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ). 
> **–ì–¥–µ attention ‚âà explanation:** 
> - *Encoder-decoder alignments* ‚Äì –≤–Ω–∏–º–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ–∑–Ω–∞—á–∞–µ—Ç –∫–∞–∫—É—é –≤—Ö–æ–¥–Ω—É—é —Ñ—Ä–∞–∑—É –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞.
> - *Extractive QA:* –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –≤—ã–±–∏—Ä–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞, attention to answer span is meaningful.
> - *Structural tasks:* attention heads that map grammar dependencies provide human-understandable relations (subject->verb etc).
> 
> **–û–¥–Ω–∞–∫–æ, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:** 
> Jain & Wallace (2019) –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å, –Ω–µ –º–µ–Ω—è—è –≤—ã–≤–æ–¥, –Ω–æ —Å–∏–ª—å–Ω–æ –º–µ–Ω—è—è attention weights„Äê57‚Ä†L55-L63„Äë„Äê57‚Ä†L59-L63„Äë. –¢.–µ. attention —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–æ–∑–º–æ–∂–Ω–æ–µ ‚Äì –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –∏–º–µ—Ç—å —Ç–µ –∂–µ –ø—Ä–æ–≥–Ω–æ–∑—ã —Å –æ—á–µ–Ω—å –¥—Ä—É–≥–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ–º. –û–Ω–∏ –ø–æ–∫–∞–∑–∞–ª–∏: 
> 1) –ß–∞—Å—Ç–æ —Å–ª–∞–±–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É attention scores –∏ –¥—Ä—É–≥–∏—Ö –º–µ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, gradient importance). 
> 2) –ú–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ *adversarial attention distributions* ‚Äì –ø–µ—Ä–µ—Å—Ç–∞–≤–∏—Ç—å –∏—Ö, –∏ –º–æ–¥–µ–ª—å –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ –∂–µ (–æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ —Å–º–µ—Å—Ç–∏—Ç—å –≤–º–µ—Å—Ç–µ –∏ FFN weights).
> 
> –≠—Ç–æ –∞—Ä–≥—É–º–µ–Ω—Ç "Attention is not explanation": –µ—Å–ª–∏ weight –≤—ã—Å–æ–∫, –Ω–µ –∑–Ω–∞—á–∏—Ç —Ç–æ—Ç —Ç–æ–∫–µ–Ω –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ‚Äì –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –Ω–∞—Å—Ç–æ—è—â–µ–π –ø—Ä–∏—á–∏–Ω–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é, –Ω–æ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ. Softmax –º–æ–∂–µ—Ç —Ä–∞–∑–¥—É–≤–∞—Ç—å –≤–µ—Å–∞ –±–µ–∑ —Å–∏–ª—å–Ω–æ–π causal impact.
> 
> **–ü—Ä–∏–º–µ—Ä:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: "I totally hated this movie. It's not good." Model outputs 'negative'. Attention might evenly spread, or even put weight on "good" ironically (if model ironically learned "good" in presence of negation cues negative). –í–∑–≤–µ—à–µ–Ω–Ω—ã–µ averaging embeddings ‚Äì high weight on "good" doesn't cause output positive if model via FFN flips it. 
> –í —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö, attention misleads explanation.
> 
> **–ì–¥–µ attention fails as explanation:** 
> - –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å "—Ä–∞–∑–º–∞–∑—ã–≤–∞–µ—Ç" –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: e.g. BERT often uses [CLS] token embedding that aggregates, attention to specific words might not capture knowledge in MLP layers.
> - Complex reasoning: attention may highlight a number, but actual logic done in FFN. 
> - Multi-hop: maybe no single head covers entire chain, scattered among layers.
> 
> **Causality vs correlation:** Attention shows correlation: token j influenced token i's representation. But *why* model made final decision might involve interplay beyond that one link. There might be hidden circuits not visible just by raw weights (like combination of heads outputs). 
> Also, some heads (like in IOI, "inhibitor heads") produce negative influence ‚Äì not obvious from weight sign because weights just for positive aggregation.
> 
> **Permutation test (Serrano & Smith 2019):** They zeroed out highest attention weight tokens and saw if output changes ‚Äì often it didn't significantly, meaning attention weight alone not reliable predictor of importance.
> 
> **Explanations beyond attention:** 
> - *Integrated Gradients, SHAP etc:* attribute output to input tokens by examining model's internal gradients. These often yield different importance order than attention. If they disagree, which to trust? Usually gradient-based considered more faithful to output because they measure effect on output.
> - *Attention rollout:* tries to propagate attention to output, but it still linear combination notion, not capturing non-linear MLP effect.
> 
> **Where attention is still helpful:** It's intuitive for debugging model's understanding. E.g., if we prompt model with some info in context and see it attends to wrong part, maybe it's explanation of an error (model didn't focus where it should). For relative comparisons or analyzing model structure, attention maps used by engineers to identify potential issues (like data bias if always attends certain words).
> Also for mechanistic interpretability, certain heads clearly implement functions, which is explanatory at component level (like "this head does copy").
> 
> **Public controversies:** After "Attention is not explanation", others responded with "Attention is not not explanation" (Wiegreffe & Pinter 2019) arguing that while not direct causation, analyzing attention can still yield insights and isn't worthless„Äê56‚Ä†L9-L17„Äë. They say one can use attention as *analysis tool* in combination with others (e.g., see if attention aligns with known alignments, if not, might indicate model strategy differences).
> So attention interpretation should be done carefully and often confirm with other methods.
> 
> **Conclusion:** Attention sometimes matches human intuition about what context matters (especially in tasks like translation, summarization where referencing input spans is natural). But in tasks where reasoning is implicit (language modeling, classification), high attention to a token doesn't always mean it's why model predicted something ‚Äì because the transformation after attention (FFN) could invert or transform that information.
> 
> **Therefore**:
> - Use attention viz as *qualitative* hint, not definitive explanation.
> - Combine with gradient/ablation to check importance. 
> - Interpret individual head function (which may be stable across examples) more safely than a single example's attention distribution.
> 
> **Analog:**
> In humans, just because you looked at something (attention) doesn't guarantee it was crucial for your decision ‚Äì maybe subconsious processing used something else. Similarly, model's "glance" doesn't fully reveal internal decision.
> 
> **–°—Å—ã–ª–∫–∏:** Jain & Wallace 2019„Äê57‚Ä†L55-L63„Äë; Wiegreffe & Pinter 2019 (argument for alternative)„Äê56‚Ä†L19-L27„Äë; IAAR 2024 (notes bias in focusing solely on attention)„Äê44‚Ä†L187-L195„Äë.
> 
> ## 20. **–ß—Ç–æ –±—É–¥–µ—Ç –ø–æ—Å–ª–µ attention: –º–µ—Ö–∞–Ω–∏–∑–º—ã beyond attention –≤ AGI-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö** 
> **–ü—Ä–µ–¥–µ–ª—ã –≤–Ω–∏–º–∞–Ω–∏—è:** –•–æ—Ç—è –≤–Ω–∏–º–∞–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç –≤ NLP —Å–µ–π—á–∞—Å, –æ–Ω–æ –Ω–µ –ø–∞–Ω–∞—Ü–µ—è. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (–¥–ª—è –æ—á–µ–Ω—å –¥–æ–ª–≥–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —É–∂–µ –∏—â–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã, —Å–º. —Ä–∞–∑–¥–µ–ª 17), –∏ –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è parametric memory: LLM –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ 100k —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –±—É–¥–µ—Ç –ø–æ–º–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –≤ output, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ—Ç –≤–Ω–µ—à–Ω–µ–π –ø–∞–º—è—Ç–∏, —Ç–æ–ª—å–∫–æ context. AGI –≤–µ—Ä–æ—è—Ç–Ω–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏, planning, etc ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ self-attention.
> 
> **–í–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã beyond attention:**
> 
> - **External memory + retrieval:** –£–∂–µ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –≥–¥–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–∫—Ç—ã –≤–æ –≤–Ω–µ—à–Ω–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –∏—Ö –ø–æ –∫–ª—é—á—É (—Ç–∞–∫, –∫–∞–∫ DB). –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ attention, –Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ trainable weights ‚Äì –±–æ–ª–µ–µ dynamic. Eg. KATE, RETRO (DeepMind) ‚Äì language model that retrieves from a database using embeddings. AGI –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –º–æ–¥—É–ª—å ‚Äì –Ω–µ heads –≤–Ω—É—Ç—Ä–∏, –∞ –æ—Ç–¥–µ–ª—å–Ω—ã–π memory system (like hippocampus analog), —á—Ç–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å LLM (cortex analog) via attention-like retrieval queries. 
>  –ù–æ —ç—Ç–æ —É–∂–µ beyond monolithic Transformer ‚Äì architecture with modules.
> 
> - **Mixture-of-Experts (MoE):** –≠—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞: –≤–º–µ—Å—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è heads, –∏–º–µ—Ç—å –º–Ω–æ–≥–æ expert feed-forward networks and a router gating which expert to use per token. That effectively extends idea of specialization: not just heads mixing context, but entire submodels picking depending on context token. MoE can vastly increase parameter count without increasing each inference cost by same factor (only use some experts). Google Switch Transformer (Fedus et al. 2021) is an example with >1000 experts. It had gating, which is like a learned key->expert mapping. Possibly AGI uses many expert modules (some might specialized like separate algorithms, e.g. math solver, code interpreter, etc.), beyond simple heads.
> 
> - **Neurosymbolic / modular reasoning:** Perhaps combination of differentiable attention with symbolic logic modules ‚Äì for tasks requiring precise multi-step reasoning, an AGI might call a symbolic planner or use an internal scratchpad (Chain-of-thought as separate module). Already there's research: e.g., positing an LLM that delegates tasks to sub-programs.
> 
> - **Architectures combining modalities:** Beyond text attention ‚Äì combining visual, auditory ‚Äì might require new mech. Current multi-modal uses cross-attention layers to integrate modalities. AGI likely uses unified attention to all modalities (some research e.g. Perceiver IO by DeepMind had a modality-agnostic latent array that attends to inputs from different modalities and outputs). That extends attention to multi-source scenario and large dims.
> 
> - **Recurrent mechanisms revival:** It's possible that in pursuit of efficiency, some recurrence or adaptive computation per token returns. E.g., Universal Transformers or StructFormer try to iterate multiple times (like thinking steps) with sharing weights. AGI might not just feed-forward once, but reflect. This could be implemented with recurrence or an "inner loop" of attention (like iterative refinement of output). Already some approaches: DeepMind's Recurrent Memory Transformer (2023) which has recurrent state.
> 
> - **Continuous time / fractal attention:** Thinking about brain: not purely discrete tokens. Maybe architectures that handle continuous input streams with dynamic focus, akin to attention but in continuous domain (some efforts with Neural ODEs plus attention).
> 
> - **Beyond attention** as in cognitive sense: attention is one cognitive function. AGI might need others: curiosity drive, goal-oriented planning, self-monitoring. Some of these might be implemented via learning (like RLHF gave a form of goal alignment). But possibly new architectural components: e.g., a "planner module" that uses something like attention but at higher granularity (like attending over possible future actions).
> 
> - **Hardware-driven changes:** If memory and compute allow, one could implement even direct 1M length attention with specialized hardware (some talk of optical computing for transforming sequences via Fourier). Perhaps AGI uses analog computing blending with digital nets.
> 
> - **Algorithmic augmentations:** E.g., using an explicit differentiable stack or tape (Neural Turing Machines concept by Graves 2014). That includes pointer mechanisms beyond soft attention (which is pointer but ephemeral).
> 
> **So, will attention remain central?** For now, it's likely part of core, but an AGI architecture might embed attention inside a larger scaffold. Possibly a hierarchical attention: small-scale attention for short context, another module controlling which parts of memory to bring in (like an index retrieval ‚Äì which is a kind of big-picture attention focusing on relevant documents out of millions).
> 
> **The "boundary of attention vs consciousness":** 
> This is a philosophical question often raised: does improving attention lead to something akin to consciousness (since attention in psychology is part of consciousness)? Or is it just an engineering trick far from self-aware thought? 
> We might think: current attention mechanism is very far from human global workspace. It's more local and doesn't have meta-cognition. 
> AGI might require something beyond ‚Äì e.g., a global workspace architecture where various expert modules communicate via a central blackboard (which somewhat maps to "attention" ‚Äì broadcast relevant info globally).
> So maybe AGI uses a Global Workspace Theory-inspired model: multiple specialist networks whose outputs go into a global context via an attention mechanism that broadcasts winner info back to all ‚Äì giving a conscious-like cycle.
> 
> **Limits of scaling attention:** There's speculation that just scaling Transformers will plateau ‚Äì because some tasks like true reasoning or interacting with environment require additional structure. So beyond-attention might involve differentiable logic circuits or neural symbolic integration.
> 
> **Trends:** Already GPT-4 introduction rumored to incorporate mixture-of-experts. Also, adaptivity: e.g., some sequences need deeper computation, AGI could allocate more steps (some attempts via "depth uncertain networks").
> 
> **In summary:** Attention was revolutionary, but AGI likely augments it with memory, modularity, and planning. Attention might be to AGI what a transistor is to a computer ‚Äì basic building block but not the whole machine. 
> Post-attention era might use it as component but overshadowed by more complex architecture akin to neural OS with attention-based messaging between components.
> 
> **–°—Å—ã–ª–∫–∏:** MoE Switch Transformer (Fedus 2021); RETRO (Borgeaud 2022 retrieval augmented); Neural Turing Machines; Global Workspace theory analogies (go beyond scope maybe just mention conceptually).
> 
> ---
> 
> **–§–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∑–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π:** 
> 
> –í–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º ‚Äì –∏–∑—è—â–Ω–æ–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω–æ –∑–∞—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª–æ —Å–µ–±—è "—Å–ª–æ–µ–º –º—ã—à–ª–µ–Ω–∏—è" ‚Äì –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª–∏ –≤—ã–±–∏—Ä–∞—Ç—å, –æ —á—ë–º –¥—É–º–∞—Ç—å. –ù–æ –º–æ–∂–Ω–æ –ª–∏ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ä–∞–∑–≥–∞–¥–∞–≤ –≤–Ω–∏–º–∞–Ω–∏–µ, –º—ã –ø—Ä–∏–±–ª–∏–∑–∏–ª–∏—Å—å –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–æ–∑–Ω–∞–Ω–∏—è? –û—Ç—á–∞—Å—Ç–∏: —Å–æ–∑–Ω–∞–Ω–∏–µ —Ç–æ–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ –≤—ã–±–æ—Ä, –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ. –û–¥–Ω–∞–∫–æ, –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì —Å—Ç—Ä–æ–≥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ, –Ω–µ –æ–±–ª–∞–¥–∞—é—â–µ–µ —Å–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏–µ–º –∏–ª–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ–º. –í –ø—Ä–µ–¥–µ–ª–µ, –¥–∞–∂–µ –µ—Å–ª–∏ –º—ã —Å–¥–µ–ª–∞–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ —Å–∫–æ–ª—å —É–≥–æ–¥–Ω–æ —Å–ª–æ–∂–Ω—ã–º (–º–Ω–æ–≥–æ –≥–æ–ª–æ–≤, –π–æ—Ç–∞–±–∞–π—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞), —ç—Ç–æ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä–æ–π. –í–µ—Ä–æ—è—Ç–Ω–æ, –ø—É—Ç—å –∫ AGI –ø–æ—Ç—Ä–µ–±—É–µ—Ç –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ‚Äì –≤–Ω–∏–º–∞–Ω–∏—è, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–≥–æ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –º–æ–¥—É–ª—è–º–∏ –∏, –≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º—Å—è —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º (–º–æ–¥–µ–ª—å, —Å–ª–µ–¥—è—â–∞—è –∑–∞ —Å–≤–æ–∏–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º). 
> 
> –í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –≥—Ä–∞–Ω–∏—Ü–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –≤ –∑–∞–¥–∞—á–∞—Ö —Ç—Ä–µ–±—É—é—â–∏—Ö –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞ –∑–Ω–∞–Ω–∏–π –∑–∞ —Ä–∞–º–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –∑–∞–ø–æ–º–∏–Ω–∞—é—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ —Ñ–∞–∫—Ç–æ–≤ –≤ –≤–µ—Å–∞—Ö, –Ω–æ –Ω–µ —É–º–µ—é—Ç –∞–∫—Ç–∏–≤–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã –Ω–∞ –ª–µ—Ç—É –≤ –ø–∞–º—è—Ç—å ‚Äì –¥–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, ‚Äú–ø–æ—Å–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è‚Äù ‚Äì –≤–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ä–∞ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –≥–¥–µ attention-—Å–µ—Ç—å —Å–æ—á–µ—Ç–∞–µ—Ç—Å—è —Å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏, –±–∞–∑–∞–º–∏ –∑–Ω–∞–Ω–∏–π, –∏ –¥—Ä—É–≥–∏–º–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏. 
> 
> Attention –ø—Ä–∏–Ω–µ—Å–ª–æ –Ω–∞—Å –¥–∞–ª–µ–∫–æ ‚Äì –æ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö RNN –∫ –º–æ–¥–µ–ª—è–º, —á—Ç–æ –ø–∏—à—É—Ç –∫–æ–¥ –∏ —Å–æ—á–∏–Ω—è—é—Ç —ç—Å—Å–µ. –í–æ–∑–º–æ–∂–Ω–æ, —á—Ç–æ –ø–æ—Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥ ‚Äì –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –Ω–µ —Ç–æ–ª—å–∫–æ *–≤–Ω–∏–º–∞—Ç—å* –∫–æ–Ω—Ç–µ–∫—Å—Ç—É, –Ω–æ –∏ *–ø–æ–Ω–∏–º–∞—Ç—å —Å–µ–±—è*, —Å–≤–æ–∏ —Ü–µ–ª–∏. –ö–æ–≥–¥–∞-–Ω–∏–±—É–¥—å –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–Ω–µ—Å—ë–Ω –º–æ–¥—É–ª—å —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞: –º–æ–¥–µ–ª—å, —É–¥–µ–ª—è—é—â–∞—è –≤–Ω–∏–º–∞–Ω–∏–µ —Å–≤–æ–µ–º—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é, –æ—Ü–µ–Ω–∫–µ —Å–≤–æ–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠—Ç–æ —É–∂–µ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –≤ –æ–±–ª–∞—Å—Ç—å –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ü–∏–∏. 
> 
> –¢–∞–∫ –∏–ª–∏ –∏–Ω–∞—á–µ, –Ω–∞—Å–ª–µ–¥–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Å—Ç–∞–Ω–µ—Ç—Å—è: –¥–∞–∂–µ –µ—Å–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–∑–º–µ–Ω—è—Ç—Å—è, –ø—Ä–∏–Ω—Ü–∏–ø —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äì –∫–ª—é—á –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É. ‚ÄúBeyond attention‚Äù ‚Äì –Ω–µ –∑–Ω–∞—á–∏—Ç ‚Äúwithout attention‚Äù, –∞ —Å–∫–æ—Ä–µ–µ ‚Äúwith attention and more‚Äù. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è AGI, –≤–µ—Ä–æ—è—Ç–Ω–æ, –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ ‚Äú—Å–ª–æ—è–º–∏ –º—ã—à–ª–µ–Ω–∏—è‚Äù ‚Äì –ø–∞–º—è—Ç—å—é, –º–æ–¥–µ–ª—å—é –º–∏—Ä–∞, —Ü–µ–ª–µ–ø–æ–ª–∞–≥–∞–Ω–∏–µ–º ‚Äì —Ñ–æ—Ä–º–∏—Ä—É—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤–Ω–∏–º–∞–Ω–∏–µ –≤—Å—ë –µ—â—ë –∏–≥—Ä–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é, –Ω–æ —É–∂–µ –Ω–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Å–∫—Ä–∏–ø–∫—É. 
> 
> ---
> 
> **–ó–∞–∫–ª—é—á–µ–Ω–∏–µ:** 
> 
> –í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –º—ã –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –≤—ã—è—Å–Ω–∏–≤, —á—Ç–æ:
> - –ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì –±–∞–∑–æ–≤—ã–µ –º–æ–¥—É–ª–∏, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—é—â–∏–µ —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞„Äê24‚Ä†L252-L259„Äë.
> - –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–ª–æ–∂–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –∫–∞–Ω–∞–ª—ã, —á—Ç–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ —Ä–æ—Å—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤„Äê20‚Ä†L107-L115„Äë„Äê24‚Ä†L258-L261„Äë.
> - –û–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–ø–æ–Ω—Ç–∞–Ω–Ω–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–æ–ª–æ–≤: –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –¥–æ —Å–ª–æ–∂–Ω—ã—Ö ‚Äú–∏–Ω–¥—É–∫—Ç–∏–≤–Ω—ã—Ö‚Äù –≥–æ–ª–æ–≤, –æ—Ç–≤–µ—á–∞—é—â–∏—Ö –∑–∞ in-context learning„Äê35‚Ä†L56-L64„Äë.
> - –°—É—â–µ—Å—Ç–≤—É—é—Ç —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ —Ç–∏–ø—ã –≥–æ–ª–æ–≤: –∫–æ–ø–∏—Ä—É—é—â–∏–µ, —Å–≤—è–∑—ã–≤–∞—é—â–∏–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è, —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –∏ –¥—Ä., —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –∞–Ω–∞–ª–∏–∑–∞–º–∏ –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∞–±–ª—è—Ü–∏–æ–Ω–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏„Äê37‚Ä†L355-L363„Äë„Äê44‚Ä†L15-L23„Äë.
> - –í —Ç–æ –∂–µ –≤—Ä–µ–º—è, –Ω–µ –≤—Å–µ –≥–æ–ª–æ–≤—ã —Ä–∞–≤–Ω—ã ‚Äì –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å, –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –º–æ–∂–Ω–æ —É–¥–∞–ª—è—Ç—å –±–µ–∑ —É—â–µ—Ä–±–∞„Äê20‚Ä†L205-L212„Äë.
> - –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω —Å MLP-—á–∞—Å—Ç—å—é (FFN), –ø—Ä–∏—á—ë–º –≤–Ω–∏–º–∞–Ω–∏–µ —Å–æ–±–∏—Ä–∞–µ—Ç, *–∫—Ç–æ* –≤–∞–∂–µ–Ω, –∞ FFN —Ä–µ—à–∞–µ—Ç, *—á—Ç–æ* —Å —ç—Ç–∏–º –¥–µ–ª–∞—Ç—å„Äê3‚Ä†L278-L285„Äë„Äê38‚Ä†L65-L74„Äë.
> - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–º—É –≤–Ω–∏–º–∞–Ω–∏—é —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π: –æ—Ç –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (Performer)„Äê27‚Ä†L819-L827„Äë–¥–æ –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤—Ä–æ–¥–µ Mamba„Äê26‚Ä†L11-L18„Äë, ‚Äì –Ω–æ –ø–æ–∫–∞ softmax-–≤–Ω–∏–º–∞–Ω–∏–µ –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º.
> - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì –ø–∞–ª–∫–∞ –æ –¥–≤—É—Ö –∫–æ–Ω—Ü–∞—Ö: –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –¥–∞—é—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–æ –≤ –¥—Ä—É–≥–∏—Ö ‚Äì –º–æ–≥—É—Ç –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ„Äê57‚Ä†L55-L63„Äë. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—á–µ—Ç–∞—Ç—å –∏—Ö —Å –ø—Ä–∏—á–∏–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –≤–∫–ª–∞–¥ feed-forward —Å–ª–æ—ë–≤. 
> - –ë—É–¥—É—â–µ–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä AI, –≤–µ—Ä–æ—è—Ç–Ω–æ, –Ω–µ –æ—Ç–∫–∞–∂–µ—Ç—Å—è –æ—Ç –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç –µ–≥–æ: –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–π –ø–∞–º—è—Ç–∏, –º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å (Mixture-of-Experts) –∏ –ø—Ä–æ—á–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã, –≤—ã—Ö–æ–¥—è—â–∏–µ –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ self-attention, –±—É–¥—É—Ç —Å—Ç—Ä–æ–∏—Ç—å—Å—è –≤–æ–∫—Ä—É–≥ –Ω–µ–≥–æ. 
> 
> –í —Ü–µ–ª–æ–º, –≤–Ω–∏–º–∞–Ω–∏–µ –æ–∫–∞–∑–∞–ª–æ—Å—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º ‚Äú—Å–ª–æ–µ–º –º—ã—à–ª–µ–Ω–∏—è‚Äù –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö ‚Äì –ø–æ–∑–≤–æ–ª—è—è –∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–¥–µ–ª—è—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏—è. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞—á–∞—Ç–∫–∏ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ –º–æ–¥—É–ª—è–º. –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –∞–Ω–∞–ª–æ–≥–∏—é —Å —Ä–∞–±–æ—Ç–æ–π –º–æ–∑–≥–∞, –≥–¥–µ —Ç–æ–∂–µ –µ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ç–∏, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏–µ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π —Ñ–æ–∫—É—Å –≤–Ω–∏–º–∞–Ω–∏—è. –û–¥–Ω–∞–∫–æ –¥–æ –ø–æ–ª–Ω–æ–≥–æ –ø–æ–¥–æ–±–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è –µ—â—ë –¥–∞–ª–µ–∫–æ: —Ç–µ–∫—É—â–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–µ –æ–±–ª–∞–¥–∞–µ—Ç —Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ–º –∏–ª–∏ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ü–µ–ª—å—é. 
> 
> **–ì—Ä–∞–Ω–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è** –∫–∞–∫ –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–≥–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –∑–∞–∫–ª—é—á–∞—é—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–æ —Ä–µ—à–∞–µ—Ç –≤–æ–ø—Ä–æ—Å ‚Äú–Ω–∞ —á—Ç–æ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ —Å–µ–π—á–∞—Å?‚Äù, –Ω–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ‚Äú—á—Ç–æ –º–Ω–µ –¥–µ–ª–∞—Ç—å —Å –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ?‚Äù. –î–ª—è —ç—Ç–æ–≥–æ —Ç—Ä–µ–±—É—é—Ç—Å—è –Ω–æ–≤—ã–µ —É—Ä–æ–≤–Ω–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –ú—ã —Å—Ç–æ–∏–º –Ω–∞ –ø–æ—Ä–æ–≥–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è —Å –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –ø–∞–º—è—Ç–∏, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ ‚Äì —á—Ç–æ, –≤–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–∞–Ω–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º –∫ —Å–æ–∑–¥–∞–Ω–∏—é –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. 
> 
> –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –∏–º–µ–Ω–Ω–æ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–ª–æ–∂–∏–ª –¥–æ—Ä–æ–≥—É –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —É—Å–ø–µ—Ö–∞–º –ò–ò, –∏, –≤–µ—Ä–æ—è—Ç–Ω–æ, –µ—â—ë –¥–æ–ª–≥–æ –±—É–¥–µ—Ç —Å–µ—Ä–¥—Ü–µ–º —Å–∏—Å—Ç–µ–º, –Ω–∞–ø—Ä–∞–≤–ª—è—è –∏—Ö –≤–∑–æ—Ä –Ω–∞ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –≤ –ø–æ—Ç–æ–∫–µ –¥–∞–Ω–Ω—ã—Ö ‚Äì –∫–∞–∫ —Å–≤–µ—Ç —Å–æ–∑–Ω–∞–Ω–∏—è, –≤—ã—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ —Ö–∞–æ—Å–∞ –æ–∫—Ä—É–∂–∞—é—â–µ–≥–æ –º–∏—Ä–∞.
> 
> ---
> 
> **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤–∏–¥–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è:** 
> 
> | **–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è** | **–°—É—Ç—å** | **–°–ª–æ–∂–Ω–æ—Å—Ç—å** | **–ü–ª—é—Å—ã** | **–ú–∏–Ω—É—Å—ã** |
> |--------------------------|------------------------------------------|--------------------|---------------------------------------|-----------------------------------------|
> | **–ü–æ–ª–Ω–æ–µ Self-Attention** (softmax) | –ö–∞–∂–¥–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫–æ –≤—Å–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–º —Å –≤–µ—Å–∞–º–∏ softmax. | O(n¬≤ ¬∑ d) –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞–º—è—Ç–∏. | –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ç–æ—á–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å–≤—è–∑–µ–π. Emergent —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ (–∏–Ω–¥—É–∫—Ü–∏—è –∏ –ø—Ä.). | –ü–ª–æ—Ö–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–µ n (–∫–æ–Ω—Ç–µ–∫—Å—Ç). |
> | **–ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ (Multi-Head)** | –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ h –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≤–Ω–∏–º–∞–Ω–∏–π –Ω–∞ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö. | O(h¬∑n¬≤¬∑(d/h)). | –£–ª–∞–≤–ª–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —Å–≤—è–∑–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ„Äê24‚Ä†L252-L259„Äë. –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—é. | –ë–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–Ω–æ –æ–±—ã—á–Ω–æ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ d). –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≥–æ–ª–æ–≤—ã –º–æ–≥—É—Ç –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—Ç—å. |
> | **–õ–æ–∫–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ** | –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–æ—Å–µ–¥–µ–π (–æ–∫–Ω–æ w). | O(n¬∑w¬∑d). | –õ–∏–Ω–µ–π–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ n. –•–æ—Ä–æ—à–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–≤—É–∫, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ). | –ù–µ –ª–æ–≤–∏—Ç –¥–∞–ª—å–Ω–∏–µ —Å–≤—è–∑–∏ –≤–Ω–µ –æ–∫–Ω–∞. –¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–æ–ª–≥–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. |
> | **Sparse –≤–Ω–∏–º–∞–Ω–∏–µ** | –†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã: –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –ø—Ä–∞–≤–∏–ª—É (–¥–∏–∞–≥–æ–Ω–∞–ª—å, —Å–ª—É—á–∞–π–Ω—ã–µ, –≥–ª–æ–±–∞–ª—å–Ω—ã–µ). | O(n¬∑k¬∑d) (k << n). | –ë–ª–∏–∑–∫–æ –∫ –ª–∏–Ω–µ–π–Ω–æ–º—É. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–∞–ª—å–Ω–∏–µ —Å–≤—è–∑–∏ (–µ—Å–ª–∏ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω—ã –≥–ª–æ–±–∞–ª—å–Ω—ã–µ). | –°–ª–æ–∂–Ω–æ –æ–±–æ–±—â–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–∂–µ—Ç –ø–∞–¥–∞—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ç—Ä–µ–±—É—é—â–∏—Ö –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. |
> | **–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ** | –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è softmax —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ: Attention(Q,K,V) ‚âà œÜ(Q) œÜ(K)^T V„Äê27‚Ä†L819-L827„Äë. | O(n¬∑d¬≤) (–ø—Ä–∏ d > n: O(n¬≤)). | –ü–∞–º—è—Ç—å/–≤—Ä–µ–º—è –ª–∏–Ω–µ–π–Ω—ã –ø–æ n. –ú–æ–∂–µ—Ç –ø–æ–∑–≤–æ–ª–∏—Ç—å **–∫–æ–Ω—Ç–µ–∫—Å—Ç 100k+**. | –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –≤–Ω–æ—Å–∏—Ç –æ—à–∏–±–∫—É; –Ω–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ (—Å—É–ø–µ—Ä–ø–æ–∑–∏—Ü–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —à—É–º—É). –ú–µ–Ω–µ–µ —Ç–æ—á–Ω–æ–µ ‚Äú–ø–æ–Ω–∏–º–∞–Ω–∏–µ‚Äù –ø—Ä–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö. |
> | **Flash-Attention** | –¢–æ—Ç –∂–µ softmax-attention, –Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –±–ª–æ–∫–∞–º–∏ –≤ GPU-–ø–∞–º—è—Ç–∏, –±–µ–∑ —è–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è. | O(n¬≤) –ø–æ –æ–ø–µ—Ä–∞—Ü–∏—è–º, –Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –ø–∞–º—è—Ç–∏ (–º–µ–Ω—å—à–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞). | –ü–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ (–¥–æ 16k‚Äì32k), –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏„Äê34‚Ä†L1-L4„Äë. –ë—ã—Å—Ç—Ä–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏. | –ù–µ –º–µ–Ω—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏; –ø—Ä–∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–∏—Ö n –≤—Å—ë –µ—â–µ –º–µ–¥–ª–µ–Ω–Ω–æ. |
> | **Cross-Attention** (–≤ —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä–µ) | –ó–∞–ø—Ä–æ—Å—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–¥–µ–∫–æ–¥–µ—Ä) –æ–±—Ä–∞—â–∞—é—Ç—Å—è –∫ –∫–ª—é—á–∞–º/–∑–Ω–∞—á–µ–Ω–∏—è–º –¥—Ä—É–≥–æ–≥–æ (—ç–Ω–∫–æ–¥–µ—Ä). | O(n_src * n_tgt). | –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏–ª–∏ —ç—Ç–∞–ø–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ–≤–æ–¥: –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ). | –¢—Ä–µ–±—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞. –û–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å: –Ω–µ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ —Å–æ–≤–º–µ—â–∞–µ—Ç –∏–Ω—Ñ–æ (—ç–Ω–∫–æ–¥–µ—Ä->–¥–µ–∫–æ–¥–µ—Ä, –Ω–æ –Ω–µ –Ω–∞–æ–±–æ—Ä–æ—Ç). |
> | **–†–µ–ª—è—Ç–∏–≤–∏—Å—Ç—Å–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ** (Relative) | –í –≤–Ω–∏–º–∞–Ω–∏–µ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è —Å–º–µ—â–µ–Ω–∏—è, –∑–∞–≤–∏—Å—è—â–∏–µ –æ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –ø–æ–∑–∏—Ü–∏—è–º–∏„Äê33‚Ä†L318-L327„Äë. | O(n¬≤) (—Å –º–∞–ª—ã–º–∏ –¥–æ–ø. –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –Ω–∞ —Ö—Ä–∞–Ω–µ–Ω–∏–µ bias). | –ú–æ–¥–µ–ª—å —É—á–∏—Ç—ã–≤–∞–µ—Ç *–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ* –ø–æ–∑–∏—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú—Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ‚Äù —É–∑–Ω–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞). –õ—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–∏–µ –¥–ª–∏–Ω—ã. | –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–∂–Ω–µ–µ; –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –Ω–µ–∫–æ–¥–∏—Ä—É–µ—Ç—Å—è ‚Äì –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø—É—Ç–∞—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫—Ç–æ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ. |
> | **ALiBi** (Attention with Linear Bias) | –í Score –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, —Ä–∞–∑–Ω—ã–π –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã„Äê34‚Ä†L1-L4„Äë. | O(n¬≤). | –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–¥–∞—ë—Ç –≥–æ–ª–æ–≤–∞–º —Ä–∞–∑–Ω—ã–µ ‚Äú—Ä–∞–¥–∏—É—Å—ã –≤–Ω–∏–º–∞–Ω–∏—è‚Äù ‚Äì –æ–¥–Ω–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ, –¥—Ä—É–≥–∏–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ. –£–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω—ã –≤–Ω–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö. | Bias —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á (–Ω–æ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç). –ù–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ç—É–∞—Ü–∏–∏, –≥–¥–µ –¥–∞–ª—å–Ω–∏–µ —Ç–æ–∫–µ–Ω—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å *–≤–∞–∂–Ω–µ–µ* –±–ª–∏–∑–∫–∏—Ö. |
> | **–ú–µ—Ö–∞–Ω–∏–∑–º—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è (S4, Mamba)** | –í–º–µ—Å—Ç–æ —è–≤–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì –æ–±—É—á–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã, –ø–µ—Ä–µ–¥–∞—é—â–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–∫–≤–æ–∑—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–∞–Ω–∞–ª–æ–≥ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è). | O(n¬∑log n) –∏–ª–∏ O(n) –ø—Ä–∏ FFT; –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π. | –û—Ç–ª–∏—á–Ω–æ –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–∑–≤—É–∫, –≤–∏–¥–µ–æ) ‚Äì –Ω–µ —Ç—Ä–µ–±—É–µ—Ç softmax. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –¥–∏—Ñ—Ñ. —É—Ä–∞–≤–Ω–µ–Ω–∏—è–º–∏ (–Ω–µ —Å—Ç—Ä–∞–¥–∞–µ—Ç –æ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ—Å—Ç–∏ —à–∞–≥–æ–≤). | –ú–µ–Ω–µ–µ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ. –ü–æ–∫–∞ —É—Å—Ç—É–ø–∞–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ –∫–∞—á–µ—Å—Ç–≤—É. –ú–æ–∂–µ—Ç —Ç—è–∂–µ–ª–æ –æ–±—É—á–∞—Ç—å—Å—è (–¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ —à–∞–≥–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞). |
> 
> *(–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: d = —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è; n = –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏; h = —á–∏—Å–ª–æ –≥–æ–ª–æ–≤; k = —Å—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ —É—á–∏—Ç—ã–≤–∞–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ sparse.)*
> 
> „Äê47‚Ä†embed_image„Äë *–ü—Ä–∏–º–µ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ GPT-2: –∫–∞–∂–¥–∞—è —Å—É–±-–∫–∞—Ä—Ç–∏–Ω–∫–∞ ‚Äì –º–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –≥–æ–ª–æ–≤—ã. –°–ª–µ–≤–∞ ‚Äì –≥–æ–ª–æ–≤–∞, –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—â–∞—è –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–∞ (1., 2., 3.); –≤ —Ü–µ–Ω—Ç—Ä–µ ‚Äì –≥–æ–ª–æ–≤–∞, —É–¥–µ–ª—è—é—â–∞—è –≤–Ω–∏–º–∞–Ω–∏–µ –≥–ª–∞–≥–æ–ª–∞–º –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏; —Å–ø—Ä–∞–≤–∞ ‚Äì –≥–æ–ª–æ–≤–∞, —Ä–∞—Å–ø–æ–∑–Ω–∞—é—â–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã. –¢—ë–º–Ω—ã–µ –ø–æ–ª–æ—Å—ã —É–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ ‚Äú—Å–º–æ—Ç—Ä—è—Ç‚Äù –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ„Äê44‚Ä†L15-L23„Äë. –¢–∞–∫–∏–º–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–ª—è—é—Ç —Ñ—É–Ω–∫—Ü–∏—é –≥–æ–ª–æ–≤.* 
> 
> **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π:** –ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –≤—ã—à–µ –≤–∏–¥–Ω—ã –ø—Ä–∏–º–µ—Ä—ã attention-pattern‚Äô–æ–≤ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≥–æ–ª–æ–≤ GPT-2„Äê44‚Ä†L15-L23„Äë. –õ–µ–≤–∞—è –ø–∞–Ω–µ–ª—å: –≥–æ–ª–æ–≤–∞, —É –∫–æ—Ç–æ—Ä–æ–π –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø—É–Ω–∫—Ç ‚Äì —è–≤–Ω–æ ‚Äú–ø–æ–Ω–∏–º–∞–µ—Ç‚Äù –Ω—É–º–µ—Ä–∞—Ü–∏—é —Å–ø–∏—Å–∫–∞. –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è: –≥–æ–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω–∞—è —Å –≥–ª–∞–≥–æ–ª–∞–º–∏ ‚Äì –µ—ë –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –ø–æ –≤—Å–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–ª–æ–≤–∞–º, –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ (–≤–µ—Ä–æ—è—Ç–Ω–æ, –æ–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≥–ª–∞–≥–æ–ª–∞). –ü—Ä–∞–≤–∞—è: –≥–æ–ª–æ–≤–∞, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ –∞–∫—Ä–æ–Ω–∏–º–∞—Ö ‚Äì –∫–∞–∂–¥–∞—è –∑–∞–≥–ª–∞–≤–Ω–∞—è –±—É–∫–≤–∞/–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –ø–æ—Ö–æ–∂–∏–µ –ø–æ —Ñ–æ—Ä–º–µ —Ç–æ–∫–µ–Ω—ã. –ü–æ–¥–æ–±–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —Ä–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã –≤—ã—É—á–∏–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏: –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –¥–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö.
> 
> **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–Ω–∏–º–∞–Ω–∏—è –∏ –≥—Ä–∞–Ω–∏—Ü—ã:** –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª–∏–ª –º–æ–¥–µ–ª—è–º –¥–æ—Å—Ç–∏—á—å –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏—Ö –≤—ã—Å–æ—Ç –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –û–Ω –æ—Å—Ç–∞—ë—Ç—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≥–∏–±–∫–æ—Å—Ç—å –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –º—ã—à–ª–µ–Ω–∏—è. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –ø–æ –º–µ—Ä–µ —É—Å–ª–æ–∂–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –æ–±–æ–≥–∞—â–∞—é—Ç—Å—è –Ω–æ–≤—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ —Å–≤–µ—Ä—Ö –≤–Ω–∏–º–∞–Ω–∏—è. –ú–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äì –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π, –Ω–æ –Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –æ–±—â–µ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ï–≥–æ –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–ª–µ–≥–∞–µ—Ç —Ç–∞–º, –≥–¥–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–æ –∏ –æ–ø–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏, —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –ø–ª–∞–Ω—ã, —Ä–∞—Å—à–∏—Ä—è—Ç—å –∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –≤–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠—Ç–∏ –∞—Å–ø–µ–∫—Ç—ã, –≤–µ—Ä–æ—è—Ç–Ω–æ, –ø–æ—Ç—Ä–µ–±—É—é—Ç ‚Äú–º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è‚Äù ‚Äì –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤ –ø–∞–º—è—Ç–∏, –Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏–ª–∏ –¥–∞–∂–µ –≤–æ–∑–≤—Ä–∞—Ç–∞ –∫ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ä–µ–∫—É—Ä—Å–∏–∏. 
> 
> –ù–æ –Ω–∞ –ø—É—Ç–∏ –∫ AGI –≤–Ω–∏–º–∞–Ω–∏–µ, –±–µ–∑—É—Å–ª–æ–≤–Ω–æ, –±—É–¥–µ—Ç –æ–¥–Ω–∏–º –∏–∑ –∫—Ä–∞–µ—É–≥–æ–ª—å–Ω—ã—Ö –∫–∞–º–Ω–µ–π ‚Äì –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–º –≤–æ–ø–ª–æ—â–µ–Ω–∏–µ–º –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–Ω–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—à–∏–Ω–µ –≤—ã–¥–µ–ª—è—Ç—å —Å–º—ã—Å–ª –∏–∑ —Ö–∞–æ—Å–∞ –¥–∞–Ω–Ω—ã—Ö. Understanding and mastering attention heads ‚Äì —ç—Ç–æ –ø–æ —Å—É—Ç–∏ –∏–∑—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤—ã ‚Äú–º—ã—Å–ª–∏‚Äù –Ω—ã–Ω–µ—à–Ω–∏—Ö –ò–ò. –ò —Ö–æ—Ç—è —Å–∞–º–∏ –ø–æ —Å–µ–±–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì –ª–∏—à—å –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏, –≤ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ –∏—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –±–æ–≥–∞—Ç—É—é –º–æ–∑–∞–∏–∫—É, —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–∞–ø–æ–º–∏–Ω–∞—é—â—É—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ. –í —ç—Ç–æ–º —Å–º—ã—Å–ª–µ, –≥–ª—É–±–æ–∫–æ –∏–∑—É—á–∞—è –º–µ—Ö–∞–Ω–∏–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è, –º—ã –¥–µ–ª–∞–µ–º —à–∞–≥ –∫ —Ç–æ–º—É, —á—Ç–æ–±—ã –ø—Ä–∏–¥–∞—Ç—å –º–∞—à–∏–Ω–∞–º –≤—Å—ë –±–æ–ª–µ–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ –ø–æ–¥–æ–±–∏–µ –º—ã—à–ª–µ–Ω–∏—è ‚Äì –∞ –∑–Ω–∞—á–∏—Ç, –¥–≤–∏–∂–µ–º—Å—è –Ω–∞ –µ—â—ë –æ–¥–∏–Ω —à–∞–≥ –±–ª–∏–∂–µ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—É–º–∞.
<!-- UID: 959e4db5-6384-4a99-9652-dfe5c7733d3a -->