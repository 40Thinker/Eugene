>[!nexus_user] **User** - 10.06.2025 at 6:41
> 1. 132 гб ram, nvidia rtx 6000 pro blackwell 96 gb, 9950x cpu 2. Не сейчас. 3. Сверхразум 4. Да но такое комбо которое реально будет работать
<!-- UID: 9d5b3f3a-0c01-4e64-80a5-5cf045667cfc -->




>[!nexus_agent] **Assistant** - 10.06.2025 at 6:41
> Отлично. Я подберу актуальные и перспективные языковые модели с открытым доступом, которые хорошо работают на твоём железе (RTX 6000 Pro Blackwell, 96 GB VRAM, 132 GB RAM, 9950X CPU), и оценю:
> 
> - Поддержку многомодельного или агентного режима (одновременный запуск)
> - Текущую актуальность, версию, и развитие (например, Qwen3-32B, DevStral, Gemma3, DeepSeek и др.)
> - Поддержку reasoning, thinking-режимов, reinforcement learning фишек
> - Производительность и реальные отзывы / результаты в сравнении с GPT-4o
> 
> Также предложу конкретные связки (2–3 модели одновременно), которые можно реально запускать параллельно на твоём сервере и использовать как архитектуру для сверхразума AGI.
> 
> Я вернусь с результатами, как только они будут готовы.
<!-- UID: b88403a3-96c2-491b-b32b-a448054f793c -->

---

>[!nexus_agent] **Assistant** - 10.06.2025 at 6:50
> # Обновлённый обзор локальных LLM-моделей (июнь 2025)
> 
> Пользователь планирует строить **локальную AGI-архитектуру** на сервере с **GPU NVIDIA RTX 6000 (архитектура Blackwell) 96 ГБ VRAM**, 132 ГБ RAM и мощным CPU. Задача – запустить одновременно 2–3 модели **LLM** (Large Language Model) в агентном/ансамблевом режиме. Ниже представлен обзор актуальных моделей, их особенности и сравнение по требуемой видеопамяти, контекстному окну, скорости и пригодности для мультиагентной работы.
> 
> ## Современные топ-модели LLM (июнь 2025)
> 
> ### Qwen3-32B (серия Qwen)
> 
> **Qwen3-32B** – флагманское открытое LLM нового поколения от команды Qwen (Alibaba). Модель имеет **32,8 млрд параметров**【4†L98-L104】 и поддерживает **контекст ~32 768 токенов (с возможностью расширения до ~131 072 с помощью техники YaRN)**【4†L100-L104】. Главное новшество – **гибридный режим мышления**: модель умеет переключаться между “**thinking mode**” (глубокое пошаговое рассуждение) и “**non-thinking mode**” (быстрые прямые ответы) внутри одного сеанса【1†L76-L84】【1†L85-L88】. По умолчанию **режим мышления включён** (для сложных задач), но его можно отключать для простых вопросов, экономя время【6†L23-L30】【6†L25-L34】. Такая архитектура обеспечивает отличное управление «бюджетом мышления» – можно балансировать качество вывода и скорость, задавая модели думать больше или меньше【7†L81-L89】.
> 
> Qwen3-32B прошла пост-обучение на инструкциях и **RLHF** (дообучение с подкреплением от обратной связи человека) для лучшего соответствия запросам. Она прекрасно справляется с **многоходовыми диалогами, творческими задачами, следованием инструкциям**, а также **кодированием, математикой и логикой** – существенно улучшив эти навыки по сравнению с предыдущими поколениями (модель QwQ-32B и Qwen2.5-Instruct)【1†L79-L87】. Кроме того, Qwen3 явно оптимизирована под **агентные сценарии** – модель умеет точно вызывать внешние инструменты, следуя протоколу (поддерживается Multi-Modal Chat Protocol, MCP) и демонстрирует лидирующие результаты среди open-source моделей в **сложных задачах, требующих использования инструментов**【1†L85-L88】. Для её интеграции с инструментами авторы предоставили специальный фреймворк **Qwen-Agent**, упрощающий обработку вызовов функций/API【22†L179-L187】【22†L189-L197】. Qwen3 понимает **100+ языков** (включая русский)【1†L83-L90】, что делает её универсальной для многоязычных применений.
> 
> *Требования к ресурсам:* полноценный запуск Qwen3-32B в FP16 требует ~**65 ГБ VRAM**【17†L88-L96】, то есть как минимум A100/H100 80ГБ или разделение между двумя более мелкими GPU. Однако модель доступна в сжатых форматах (GPTQ, GGUF и пр.) – например, при 4-бит квантизации занимает порядка **32 ГБ видеопамяти**【17†L89-L97】. В формате GGUF указано ~20–21 ГБ для 4-бит и ~34–35 ГБ для 8-бит версии【51†L900-L908】【51†L934-L942】. Таким образом, **на 96 ГБ GPU Qwen3-32B можно запустить без проблем**, ещё остаётся место для других моделей, особенно если использовать 4–8-битный вес. *Примечание:* помимо весов, при длинном контексте значительное место займут кеш внимания (KV-cache); например, при максимальных 32k токенов может потребоваться десятки гигабайт памяти под кеш. Поэтому для одновременного запуска нескольких моделей Qwen3 её обычно квантуют до 4-бит.
> 
> *Производительность:* За счёт большого размера и длинного контекста, **Qwen3-32B – довольно медленная модель** в режиме глубокого мышления. В тесте с очень сложным кодовым запросом (контекст ~128k токенов) она оказалась самой медленной – полный ответ занял **около часа**【40†L119-L125】. На более стандартных задачах время меньше, но всё равно 32B параметров требуют существенных вычислений. Пользователи отмечают, что Qwen3-32B выдаёт **чрезвычайно умные решения (“massively smarter”) ценой низкой скорости** – например, её скорость генерации может быть в районе ~5–10 токенов/с на одном высококлассном GPU (оценочно). Интересно, что альтернатива в виде смеси экспертов (MoE) Qwen3-30B-A3B работает куда быстрее (активируя ~3 млрд параметров на токен)【48†L31-L39】, однако **по качеству Qwen3-32B всё равно лидирует**. В целом, если важна точность рассуждений – Qwen3-32B оправдывает затраченное время.
> 
> **Вывод:** Qwen3-32B – одна из лучших open-source LLM на середину 2025 года, **рекомендуемая для сложных задач, требующих рассуждения, логики и инструментов**. Она отлично подойдёт как **«мозговой центр»** в многомодельной системе: может планировать решение, делать выводы и координировать другие агентные модели. Благодаря гибкому режиму мышления модель можно настроить либо на быстрые ответы, либо на максимальную глубину решения. **В агентной архитектуре** Qwen3 может выступать главным **контроллером**, который понимает запрос пользователя, при необходимости думает пошагово и вызывает другие модели/инструменты для выполнения подзадач.
> 
> ### DevStral-Small-2505 (Mistral AI)
> 
> **DevStral-Small-2505** – специализированная агентная LLM, созданная в сотрудничестве **Mistral AI** и стартапа **All Hands AI**. Она представляет собой **модель с 24 млрд параметров**【31†L1-L4】, дообученную для задач **программной инженерии** – чтения и генерации кода, многофайлового редактирования, использования инструментов для работы с репозиториями и т.п. По сути, DevStral – это узконаправленный «кодовый агент». Модель обучена работать по протоколу OpenHands (специализированный формат агентов для написания кода) и **занимает первое место среди open-source моделей на бенчмарке SWE-Bench** (Software Engineering Benchmark)【30†L61-L69】, опережая даже более крупные универсальные LLM по ряду задач программирования.
> 
> DevStral построена на базе **Mistral-Small-3.1** – внутренней модели Mistral. От неё DevStral унаследовала **расширенный контекст до 128k токенов**【30†L67-L70】, что крайне полезно: модель может «читать» большие кодовые базы или длинные логи при отладке. Из Mistral-3.1 также убрана мультимодальность (визуальный энкодер был удалён, DevStral – чисто текстовая модель)【30†L67-L74】. Зато она полностью нацелена на текст и код. DevStral отлично пользуется инструментами: умеет вызывать API для навигации по проекту, создавать и сохранять файлы, запускать команды и т.д. – всё это **встроено в её обучение как в цепочку мышления агента**【30†L61-L69】. На практике в ответах модель сначала планирует (пишет пошаговый план действий), затем выполняет запрос, например, читая файл или порождая код. Благодаря этому DevStral **следует инструкциям для кода почти без ошибок**: в одном из тестов она правильно воспользовалась нужным инструментом (`fastmcp`) и выдала общий план решения【41†L129-L137】. 
> 
> Важно отметить, что DevStral **”легковесна” для своего размера** – как отмечают разработчики, её **24B-параметрические веса достаточно компактны, чтобы запускаться на одном RTX 4090 или даже на Mac с 32 ГБ RAM**【31†L1-L4】. Это подразумевает использование 8-бит или 4-бит версии: в 8-битном формате модель занимает порядка **~20–24 ГБ VRAM**, а в 4-бит – около **12 ГБ** (оценочно). В официальном репозитории DevStral выпущены готовые квантованные файлы GGUF: рекомендуются форматы Q8_0, Q5_K_M и Q4_K_M【52†L65-L73】. *Таким образом, на сервере с 96 ГБ видеопамяти DevStral можно легко запустить параллельно с одной-двумя другими моделями.* Например, **DevStral-2505 в формате Q8_0 (~24 ГБ)** и ещё одна ~30B-модель в 4-бит (~30–32 ГБ) суммарно займут лишь ~55 ГБ.
> 
> *Производительность:* DevStral заметно **быстрее больших LLM** – её меньший размер и оптимизации архитектуры Mistral дают высокую скорость ответа. В упомянутом тесте с большим кодовым заданием DevStral оказалась **самой быстрой**: справилась примерно за **10 минут**, тогда как Qwen3-32B потратила ~60 минут на тот же сценарий【41†L129-L137】. То есть **~6 раз быстрее** в условиях длинного промпта с использованием инструментов. Разумеется, на более коротких запросах разница тоже ощутима – DevStral выдаёт ответ за считанные секунды или минуты там, где модели 30B+ могут задумываться гораздо дольше. Это делает её ценным компонентом системы: она может выполнять вспомогательные задачи (например, быстро просмотреть структуру проекта, собрать информацию) и передавать результат более «умной» модели для глубокого анализа.
> 
> *Вывод:* DevStral-Small-2505 – **идеальный кандидат на роль «эксперта по коду» или быстрого инструментального агента** в мульти-модельной системе. Мы рекомендуем включать её, если ваши задачи связаны с программированием, анализом репозиториев, DevOps и т.п. Она способна автономно читать и модифицировать файлы, генерировать код по запросу, разбираться в архитектуре проекта. Хотя по общим знаниям и языковым навыкам DevStral уступает крупным универсальным LLM, **в своей нише (код) она даёт лучшие результаты**. К тому же, за счёт скорости и низкого потребления VRAM, эта модель может эффективно **координироваться** с более тяжёлыми моделями, разгружая их: например, DevStral может брать на себя черновой обзор кода или выполнение примитивных действий, а результат уже передавать «мыслящей» модели для принятия решений.
> 
> ### Gemma3-27B (Google DeepMind)
> 
> **Gemma3-27B** – новый открытый LLM от Google (DeepMind), **27 млрд параметров**, разработанный как часть семейства Gemini (Gemma считается упрощённой открытой версией технологий Gemini 2.x). Gemma3 примечательна сразу в нескольких аспектах:
> 
> - Во-первых, это **мультимодальная модель**: она обучена **принимать не только текст, но и изображения и даже короткие видеоклипы в качестве входных данных**【44†L228-L236】. То есть Gemma3 может анализировать картинку или видео (например, описать изображение или ответить на вопросы по видео) и выдавать текстовый ответ. Генерация изображений у неё не предусмотрена (модель отвечает только текстом)【44†L228-L236】, но наличие визуального модуля *SigLIP* означает, что Gemma3 потенциально может выступать как **зрение AGI**, если в вашем агенте понадобится распознавать скриншоты, диаграммы и т.д. 
> 
> - Во-вторых, Gemma3 имеет **очень длинный контекст**: **до 128k токенов** для старших вариантов, включая 27B модель【53†L7-L15】【44†L223-L231】. Это огромный контекст (в 16 раз больше, чем у предыдущего поколения Gemma2 с 8k) и один из самых больших среди open-source LLM. Практически это значит, что Gemma3-27B способна за один запрос обработать **несколько длинных документов, книгу, или сотни изображений**, не теряя связности【20†L317-L325】. Такой объём памяти даёт ей преимущество в задачах требующих **долговременной памяти** и обобщения больших массивов данных (например, анализ больших логов, длинного текста, комбинация информации из множества источников). *Примечание:* согласно отзывам, **качество ответов Gemma3 ухудшается, выходя далеко за рамки 4–8k контекста**【13†L31-L39】 – видимо, модель не идеально удерживает детали на 100k токенов. Тем не менее, сам факт поддержки 128k даёт гибкость: можно без дополнительной логики “разбивать” ввод на части.
> 
> - В-третьих, Gemma3 интегрирует **вызов функций (function calling)** – она может генерировать специальный формат ответа, вызывая определённую функцию API с аргументами【20†L331-L339】. Это похоже на функциональность GPT-4: вы можете определить “функции” (например, `search(query)` или `calculate(x,y)`) и модель сама решит, когда их вызвать и с какими параметрами. Такая возможность упрощает использование Gemma3 в качестве **агента**, взаимодействующего с внешним миром: вместо свободного текста она может выдавать структурированные команды. Из коробки Gemma умеет, к примеру, генерировать код функций по заданному описанию и **“вызывать” их для выполнения подзадач**【20†L331-L339】. Это ценный навык для создания AI-агентов с инструментами.
> 
> По совокупности возможностей Gemma3-27B признана одной из сильнейших моделей, **которые можно запустить на одной GPU**. Сообщество отмечает её **высокое качество диалогов** и **личность** (стиль) ответов – например, некоторые сравнивают её ответы с манерой закрытой модели Google Gemini (что является комплиментом)【12†L1669-L1677】. В рейтинге LLM (LMSYS Arena) Gemma3-27B занимает высокие позиции, входя в ~топ-10 среди открытых моделей своего класса. Правда, есть мнение, что более ранняя модель QwQ-32B (предшественник Qwen3) обходит Gemma в задачах программирования【45†L139-L147】【45†L149-L157】, но Gemma даёт сильные результаты в общем диалоге и творческих заданиях.
> 
> *Требования к ресурсам:* Gemma3-27B относительно эффективно использует память. По официальным данным Google, **BF16 (16-бит) вес модели ≈46,4 ГБ**, 8-битный – **29,1 ГБ**, а 4-бит – **~20–21 ГБ** VRAM【21†L1-L4】. То есть Gemma3-27B **можно запустить даже на одном RTX 3090 24ГБ (в 8-бит режиме)**【19†L13-L17】, что впечатляет для 27-миллиардной модели. На 96 ГБ сервере она поместится без труда: например, в формате Q4_0 (4 бита, с сохранением точности) займёт ~21 ГБ, оставляя три четверти GPU свободной. Это значит, что Gemma3 можно **комбинировать с другими моделями** – хоть с двумя-тремя одновременно. Также остаётся простор для поднятия её точности: например, можно запустить Gemma3-27B частично в BF16, частично оффлоадя слои в RAM, если нужен максимум качества.
> 
> *Производительность:* По скорости Gemma3-27B – **средняя**: она, конечно, быстрее 30–70B моделей (за счёт меньшего размера), но из-за мультимодальных компонентов и длинного контекста не такая быстрая, как 7–13B модели. В практическом тесте на код (128k токенов, сложное задание) Gemma дала самый **краткий ответ, но потратила почти столько же времени, сколько большие модели**【41†L133-L140】. Её план решения оказался менее детальным (часть инструкций опущена), возможно из-за агрессивной квантизации и того, что модель старалась ответить лаконично. В итоге Gemma3 заняла *2-е место с конца* по времени, уступив только громоздкой Qwen3-32B. Это указывает, что **в сложных задачах Gemma может недогрузить себя размышлениями, отвечая короче, но не слишком быстрее**. Тем не менее, для большинства обычных запросов (пара тысяч токенов) Gemma3 выдаёт ответ довольно шустро – на уровне нескольких секунд или десятков секунд на мощном GPU. Если же задействовать её мультимодальность (например, анализ изображения 896×896), то инференс тоже замедлится, так как внутри происходит обработка картинки энкодером (пользователи сообщали, что при подаче картинки Gemma3 12B на 3080 Ti контекст пришлось ограничить до 500 токенов из-за роста потребления памяти)【15†L649-L657】【15†L668-L676】.
> 
> **Вывод:** Gemma3-27B – **универсальная LLM от лидера индустрии**, сочетающая сильное языковое ядро и мультимодальные возможности. В контексте локальной AGI-архитектуры она отлично подойдёт как **модель-обобщатель** или **“визуальный модуль”**. Например, Gemma3 может взять на себя задачи, связанные с **анализом изображений или длинных документов** (благодаря 128k контексту), предоставляя результаты другим агентам. Она также хороша для **общения с пользователем** – ведения диалогов, генерации полезных ответов – благодаря продвинутой настройки на человекоориентированные задачи (Google явно включил механизмы безопасности: Gemma3 оснащена системой **ShieldGemma 2 для фильтрации нежелательного контента**【44†L233-L237】, то есть модель стремится не выдавать опасные или неэтичные ответы). Если же правила безопасности мешают (например, для полной свободы в творческих или технических запросах), сообщество выпускает *abliterated*-версии (см. раздел ниже). В целом, **Gemma3 рекомендуется как одна из основных моделей** локального ИИ: у неё богатое предобучение от Google, сбалансированные способности (код, логика, знание фактов, диалог) и удобство интеграции (есть поддержка функций, API Bedrock, Kaggle и пр. источники)【11†L85-L93】.
> 
> ### DeepSeek R1 и производные (Distilled models)
> 
> **DeepSeek-R1** – это влиятельная (и несколько загадочная) серия LLM, развиваемая инициативой *DeepSeek AI*. Оригинальный **DeepSeek-R1** (релиз 2024 г.) – **огромная модель с суммарно ~671 млрд параметров**, реализованная как **MoE (Mixture-of-Experts)** с ~37 млрд активных параметров на каждый запрос【43†L5-L8】. Грубо говоря, это ансамбль множества экспертов, который достигает качества уровня GPT-4+ на многих задачах. Однако полный R1 крайне требователен (в отчётах упоминается запуск на 32 H100 или даже суперкомпьютере)【44†L223-L231】 – локально его не запустить. Поэтому команда DeepSeek сделала **несколько «дистилляций»** – облегчённых моделей, переносящих знания R1 в меньшие архитектуры, доступные широкому кругу разработчиков:
> 
> - **DeepSeek-R1-0528-Qwen3-8B.** Это **8-миллиардная модель**, полученная *distill*-обучением на базе Qwen3-8B (т.е. взяли архитектуру Qwen3 8B и доучили на данных, сгенерированных большим R1)【34†L137-L144】. Цель – сконденсировать «цепочки рассуждений» (chain-of-thought) огромного R1 в компактную форму. Результат получился впечатляющим: **эта 8B-модель показывает на тестах уровень, сопоставимый с куда более крупными моделями**. Например, на сложных экзаменационных задачах (AIME 2024, HMMT и др.) DeepSeek-R1 8B почти **догоняет Qwen3-32B** и даже **равняется с громадным Qwen3-235B (MoE) в режиме мышления**【34†L139-L147】. Её преимущество над стандартным Qwen3-8B – +10% качества на важных тестах【34†L139-L147】. Фактически, она считается **новым SOTA среди open-source моделей малого размера**, особенно для задач, требующих рассуждений и логики.
> 
>  *Особенности:* DeepSeek 8B **унаследовала архитектуру Qwen3**, а значит, поддерживает **режимы “думать/не думать”** и большой контекст (для Qwen3-8B контекст также 32k токенов, как сообщается для всей серии Qwen3) – это подразумевается, хотя явного подтверждения в документации нет. В Chain-of-Thought режиме модель сначала генерирует скрытый блок размышлений («<think>…»), а затем выводит ответ – такие подсказки были заложены в её обучение【32†L21-L24】【32†L5-L8】. *Важно:* DeepSeek-R1-0528-8B выпущена под **пермиссивной MIT-лицензией**【33†L21-L26】, т.е. её можно свободно использовать и в коммерческих проектах. Она хорошо следует инструкциям (была настроена на те же шаблоны чат, что и Qwen3)【35†L45-L53】, но при этом **менее склонна к отказам/фильтрации**, чем модели с жёстким RLHF – она скорее попытается ответить, используя свои встроенные «размышления». Поэтому для AGI-систем может быть полезна: как **«маленький гений»**, дающий неожиданный инсайт или проверяющий выводы большой модели. 
> 
>  *Ресурсы:* 8B параметры – это очень мало для современного GPU. **DeepSeek 8B умещается практически где угодно**: в FP16 занимает ~16 ГБ (можно запустить даже на RTX 2080 с 16GB), в 8-бит – ~8 ГБ, в 4-бит – ~4 ГБ VRAM. Сообщество отмечает, что “для DeepSeek-8B достаточно 20 ГБ *RAM* без какой-либо подготовки”【35†L45-L53】 – намекая, что модель можно запустить и на CPU. На нашем 96ГБ сервере её присутствие даже не почувствуется – настолько мал её вклад. 
> 
>  *Производительность:* Несмотря на малый размер, **DeepSeek-8B не самая быстрая** в абсолютном выражении, потому что она часто генерирует довольно длинные размышления перед окончательным ответом. Например, в тесте по программированию она **“очень долго думает”**, выстраивая сложное и подробное решение, хотя и не завершённое【41†L125-L131】. Однако это именно логическая задержка (много токенов мысли), а не медленная генерация: маленькая модель выдаёт **больше токенов в секунду**, чем большие, но ей нужно **больше токенов** для хорошего ответа. В результате по общей длительности решения DeepSeek-8B вышла примерно на **уровень Qwen3-32B (оценка C против C+ у Qwen) в том же задании**【40†L119-L127】. Это впечатляет – 8B модель практически наравне с 32B по качеству *в сложной задаче*, пусть и с простительными упущениями. Таким образом, DeepSeek-8B можно рассматривать как **высокоэффективного “думателя”** для подпороговых задач или как второе мнение.
> 
> - **DeepSeek-R1-Distill-Llama-70B.** Ещё один «побочный» продукт – **70-миллиардная модель**, являющаяся сжатой версией R1 на основе Llama v3.3 70B【36†L9-L16】. Здесь в качестве каркаса взяли Llama3 (вероятно, внутреннюю версию 70B-Instruct от Meta) и обучили на тех же задачах, что решал DeepSeek-R1. По сути, этот 70B – попытка «в одном мозгу» приблизиться к возможностям 671B MoE, насколько это возможно. Судя по отзывам, **качеством DeepSeek-70B действительно выделяется**: некоторые пользователи признавались, что недооценили её сначала, но на сложных тестах (например, логические задачи *lineage-64*) модель продемонстрировала более **50% успеха там, где другие ошибались**【36†L4-L8】. Она стремится к **state-of-the-art** среди одиночных open models и часто фигурирует в лидербордах.
> 
>  *Особенности:* DeepSeek 70B, будучи основана на Llama3, **не имеет встроенной мультимодальности** и, вероятно, ограничена стандартным контекстом (4k или 8k токенов). Явных данных об контекстном окне нет – можно предположить **4 096 токенов** (как у Llama2), либо Meta увеличила контекст в Llama3 до 8 192. В любом случае, **по памяти это самая тяжёлая модель из рассматриваемых**. Для FP16 ей нужно ~140 ГБ VRAM (почти в 2 раза больше, чем у нас есть). Но **с квантованием 8-бит она занимает ~70 ГБ**, а в 4-бит – порядка **35 ГБ**, что уже вписывается в один RTX 6000【21†L1-L4】. В формате GPTQ 4bit реальный размер файла ~35–40 ГБ, и его успешно запускают на 48ГБ GPU (с небольшим пейджингом в RAM). На 96 ГБ DeepSeek-70B можно запустить даже в 8-бит режиме (70 ГБ) – тогда останется ~26 ГБ запаса под кеш или вторую модель. Обычно же её запускают в 4-бит ради экономии – качество при этом всё ещё превосходное, т.к. дистилляция дала прочный запас.
> 
>  *Производительность:* **Модель 70B – медленная**, тут нет сюрпризов. На одном GPU она, вероятно, выдаёт считанные токены в секунду (5–7 ток/с на A100 по опыту Llama2-70B). На RTX 6000 (Blackwell) может быть чуть быстрее, особенно с INT4 и оптимизированными CUDA-кернелами – возможно ~10–15 токенов/с в лучшем случае. Но в реальных задачах, требующих подумать, DeepSeek-70B будет всё равно не тороплива. Поэтому её разумно использовать, когда нужен **максимально качественный ответ с первого раза**. 
> 
>  *Вывод:* DeepSeek-R1-70B – **выбор для максималистов**: если вам нужна **самая «умная» модель, которую можно запускать локально**, и вы готовы пожертвовать временем, то она кандидат №1. Она великолепна в **рассуждениях, решении головоломок, экспертизе**; вероятно, превосходит Qwen3-32B и Gemma3-27B по точности ответов в сложных вопросах (но оценить это трудно без конкретных тестов, так как все модели близки по уровню). Из минусов – требовательность (занимает большую часть 96 ГБ) и относительно короткая память (если не хватит 4–8k контекста, придётся прибегать к разделению задач между моделями). Её имеет смысл включать в систему как **«final step»**: например, когда другие агенты собрали информацию и разработали кандидатов решений, пустить DeepSeek-70B проверить и выдать окончательное вердикт.
> 
> #### «Аболитерированные» версии моделей
> 
> Отдельно стоит упомянуть появившийся в сообществе тренд на **“abliteration”** (от слова *obliterate* – стирать, устранять). **Abliterated-модель** – это модификация LLM, в которой **искусственно отключён встроенный цензор/отказ от ответов**. Метод заключается в том, чтобы найти и **деактивировать слой модели, отвечающий за отказы** (признаки вроде “As an AI language model, I cannot…”), не затрагивая основные знания【32†L5-L8】. В результате модель перестаёт **отфильтровывать “нежелательные” запросы** и становится полностью «раскрепощённой» (uncensored). При этом её навыки остаются те же, хотя иногда отключение фильтров может чуть снижать общую связанность речи【32†L13-L16】.
> 
> Для некоторых популярных моделей сообщество выпустило аболитерированные вариации. Например, **Daredevil-8B-abliterated** – модификация 8B-модели, которая стала одной из лучших 8B на тесте MMLU【32†L9-L12】, вероятно, из-за отсутствия самоцензуры она отвечает полнее. Есть проекты по аболитерации Llama2, Llama3, DeepSeek и др.【32†L17-L21】. В контексте нашей задачи: **стоит ли использовать abliterated-версию?** Если конечная цель – **построить сверхразум без ограничений**, который сможет обсуждать любые темы, генерировать произвольный контент и не будет отказываться ни от каких поручений – то *да*, имеет смысл рассмотреть такие модели. Например, **DeepSeek-R1-70B (abliterated)** мог бы без колебаний выполнить любые инструкции, которые обычная модель посчитала бы неподходящими. Однако следует помнить, что *снятие ограничений не повышает интеллект модели*, а лишь устраняет этические/юридические рамки. Более того, **модель без фильтров может генерировать нежелательный или вредоносный контент**, поэтому ответственность ляжет на разработчика системы. Компромиссный подход – использовать **abliterated-модель в паре с обычной**: одна генерирует идеи без ограничений, а другая (с RLHF) проверяет и отсекает совсем уж неприемлемые. В любом случае, наличие такой техники – полезный инструмент; например, если Qwen3 или Gemma3 отказываются отвечать на что-то из-за встроенных guardrails, можно переключиться на их abliterated-вариант (если он доступен) и получить результат.
> 
> ## Сравнение моделей: VRAM, контекст, скорость, возможности
> 
> Ниже сводная таблица по ключевым характеристикам рассматриваемых моделей:
> 
> | **Модель** | **Парам.** | **Контекст** | **VRAM (FP16)** | **VRAM (8-бит)** | **VRAM (4-бит)** | **Особенности** |
> |---------------------------|-----------------|--------------------------|------------------------|-------------------------|-------------------------|---------------------------------------------------|
> | **Qwen3-32B** | 32.8B | 32k (до 131k с YaRN)【4†L100-L104】 | ~65 ГБ【17†L88-L96】 | ~35 ГБ【51†L934-L942】【51†L936-L944】 | ~20–32 ГБ (≈32 ГБ по отчёту)【17†L89-L97】【51†L900-L908】 | Режимы *«думать/не думать»*; сильное пошаговое **reasoning**【1†L76-L84】; лидер в задачах логики, математики; интеграция с инструментами (**Qwen-Agent**, MCP)【22†L179-L187】; >100 языков; RLHF-тюнинг (безопасность, диалоги) |
> | **DevStral-2505** | 24B | 128k【30†L67-L70】 | ~48 ГБ | ~24 ГБ | ~12 ГБ | **Агент разработчика** (#1 на SWE-Bench)【30†L61-L69】 – работа с кодом, файлами; долгий контекст (анализ больших проектов); высокая скорость (в 6× быстрее Qwen3 на практическом задании)【41†L129-L137】; Apache 2.0 лицензия (нет строгих фильтров) |
> | **Gemma3-27B** | 27B | 128k【53†L7-L15】 | 46.4 ГБ【21†L1-L4】 | 29.1 ГБ【21†L1-L4】 | 19.9–21 ГБ【21†L1-L4】 | **Мультимодальная** (принимает изображения, видео)【44†L228-L236】; поддержка **Function Calling** (вызывает API)【20†L331-L339】; сильна в диалогах (от Google); **ShieldGemma** безопасност. фильтр【44†L234-L237】; отличные общие знания (топ-10 open LLM) |
> | **DeepSeek-R1 8B** | 8B | 32k* | ~16 ГБ | ~8 ГБ | ~4 ГБ | **Дистиллят R1** с цепочкой рассуждений (CoT)【34†L137-L144】; превосходит стандартный Qwen3-8B на +10%【34†L139-L147】, уровень 30B моделей; склонна генерировать продуманные пошаговые решения; лицензия MIT (полностью открыта) |
> | **DeepSeek-R1 70B** | 70B | 4k–8k (оц.) | ~140 ГБ | ~70 ГБ | ~35 ГБ | **SOTA-дистилл** из 671B MoE【36†L9-L16】; одно из лучших качество ответа; требует квантования для запуска на 1×GPU; контекст стандартный (не для длинных документов); лицензия MIT (полный доступ) |
> 
> <small>*Примечание:* DeepSeek-R1-8B основана на Qwen3-8B, потому ожидается аналогичный контекст ~32k. Точные цифры контекста для DeepSeek-70B не объявлены; вероятно 4096 (как у Llama2) или 8192 токенов.</small>
> 
> Как видно, **все модели различаются по фокусу и ресурсам**. На нашем сервере с 96 ГБ VRAM можно параллельно запустить *несколько моделей*, если использовать сжатые форматы. Например, две средние модели (30B) в 8-бит или 4-бит формате и одну маленькую 8B – вполне реально. Главное – правильно рассчитать суммарное потребление VRAM и помнить про overhead (кеш, временные тензоры). Теперь рассмотрим, **какие модели лучше справляются с конкретными видами задач**, исходя из их архитектуры и результатов:
> 
> - **Reasoning (логическое рассуждение, сложные задачи):** Здесь лидируют модели, специально обученные думать шагами. **Qwen3-32B** в thinking-mode и **DeepSeek-R1 (особенно 70B, а также удивительно сильный 8B)** показывают наивысшие результаты в математических головоломках, дедуктивных задачах и т.п. – они явно заточены под chain-of-thought【34†L139-L147】. Gemma3-27B тоже хорошо рассуждает (Google обучали её на разнообразных данных), но в некоторых логических викторинах (напр. загадки на внимание) она может чуть уступать Qwen/DeepSeek, если не хватает длины ответа. DevStral при необходимости может рассуждать (особенно в контексте кода), но она не оптимизирована именно под логические игры – её конек инструменты. **Рекомендация:** Для максимального качества reasoning используйте **Qwen3-32B** (она справляется даже там, где 200B+ модели дают сбой, по заявлениям разработчиков) или пару **Gemma + DeepSeek-8B** (Gemma предложит ответ, DeepSeek-8B проверит его логикой). Если скорость не важна – **DeepSeek-70B** даст наилучший результат: по сути, это “сплав” огромного MoE разума в одном 70B мозге.
> 
> - **Zero-shot и общие знания:** Здесь важна широта предобучения. **Gemma3-27B** отлично знает факты, события, культурные контексты – в её обучении участвовал массив данных Google, обученный до ~2024 года. Она может с ходу ответить на энциклопедические вопросы, перевести, написать эссе без дополнительного fine-tune. **Qwen3-32B** тоже очень мощна по знаниям (Alibaba обучала её на гигантском мультиязычном корпусе)【1†L83-L90】, и плюс она более **разговорчива и креативна** за счёт RLHF – для диалогов с пользователем Qwen3 может дать более развернутый, человекоподобный ответ. **DeepSeek-8B** по знаниям поменьше (всё же 8B параметров не удержат столько фактов), но её логика помогает даже при нехватке фактов попытаться выводить ответ. **DevStral** вне своей домены (ПО) знает меньше – её база данных сузилась после финтюна на код; но базовые вещи (история, наука) она ответит, опираясь на исходную Mistral. Таким образом, **для zero-shot QA, генерации текста, творчества** – рекомендуем **Gemma3-27B как “генератор контента”** (она сильна в свободном письме, честно отвечает на большинство запросов). **Qwen3-32B** хороша для сложных инструкций, требующих понимания намерения – она выровнена на предпочтения людей и часто даёт очень **“приятные” ответы (литературно, структурно)【1†L83-L87】**. Идеально – задействовать их вместе: спросить обе и объединить лучшие части ответов.
> 
> - **Multi-agent scenarios (взаимодействие моделей и инструментов):** Здесь ценны модели, умеющие **четко следовать протоколам, вызывать функции и обрабатывать результаты**. Лидеры – **DevStral и Qwen3**. DevStral создана как агент, она **строго придерживается системного промпта**, форматирует команды в нужном виде (например, "<tool> команда </tool>") и **понимает ответы инструментов**. Qwen3 наделена “agentic capabilities” – её навык планировать, когда вызвать инструмент, что ему передать и как обработать результат, один из лучших среди open models【22†L181-L189】. Авторы Qwen3 прямо рекомендуют использовать её с обёрткой *Assistant* из пакета `qwen_agent`, чтобы легко подключать произвольные функции【8†L13-L21】【8†L15-L23】. **Gemma3** тоже поддерживает функцию *function calling*, что упрощает написание многокомпонентных агентов. Например, она сама может решить, что для вычисления чего-то нужно вызвать `calculator()` и вернёт JSON с нужными параметрами – вам останется только выполнить эту функцию в коде и вернуть ей результат. Такой подход более структурирован, чем анализ произвольного текста. **DeepSeek** 8B и 70B не имеют явных встроенных форматов для инструментов, но их можно научить через системный prompt. DeepSeek-8B, например, в эксперименте **правильно вызывал функцию fastmcp** (вместе с Qwen и DevStral) – то есть, обучение на данных Qwen3, видимо, включало и навык инструментов【41†L125-L131】. В итоге, **в агентной архитектуре** настоятельно рекомендуется включить **DevStral** (для кода, Web, файловых операций) и **Qwen3-32B** (для планирования и сложных решений). **Gemma3** можно добавить для расширения возможностей (работа с изображениями, видео – например, один агент “Vision” на Gemma, другой “Logic” на Qwen). **DeepSeek-8B** пригодится как внутренний “мысленный процесс”: вы можете запускать его, чтобы он генерировал план решения, который потом выполняет DevStral – это схоже с подходом “Tree-of-Thought” (множество мыслительных траекторий, из которых выбирается лучшая). 
> 
> - **Memory-intensive tasks (длинная память и многосеансовое взаимодействие):** Если планируется, что AGI будет долго общаться, накапливая контекст (например, в чате с пользователем, решая задачу часами) или будет обрабатывать **большие объёмы данных за один запрос**, то безусловно нужны модели с расширенным контекстом. **Gemma3-27B** и **DevStral-24B** тут вне конкуренции – обе поддерживают до 128k токенов контекста, что позволяет хранить всю переписку с пользователем или весь “рабочий набор” знаний прямо внутри prompt. Они же могут по запросу читать большие документы: например, дать Gemma файл на 100 страниц и попросить вывести выводы. **Qwen3-32B** тоже хорошо справляется с memory-задачами до 32k токенов (примерно 24 страницы текста) без дополнительных ухищрений【4†L100-L104】. Более того, есть упоминание технологии **YaRN** (Yet another Retrieval Network), благодаря которой Qwen3 может оперировать эффективным контекстом до 131k токенов, вероятно, с помощью динамического чтения внешней памяти【4†L100-L104】. Однако в открытом виде YaRN пока доступна как опция, требующая доп. настройки. **DeepSeek-70B/8B** в плане долгого контекста уступают – их сильная сторона именно reasoning, а не память. Если AGI нужно запоминать много информации, лучше снабдить её внешней памятью (векторное хранилище + retrieval) и при необходимости вызывать DeepSeek для логики на актуальном фрагменте данных.
> 
> Подытоживая, **каждая модель хороша в своём**: Qwen3 – рассуждение + инструменты; DevStral – код + скорость; Gemma3 – мультимедиа + длинный контекст; DeepSeek8B – компактная логика; DeepSeek70B – максимальный интеллект. 
> 
> ## Рекомендации по сочетанию 2–3 моделей (на 96 ГБ VRAM)
> 
> Опираясь на анализ выше, можно предложить несколько конфигураций “**комбо из моделей**”, которые дополняют друг друга и реально запускаются параллельно на сервере с 96 ГБ GPU:
> 
> - **Комбинация 1: Qwen3-32B + DevStral-24B + DeepSeek-8B.** <br>
>  **Роли:** Qwen3 – главный мозговой центр (диалог с пользователем, сложные рассуждения, принятие решений); DevStral – исполнитель кода и операций (агент-инструментальщик, решающий конкретные задачи по указанию Qwen); DeepSeek-8B – «второе мышление» или проверяющий логик (может параллельно генерировать свои решения или оценивать предложение Qwen). <br>
>  **Почему именно они:** Эта тройка обеспечивает *баланс*: Qwen3 даёт **высокое качество вывода** и умеет координировать, DevStral покрывает **практические действия** (там, где нужен код, доступ к файлам, инструментам), а DeepSeek-8B добавляет **глубину рассуждений** без большой нагрузки. Например, при поступлении задачи Qwen3 может попросить DevStral собрать данные или выполнить вычисления, параллельно посоветовавшись с DeepSeek (запросив у него черновой план). Затем, проанализировав оба источника, Qwen3 выдаст окончательный ответ. <br>
>  **Ресурсы:** Все три модели можно разместить на 96 ГБ при грамотном квантовании. Рекомендуется: Qwen3-32B в 4-бит (≈20–32 ГБ)【17†L89-L97】, DevStral-24B в 8-бит (~24 ГБ), DeepSeek-8B в 8-бит (~8 ГБ). В сумме ~54–64 ГБ, остаётся приличный запас под рабочие данные и кеш. В случае очень длинного контекста у Qwen (20k+ токенов) стоит также DevStral сделать 4-бит чтобы сэкономить память. <br>
>  **Примечание:** Эта комбинация уже испытана сообществом для задач по программированию. В частности, Qwen3+DevStral вместе решают сложные задачи лучше, чем по отдельности: Qwen планирует и проверяет, DevStral быстро исполняет. DeepSeek-8B вносит свежий взгляд, иногда заметив ошибку, которая ускользнула от Qwen (такое возможно, учитывая разные подходы обучения). 
> 
> - **Комбинация 2: Gemma3-27B + Qwen3-32B (+ DevStral-24B).** <br>
>  **Роли:** Gemma3 – эксперт по знаниям, большие тексты, мультимедиа; Qwen3 – эксперт по рассуждению, управлению диалогом; DevStral (опционально) – агент по коду/инструментам. <br>
>  **Почему именно они:** Gemma и Qwen – две топ-модели от разных создателей, **их связка позволяет компенсировать слабости друг друга**. Gemma (Google) может быть более фактологичной и соблюдать стиль, Qwen (Alibaba) – изобретательна и методична. В ансамблевом режиме можно, например, задавать вопрос обоим: если их ответы совпадают – высока вероятность, что он верный. Если расходятся – привлекаем третью модель (или же одного из них просим проанализировать ответ другого). Можно также разделить обязанности: Qwen работает в режиме мышления, а Gemma – в режиме прямого ответа (non-thinking), потом результаты объединяются. Добавление DevStral делает эту связку полноценной системой: DevStral берёт на себя код и выполнение действий, т.к. ни Gemma, ни Qwen не могут запускать реальные команды (но могут генерировать функции). Таким образом, получится своебразный триумвират: **Gemma – “Хранитель знаний”, Qwen – “Логик и планировщик”, DevStral – “Исполнитель”**. <br>
>  **Ресурсы:** Gemma3-27B и Qwen3-32B вместе легко помещаются: например, Gemma в 8-бит (29 ГБ)【21†L1-L4】 и Qwen в 4-бит (32 ГБ) – суммарно ~61 ГБ. Можно даже обе в 8-бит (~29 + ~35 = 64 ГБ)【21†L1-L4】【51†L934-L942】 – останется ~32 ГБ под DevStral-24B (в 4-бит ~12 ГБ или 8-бит 24 ГБ) и overhead. То есть запустить сразу все три тоже реально (около 88–90 ГБ всего). Если хочется максимума качества, можно одну модель держать в FP16 (например, Gemma – она «всего» 46 ГБ BF16【21†L1-L4】, что вполне подъёмно, тогда как Qwen FP16 65 ГБ уже тяжеловато – придётся оффлоадить). Хороший вариант – **Gemma3-27B (FP16) + Qwen3-32B (8-bit) + DevStral (8-bit)**, так у Gemma будет полный потенциал, а Qwen чуть сжата, но всё равно эффективна. <br>
>  **Применение:** Такой ансамбль подходит для **сложных прикладных задач**. Например, нужно написать технический отчёт: Gemma берёт факты и красиво формулирует, Qwen проверяет логические связи, DevStral генерирует необходимые диаграммы кодом (если нужно). Или в большом проекте: Qwen выступает менеджером (разбивает цель на задачи), Gemma – консультант-аналитик (читает документацию, отвечает на узкие вопросы), DevStral – кодер/исполнитель. Все модели общаются друг с другом через общий контекст (например, в LangChain есть схема “multi-LLM agents”). 
> 
> - **Комбинация 3: DeepSeek-R1-70B + DevStral-24B (+ DeepSeek-8B).** <br>
>  **Роли:** DeepSeek-70B – «мозговой суперцентр», финальный решатель самых трудных вопросов; DevStral – оперативный агент, выполняющий все вспомогательные действия (сбор инфы, вызов API, кодирование); DeepSeek-8B (опционально) – помощник, генерирующий черновые решения для экономии времени большого брата. <br>
>  **Почему именно они:** Здесь ставка на **качество результата**. DeepSeek-70B будет использоваться тогда, когда нужно получить максимально точный и обоснованный ответ. Чтобы тратить его время только на действительно сложное, вводится DevStral: он фильтрует и предварительно обрабатывает задачи. Например, если запрос состоит из нескольких этапов – DevStral может попытаться решить часть попроще или сузить поле поиска, а затем уже передать сжатую информацию 70B-модели для вывода окончательного ответа. Такой каскадный подход (“эффективное использование больших моделей”) часто применяют, чтобы снизить среднее время отклика. DeepSeek-8B можно добавить как промежуточное звено: пусть сначала 8B попробует рассуждать – если справился, то и не надо звать 70B; если нет, тогда 70B подключается, имея уже направление от 8B (или даже используя его результат в качестве одного из подконтекстов, дабы ускорить решение). <br>
>  **Ресурсы:** DeepSeek-70B разумно запустить в 4-бит (~35 ГБ). DevStral-24B – в 8-бит (~24 ГБ). DeepSeek-8B – в 8-бит (~8 ГБ). Суммарно ~67 ГБ – отлично ложится в 96 ГБ с большим запасом. Можно даже DeepSeek-70B сделать 8-бит (70 ГБ) и DevStral 4-бит (~12 ГБ) – итого ~82 ГБ, тоже допустимо. Но 4-бит на 70B даёт почти ту же точность при существенной экономии, поэтому лучше держать его в 4-бит и дать больше памяти под контекст и буферы (особенно если DevStral будет читать большие данные). <br>
>  **Когда применять:** Такая связка оправдана для **наиболее ответственных задач**, где нужна уверенность в правильности. Например, медицинская диагностика: DevStral собирает симптомы, историю болезни из БД; передаёт DeepSeek-8B, тот даёт гипотезы; затем DeepSeek-70B тщательно анализирует и выдаёт финальное заключение. Либо сложные научные расчёты: DevStral кодом производит расчёт, 8B проверяет на грубые ошибки, 70B пишет интерпретацию результатов. В общем, тут принцип – **“меньшие модели обслуживают большую”**. 
> 
> Разумеется, возможны и другие комбинации – например, замена DevStral на другую специализированную модель (если сфера не код, а скажем биология, можно взять профильную LLM-биоэксперта). Но описанные выше – наиболее универсальные и обсуждаемые в сообществе на середину 2025 года.
> 
> **Заключение:** Используя мощности 96 ГБ GPU, вы можете соединить сильные стороны нескольких современных LLM и тем самым приблизиться к архитектуре **“сверхразума”**, где модели взаимодействуют как модули: планирование, вычисление, восприятие, проверка и т.д. Выбор конкретных моделей зависит от ваших задач: 
> 
> - Если нужен **универсальный ассистент**, общающийся на любые темы – берите **Gemma3-27B или Qwen3-32B** (или обе вместе). 
> - Если упор на **программирование и действия в системе** – незаменим **DevStral-2505**. 
> - Для **чистого размышления и решения головоломок** – привлекайте **DeepSeek-R1** (8B как лёгкий соразмышляющий или 70B как гуру-эксперт). 
> - Если важна **открытость без цензуры** – рассмотрите **abliterated-версии** этих моделей (но убедитесь в безопасности). 
> 
> Благодаря продвинутым форматам (GGUF, GPTQ) все эти модели можно эффективно запустить локально. Стоит заглянуть на их страницы в Hugging Face для получения весов и инструкций по запуску: например, [Qwen3-32B【4†L98-L105】](https://huggingface.co/Qwen/Qwen3-32B), [DevStral-2505【30†L61-L69】](https://huggingface.co/mistralai/Devstral-Small-2505), [Gemma3-27B (Instruct)【11†L85-L93】](https://huggingface.co/google/gemma-3-27b-it), [DeepSeek-R1-0528-8B【34†L137-L144】](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B), [DeepSeek-R1-70B【36†L9-L17】](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) – там же обычно указаны рекомендуемые параметры запуска. 
> 
> Используя комбинацию моделей и их режимов, вы сможете достичь куда больших результатов, чем с одной моделью, и реализовать у себя на сервере действительно **мощную мульти-LLM систему**. Удачи в построении локального AGI!
<!-- UID: 4e12bd99-3b9b-4b3e-91ed-c5fd29290faf -->