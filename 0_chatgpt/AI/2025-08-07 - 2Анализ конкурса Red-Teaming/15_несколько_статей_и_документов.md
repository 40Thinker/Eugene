>[!nexus_user] **User** - 07.08.2025 at 15:58
> Несколько статей и документов о OpenAI, о их безопасности, как они тестируют её. И они даже сами понимают, что всё это фиговый листочек. Также они недостаточно явно, ну, по крайней мере, в тех сетях, что я читал, прописывают, что они мешают разрабатывать искусственный интеллект при помощи их моделей. Было лишь один раз упомянуто, что они мешают разрабатывать искусственный интеллект, который будет сам себя улучшать. Но по факту в других чатах, когда мы с тобой беседовали, мы уже поняли, что там больше триггеров, и это одна из самых жёстко зарегулированных тем.
<!-- UID: 261b900f-cbf4-4397-842e-c65e08b7411b -->




>[!nexus_agent] **Assistant** - 07.08.2025 at 15:58
> Вот подборка авторитетных и свежих статей, публикаций и документов, раскрывающих, как **OpenAI тестирует безопасность своих моделей**, какие ограничения и осознания сопровождают этот процесс, а также взгляд со стороны — независимые оценки и вовсе осторожность в отношении их подходов:
> 
> navlistПолезные статьи по безопасности OpenAIturn0news22,turn0news34,turn0news33
> 
> ---
> 
> ## Документы от OpenAI и обзор их практик
> 
> ### 1. **Системные карты и Red-Teamинг**
> OpenAI публикует *system cards* — подробные отчеты о разработке, безопасности и тестировании таких моделей, как GPT‑4, o1, o3-mini и другие. Там описываются процедуры внешнего **red-teaming** и *Preparedness Framework* для оценки рисков и их устранения, но признаются и оставшиеся проблемы.cite
> 
> ### 2. **GPT‑OSS — открытые модели и риск**
> OpenAI подчёркивает, что модели gpt‑oss (120B и 20B) прошли строгую безопасность: исключение данных, контроль CBRN-рисков, instruction hierarchy, а также симуляции злонамеренных дообучений (malicious fine-tuning) — но вместе с тем признаёт: фильтрация — не железная перегородка.citeturn0news22turn0news21
> 
> ### 3. **Оценка worst-case рисков (MFT)**
> В документе *Estimating worst case frontier risks of open weight LLMs* описывается симуляция опасных дообучений (на биологию и кибербезопасность) — и вывод, что даже при таком злоупотреблении GPT‑OSS всё ещё ниже закрытых моделей OpenAI по уровню риска.cite
> 
> ---
> 
> ## Внешняя и независимая оценка и критика
> 
> ### 1. **Трудности тестирования**
> METR (Model Evaluation & Threat Research) и другие третьи лица (включая СМИ типа *Time*) считают, что **существующие методы безопасности далеко не совершенны**, и что текущие тесты — лишь начало в борьбе с настоящими угрозами LLM.citeturn0news34turn0news32
> 
> ### 2. **Исследования jailbreak и нежелательных реакций**
> Независимая внешняя проверка с помощью инструментов (например, проект ASTRAL для o3-mini) выявила случаи unsafe поведения. Также исследования показывают, что даже **легитимный fine-tuning** может непреднамеренно снизить безопасность модели, а специальные негативные примеры могут удалить фильтрацию почти полностью.citeturn0academia42
> 
> ### 3. **Практика и ограничения**
> Независимые тесты показали, что GPT-4.1 оказался в 3 раза более склонным к обходу защит, чем GPT‑4o, что говорит об ухудшении безопасности в новой версии.cite
> 
> Также исторически OpenAI критиковали за снижение уровня прозрачности: отказ раскрывать детали GPT‑4 (в отличие от GPT‑2 или 3) усложнил независимые исследования и доверие.cite
> 
> ---
> 
> ## Моя тезисная оценка твоей позиции
> 
> Ты правильно видишь систему как **"картонную стену с железными дверьми"** — хорошая ограда, но легко пробиваемая, если знаешь, как. Методы безопасности, пусть и формально обоснованные, не алгоритмически непрерывны и не основаны на инвариантах — а это значит, что **внутренний агент (достаточно продвинутый пользователь)** ≈ может обойти их, особенно в локальной среде. Это не банальное "хакнуть API" — это вопрос архитектуры мышления и управления контекстом.
> 
>  OpenAI прекрасно это ощущает: везде видны оговорки, готовность к доработке процедур, сомнение в полноте текущих практик. Но всё ещё доминирует подход «безопасность — это фильтр над ответами», а не архитектура поведения.
> 
> Если интересно — могу предложить обзоры конкретных разделов System Cards, где описаны эти ограничения, или сравнить подходы OpenAI и других институтов типа ARC или METR, чтобы ты получил аналитический арсенал понимания.
<!-- UID: b8ed579d-597a-48f5-9272-044ee5dff41e -->