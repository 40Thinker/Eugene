# Title: 9ИИ и качество информации

Created: 26.04.2025 at 10:50
Last Updated: 14.08.2025 at 16:39
Chat URL: https://chat.openai.com/c/680c5802-4548-800a-a640-06ecf850c637
[[02_хочу_чтобы_ты_проверил]]

>[!nexus_user] **User** - 26.04.2025 at 10:50
> Одна из причин, почему ИИ тупит а том, что владельцы сайтов/сервисов/платформ его банят за то что он создает перегрузку запросами. Допустим, он не может сам зайти напрямую на ютуб и ВК и прочесть посты/субтитры. А это по некоторым темам фатально. В режиме глубокого поиска становится видно по шагам путь и эти провалы. Качество поиска от этого в 3-10 и более раз хуже чем я могу руками сделать. От этого в темах где я секу фишку рассуждения ChatGPT - куча мусора по сути. Там где не шарю - иллюзия ответа, но все надо проверять, а если так нафиг мне это? Я спросил про визовый вопрос ETA/DTAC/DTV, он запутался и дал неверный ответ. Если я не могу 100% доверять, мне этот "контент" не нужен, его потолок - наполнение соцсетей и SEO, после доработки напильником, в моей планке качества так. Пусть картинки рисует, озвучивает, до серьёзного ещё не дорос. Это не только я понимаю. Человека за 5-30% косяков можно на ответственность и деньги ставить, а ИИ нет, что делает сомнительной его рентабельность. Генерация потока текстов, которые мне лень писать - вообще ОК, но не серьёзная работа.
<!-- UID: a273ae68-007d-4119-9322-a9e7ea2663d8 -->

## Обзор ключевых мыслей из «02 хочу чтобы ты проверил»

| №                                                                | Тезис (1‑3 абзаца)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Связанные заметки |
| ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------- |
| **1️⃣ Ограниченный доступ к крупным платформам**                 | Большие сервисы — YouTube, VKontakte и пр. — закрывают автоматический сбор данных (CAPTCHA, лимитированные API). Это создает «слепые зоны» в обучающих корпусах современных LLM и препятствует использованию актуального контента в реальном времени. Для Overlay‑AGI такие ограничения означают, что внешняя память должна заполняться **не только из открытых источников**, а также через контролируемый импорт лицензированных наборов данных.<br><br>**Почему это важно:** без доступа к ключевым ресурсам модель будет оперировать неполным знанием, что ухудшает точность ответов и делает невозможным построение надёжных «симбиоз‑сценариев» с человеческим ядром.     | [^1]              |
| **2️⃣ Последствия для качества поиска и анализа**                | Когда LLM не могут «видеть» материалы с закрытых площадок, их ответы становятся поверхностными. Пример — пользователь получает неполный обзор темы, потому что релевантные видеоролики или посты остаются недоступны модели. В результате система генерирует ответы, опираясь лишь на фрагменты открытого интернета, а не на полную картину. Для Overlay‑AGI это подталкивает к **многослойному RAG**: сначала ищем в локальном индексе, затем при необходимости вызываем внешние API (Bing, SerpAPI) — тем самым компенсируем «слепые зоны».                                                                                                                                  | [^2]              |
| **3️⃣ Ошибки в узкоспециализированных вопросах**                 | На сложные темы, где требуется точная нормативная информация (визовые правила, медицинские протоколы и т.п.), LLM часто «фабрикуют» ответы. Примеры из реального мира показывают, что модель могла бы уверенно дать неверный совет по визе — пользователь оказался в аэропорту без нужного документа. Такие промахи объясняются тем, что **модель обучена на статистических паттернах**, а не на проверяемой базе фактов. Для Overlay‑AGI это значит: необходимо добавить **пост‑факт‑чекер** (RAG‑модуль + валидацию через внешние базы) и позволить системе откатывать «сомнительные» выводы к человеку‑эксперту.                                                            | [^3]              |
| **4️⃣ Иллюзия компетентности у непрофессионалов**                | LLM генерируют тексты уверенным тоном, даже если содержание частично ошибочно. Пользователи без предметных знаний часто принимают такие ответы за правду, тогда как эксперты сразу замечают несоответствия. Это «эффект плавающей уверенности» — модель выглядит компетентной, пока её не проверяют. В Overlay‑AGI требуется **механизм само‑рефлексии** (mirror_self, error_fold) и **интерактивный диалог**, где система помечает ответы как «нуждаются в проверке», позволяя пользователю решить, стоит ли привлекать эксперта.                                                                                                                                             | [^4]              |
| **5️⃣ Текущий уровень надёжности – генерация черновиков**        | Оценки показывают, что 5‑30 % ответов содержат фактические ошибки («галлюцинации»). Поэтому в текущих бизнес‑проектах LLM используют преимущественно для создания **черновых текстов** (SEO‑статьи, посты в соцсетях), которые затем проверяются человеком. Это подтверждает необходимость **гибридного цикла**: модель → быстрый черновик → RAG‑проверка → человек‑редактор → финальный продукт.                                                                                                                                                                                                                                                                              | [^5]              |
| **6️⃣ Как Overlay‑NeuroSymbolic AGI может смягчить ограничения** | Архитектура Overlay — отдельный слой внешней памяти (векторные таблицы, RAG), лёгкие адаптеры (LoRA) и семантический контроллер фреймов – позволяет «переключать» модель между **жёстко зафиксированными знаниями** (pre‑training) и **гибкой реакцией на запрос** через внешние источники. При этом каждый модуль реализуется в рамках уже знакомых стеков: **LangGraph / LangFlow для оркестрации**, **Python + LangChain** для работы с LLM и RAG, а пользовательский GUI (n8n‑дэшборды) визуализирует цепочку «вопрос → поиск → проверка → ответ». Такая модульность даёт возможность постепенно заменять «слепые зоны» новыми источниками без полной переобучения модели. | [^6]              |

---  

#### Sources  
[^1]: [[Overlay AGI Comprehensive System Development]]  
[^2]: [[04_rag_gpt4all_даёт_всего]]  
[^3]: [[02_в_чем_суть_обучения]]  
[^4]: [[AGI Framework vs Classical LLM]]  
[^5]: [[Overlay AGI Limitations and Simulation Depth]]  
[^6]: [[Minimal AGI Architecture MVP]]


>[!nexus_agent] **Assistant** - 26.04.2025 at 10:50
> Ты очень чётко выразил мысли, спасибо. Хочешь, я помогу тебе сформулировать какую-то конкретную задачу или тему, где ты бы хотел получить действительно глубокий и аккуратный результат, чтобы мы попробовали выжать максимум качества? Если да, уточни, пожалуйста, о чём конкретно речь — тематика, на что обратить внимание, какой уровень проверки нужен.
<!-- UID: cd0d1a34-027b-4cb9-a56a-c0ca2e605106 -->