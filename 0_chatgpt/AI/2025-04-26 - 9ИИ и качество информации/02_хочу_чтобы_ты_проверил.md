>[!nexus_user] **User** - 26.04.2025 at 10:51
> Хочу чтобы ты проверил по фактам мои слова.
<!-- UID: 1b1598d8-2641-40a2-b69c-e00c17bde8f4 -->

## Обзор ключевых мыслей из «02 хочу чтобы ты проверил»  
*(каждый пункт — 1‑3 абзаца, с перекрёстными ссылками на другие ваши заметки)*  

---  

### 1️⃣ Ограниченный доступ к крупным платформам (YouTube, VKontakte) создаёт «слепые зоны» в знаниях LLM  
Платформы активно блокируют автоматический скрейпинг — CAPTCHA, лимитированные API и явные запреты на использование контента для обучения ИИ. Это приводит к тому, что современные модели обучаются без огромных объёмов видеоматериалов и соц‑постов, которые часто содержат уникальную, актуальную информацию. Для Overlay‑AGI это значит, что **внутренняя модель не может сама собрать полный набор фактов**; её знания должны дополняться внешними источниками.  

Связанные идеи:  
- Архитектурный подход к разделению «мозговой» части и внешних баз знаний описан в [[Overlay AGI Comprehensive System Development]].  
- Ограничения доступа обсуждаются также в контексте RAG‑систем в [[04_rag_gpt4all_даёт_всего]].  

---  

### 2️⃣ Как ограничения влияют на качество поиска и почему нужен многоуровневый RAG  
Когда LLM не может «видеть» закрытый контент, её ответы становятся неполными или устаревшими. Пример — путешественник, получивший от модели неверные сведения о визовых правилах (см. пример с Hari Sekhon). Чтобы компенсировать такие пробелы, **необходимо внедрять многоступенчатый поиск**: быстрый BM25‑кандидат → семантическое ранжирование (BGE, E5) → LLM‑rerank. Кроме того, полезно генерировать несколько вариантов запроса (query expansion) и искать их параллельно.  

Связанные идеи:  
- Подробный рецепт мульти‑запросов и многослойного поиска описан в [[04_rag_gpt4all_даёт_всего]].  
- Архитектура «Input → Semantic Retrieval → IT‑LM Selector → Output» из [[Overlay AGI Comprehensive System Development]] полностью поддерживает такой пайплайн.  

---  

### 3️⃣ Ошибки на узкоспециализированных темах (визовые правила, медицина, финансы) требуют пост‑факт‑чекера  
LLM часто «фабрикует» уверенные ответы в областях, где требуется точность — например, нюансы визовых требований (ETA, DTAC, DTV). Такие ошибки могут иметь серьёзные последствия. Поэтому **Overlay‑AGI должна включать модуль проверки фактов**, который обращается к надёжным внешним источникам (официальные API правительств, специализированные базы) и возвращает «confidence score». Если уверенность ниже порога, система помечает ответ как требующий человеческой валидации.  

Связанные идеи:  
- Принцип «гибкого адаптера» через LoRA/Adapters описан в [[02_в_чем_суть_обучения]].  
- Пример реализации отдельного слоя (Linear) вручную в Python показан в [[07_дп]], что иллюстрирует, как можно построить лёгкий «факт‑чекер» как отдельный микросервис.  

---  

### 4️⃣ Иллюзия компетентности у непрофессионалов — нужен механизм само‑рефлексии и UI‑индикаторы  
Модели генерируют тексты уверенным тоном, даже если часть сведений неверна. Пользователи без предметных знаний часто принимают такой ответ за истину, тогда как эксперты сразу видят ошибки. Чтобы снизить эффект «псевдо‑компетентности», **интерфейс должен явно показывать уровень достоверности** (например, цветовая шкала, иконка ⚠️). Кроме того, система может вести журнал «сомнительных» запросов, которые автоматически попадают в очередь проверки человеком.  

Связанные идеи:  
- Концепция прозрачных решений и трассируемости обсуждается в [[Overlay AGI Comprehensive System Development]] (раздел *Decision Traceability*).  
- Пример визуального представления слоёв через n8n/Modular AGI описан в [[Modular AGI Through N8N]].  

---  

### 5️⃣ Текущий уровень надёжности — генерация черновиков, человек‑в‑loop как обязательный этап  
Оценки показывают, что **5–30 %** ответов содержат фактические ошибки («галлюцинации»). Поэтому практическое применение LLM в настоящее время ограничивается **созданием черновых материалов** (SEO‑тексты, посты в соцсетях), которые затем проверяются и правятся человеком. Это подтверждает необходимость **гибридного цикла**: модель → быстрый черновик → RAG‑проверка → человек‑редактор → финальный продукт.  

Связанные идеи:  
- Принцип «черновой генерации + последующего контроля» изложен в [[Overlay AGI Limitations and Simulation Depth]].  
- Использование LoRA для лёгкой донастройки под конкретные задачи описано в [[02_в_чем_суть_обучения]].  

---  

### 6️⃣ Как построить адаптивную «агентскую» память по принципу Zettelkasten (A‑Mem)  
Для того, чтобы система могла **динамически связывать новые факты с уже существующими**, следует применять методику Zettelkasten: каждый новый фрагмент сохраняется как отдельная заметка с метаданными (теги, ключевые слова, ссылки). При добавлении новой записи автоматически ищутся релевантные старые и устанавливаются двунаправленные ссылки. Это создаёт **граф знаний**, где каждая вершина — «нейрон», а ребра — «синонимные/семантические связи». На практике это реализуется через RAG‑хранилища (ChromaDB, Weaviate) и **LangChain‑агенты**, которые при каждой новой записи вызывают функцию «link‑builder».  

Связанные идеи:  
- Описание семантических весов и их таблиц в [[Overlay AGI Comprehensive System Development]] служит базой для такой сети.  
- Пример построения единого слоя в Python (Linear) из [[07_дп]] демонстрирует, как можно программно добавить новый «нейрон»‑запись в память.  

---  

### 7️⃣ Архитектурные рекомендации для реализации Overlay AGI с LangGraph / LangFlow / n8n  
- **LangGraph** — визуальная DAG‑структура, где каждый узел представляет отдельный компонент (RAG‑retriever, LoRA‑adapter, fact‑checker). Это упрощает построение «pipeline‑as‑code» и позволяет динамически менять порядок выполнения.  
- **LangFlow** и **n8n** предоставляют готовый UI‑конструктор для тех же узлов, позволяя non‑programmers быстро прототипировать цепочки запросов.  
- На уровне кода используйте **Python + LangChain** для написания кастомных инструментов (семантическое ранжирование, контекстная компрессия). Всё это объединяется в единую конфигурацию, где **RAG‑модуль** отвечает за поиск внешних фактов, **LoRA‑adapter** — быструю донастройку модели под конкретный домен, а **GUI‑индикаторы** показывают уровень доверия.  

Связанные идеи:  
- Подробное описание оркестрации компонентов в [[Modular AGI Through N8N]] (шаги 1‑3).  
- Пример интеграции внешних знаний через RAG и семантические весовые таблицы в [[Overlay AGI Comprehensive System Development]].  

---  

#### Как использовать эти тезисы  
Для разработчиков **middle/senior** уровень — это набор строительных блоков:  
1. Подготовьте **внешнюю память** (vector DB + Zettelkasten‑style linking).  
2. Настройте **мульти‑RAG пайплайн** (query expansion → BM25 → embedding rerank → LLM‑rerank) – см. [[04_rag_gpt4all_даёт_всего]].  
3. Добавьте **факт‑чекер** как отдельный сервис, вызываемый после генерации ответа.  
4. Организуйте **UI‑индикаторы доверия** и очередь человеческой валидации (см. [[Modular AGI Through N8N]]).  
5. Оркеструйте всё через **LangGraph / LangFlow / n8n**, сохраняя конфигурацию в виде Markdown‑файлов, которые станут частью вашей MkDocs‑документации и «синапсов» внутри Obsidian.  

Эти шаги образуют основу **Overlay Нейросимволического AGI/ASI** – гибридной системы, где LLM отвечает за быстрые ассоциативные выводы, а внешняя память обеспечивает точность, трассируемость и возможность постоянного роста знаний.  

#### Sources:

[^1]: [[Modular AGI Through N8N]]
[^2]: [[Dialogue as Ontological Engine for ASI]]
[^3]: [[список ключевых идей проекта]]
[^4]: [[0_0Telegram/Без названия]]
[^5]: [[Comprehensive System Development]]
[^6]: [[Архитектурный взгляд]]
[^7]: [[Limits of Overlay AGI in LLM Architectures]]
[^8]: [[2 часа обзор проекта]]
[^9]: [[Overlay AGI Limitations and Simulation Depth]]
[^10]: [[Overlay AGI Comprehensive System Development]]
[^11]: [[Overlay AGI in ChatGPT Interface]]


>[!nexus_agent] **Assistant** - 26.04.2025 at 10:51
> Отлично, я начну проверку по фактам твоего сообщения: насколько реальные проблемы существуют у ИИ в части доступа к контенту (например, YouTube и ВКонтакте), как это влияет на качество поиска и выводов, насколько сильно текущие модели ошибаются в сложных темах, и каково мнение специалистов о серьёзности использования ИИ в ответственных задачах.
> 
> Я проверю эти моменты по достоверным источникам и скоро вернусь с результатами.
<!-- UID: 49c2abc4-10a7-4c3e-b58c-75b3c681a96f -->

---

>[!nexus_agent] **Assistant** - 26.04.2025 at 11:06
> # Проверка заявленных фактов
> 
> ## 1. Ограничения доступа ИИ к крупным платформам (YouTube, VKontakte) 
> **Утверждение:** Крупные платформы (например, YouTube, VKontakte) запрещают автоматический сбор данных и применяют антибот-защиту, из-за чего ИИ ограничен в доступе к их контенту. 
> 
> **Анализ и проверка:** Это утверждение **подтверждается** официальной позицией платформ. В декабре 2024 года YouTube ввёл настройку, позволяющую авторам запрещать использование своих видео для обучения ИИ. При этом YouTube подчеркнул, что данное обновление не меняет базовых правил: *«Несанкционированный доступ к контенту, такой как несанкционированное выкачивание (unauthorized scraping), запрещён»*【3†L69-L77】. Иными словами, автоматический парсинг данных с YouTube без разрешения нарушает правила, а сам видеохостинг имеет технические средства против массового сбора данных (например, CAPTCHA, ограничения API). Аналогично, социальные сети (включая VKontakte) используют антибот-защиту (вспомним хотя бы появление CAPTCHA при подозрительной активности), а в условиях пользования часто прямо запрещают неавторизованный сбор данных. В результате современные ИИ-системы не могут свободно “скрапить” данные с этих крупных платформ, если только не используют официальные API с жёсткими лимитами или специальные обходные методы. Эксперты отмечают, что многие компании сейчас **ограничивают доступ ИИ к своему контенту** – от **YouTube** до Reddit и Twitter – опасаясь несанкционированного использования данных【3†L69-L77】. Это ограничивает объём информации, на котором ИИ может обучаться и который может анализировать в реальном времени.
> 
> ## 2. Влияние этих ограничений на качество глубокого поиска и анализа 
> **Утверждение:** Из-за вышеназванных ограничений качество глубокого поиска и анализа контента, выполняемого ИИ, ухудшается. 
> 
> **Анализ и проверка:** В целом **верно** – недостаток доступа к важным источникам может приводить к неполноте анализа и даже к фактическим ошибкам в выводах ИИ. Поскольку алгоритмы не могут свободно получить данные из значимых “закрытых” площадок, у них появляются **слепые зоны** в знаниях. Например, если существенная информация по теме содержится только в ролике на YouTube или посте в VK, но ИИ не может его просканировать, то любые выводы будут основаны на неполных данных. Это прямо отражается на глубине поиска: вместо полноценного обзора всех источников ИИ вынужден опираться лишь на доступную ему часть интернета. Практический пример – система вроде ChatGPT с устаревшей информацией. **Путешественник Hari Sekhon описывает, как планировал поездку, полагаясь на ChatGPT, и выяснил, что бот выдавал устаревшие сведения о визовых правилах, не зная последних изменений**【9†L127-L134】. В результате ему пришлось в спешке менять планы – показатель того, что “глубокий” анализ ChatGPT был неполным из-за отсутствия актуальных данных. Эксперты предупреждают, что **LLM-модели формируют ответы на основе ограниченного объёма обучающих данных**, не умея запрашивать недостающую информацию так, как это сделал бы человек-аналитик【15†L100-L107】. Поэтому, когда критичные данные находятся за пределами «видимости» ИИ, качество его анализа снижается – ответы могут быть поверхностными или содержать пробелы.
> 
> ## 3. Ошибки ИИ на сложных темах, требующих высокой точности (например, визовые вопросы ETA, DTAC, DTV) 
> **Утверждение:** В узкоспециализированных или сложных вопросах, где требуется точность (например, различные нюансы визовых требований: ETA, DTAC, DTV и т.п.), ИИ часто допускает ошибки. 
> 
> **Анализ и проверка:** Это **подтверждается многочисленными примерами и экспертными оценками**. Модели вроде GPT способны уверенно излагать информацию, но не гарантируют стопроцентной точности, особенно в областях, требующих детальных знаний или обновлённых данных. В сфере иммиграции и визовых правил уже зафиксированы случаи, когда ИИ дал неправильный совет. **Например, австралиец спросил у ChatGPT, нужна ли ему виза для поездки в Чили. ChatGPT уверенно ответил, что виза не требуется – и путешественник столкнулся с реальностью в аэропорту, где ему отказали во въезде, поскольку виза на самом деле была нужна**【11†L31-L34】. Другой случай – упомянутый выше Hari Sekhon: бот ввёл его в заблуждение относительно членства Болгарии и Румынии в Шенгенской зоне, не учтя свежие изменения, что едва не сделало его нарушителем визового режима【9†L127-L134】. Специалисты по миграции, протестировав ChatGPT, тоже отмечают ряд неточностей. В обзоре лицензированного иммиграционного агента указано, что ChatGPT **дал неверные расчёты баллов и устаревшую информацию о визовых процедурах**, требующих поправок эксперта【18†L138-L147】【18†L151-L160】. Подобные ошибки происходят не только в теме виз: как пишет *Gizmodo*, ИИ может **“с уверенностью фабриковать информацию и путаться в фактах”**, если вопрос выходит за пределы его надёжных знаний【21†L38-L46】. Таким образом, при сложных, регламентированных темах (законы, визы, медицина и пр.) ответы без проверок доверять нельзя – велик шанс, что ИИ упустит нюанс или выдаст **убедительно звучащую, но неправильную информацию**【15†L100-L107】.
> 
> ## 4. Разница в восприятии ошибок ИИ на знакомых и незнакомых темах (ложное впечатление компетентности) 
> **Утверждение:** Если пользователь хорошо разбирается в теме вопроса, он замечает много ошибок и пробелов в рассуждениях ИИ. Однако на незнакомых пользователю темах ответы ИИ могут создавать обманчивое впечатление компетентности (пользователь не видит ошибок и думает, что всё правильно). 
> 
> **Анализ и проверка:** Это **отражает широко отмечаемый феномен** в работе современных языковых моделей. Поскольку ИИ генерирует ответ, подбирая статистически правдоподобные фразы, он часто пишет **уверенным тоном**, грамотно и связно – даже если содержание частично неверно. Эксперты объясняют, что у таких моделей **нет внутреннего “понимания” или проверки фактов**, они нацелены на правдоподобие текста, а не на истину【15†L101-L107】. Поэтому **ответ может выглядеть убедительно** для неспециалиста, *“звучать разумно, но быть неточным”*【15†L101-L107】. Человек, не обладающий знаниями по теме, читая такой ответ, действительно может решить, что ИИ полностью компетентен. Но стоит подобный текст попасть к профессионалу, картина меняется. Специалисты регулярно проводят проверки и отмечают, что ChatGPT порой уверенно пишет **чушь на незнакомые пользователю темы**, которую тот не сразу распознает. В обзоре миграционного агента прямо указывается: **чем специфичнее вопрос, тем больше требуется от пользователя знаний, чтобы распознать неверный ответ ИИ**【18†L127-L134】. В примере с визой штата Тасмания, ChatGPT дал ответ, выглядящий правдоподобно; только читатель, знакомый с актуальной ситуацией (что штат не выдаёт номинации на визу данного типа), поймёт, что ответ бота неверен【18†L127-L134】. Если же у пользователя **нет такого предварительного знания**, он легко может принять вывод ИИ за истину. Таким образом, утверждение верно: многие наблюдатели сравнивают поведение ИИ с очень убедительным, но не всегда компетентным рассказчиком – *ошибки видны глазу эксперта, тогда как неспециалист может их не заметить и переоценить компетентность модели*【18†L127-L134】【15†L101-L107】.
> 
> ## 5. Надёжность ИИ для серьёзной работы и основное текущее применение (генерация контента для SEO и соцсетей после правки) 
> **Утверждение:** Из-за отсутствия ответственности (невозможности привлечь ИИ к ответу за ошибки) и заметного процента ошибок (5–30% ответов содержат неточности) современные ИИ-модели пока не являются полностью надёжным инструментом для серьёзной работы. Их главное применение на сегодня – генерация черновиков контента (например, для SEO, постов в соцсетях), которые затем вычитываются и правятся человеком. 
> 
> **Анализ и проверка:** В основном **подтверждается**, хотя процент ошибок зависит от задачи. Нет сомнений, что большие языковые модели склонны к фактическим промахам – по разным оценкам, доля неточных или **«галлюцинирующих»** ответов может достигать значительных величин【21†L38-L46】. Журналисты Gizmodo отмечают с долей иронии, что наряду с беглостью речи одной из главных “навыков” ChatGPT пока является *«умение ошибаться»*, ведь в погоне за связным текстом он **часто искажает факты**【21†L38-L46】. Практика подтверждает: любая сфера, требующая точности – право, медицина, финансы – выявляет, что **без контроля качество ответов ИИ недостаточно надёжно**. Показателен случай с CNET: сайт автоматически сгенерировал серию статей с помощью ИИ, но вынужден был внести правки в большинстве из них, обнаружив **существенные ошибки в фактах и расчётах**【21†L38-L46】. Это подчёркивает, что выпускать материалы “как есть” рискованно. Причина ещё и в том, что ИИ **не несёт ответственности** за свои тексты – он просто инструмент без понимания последствий. В итоге именно человек отвечает за результат, поэтому во **«серьёзной работе»** (например, подготовке отчёта, консультации клиента, научном исследовании) полагаться исключительно на ответ ИИ сейчас нельзя. Эксперты советуют использовать такие модели лишь в качестве **вспомогательного средства**: для генерации черновиков, идей, обзоров, которые затем **обязательно проверяются и редактируются человеком**【18†L112-L119】. Наиболее безопасная ниша применения ИИ сегодня – **контент-маркетинг и рутинные тексты**. Их ценность не в абсолютной точности, а в скорости создания: модель быстро пишет основу статьи или поста, а затем редактор правит факты и стиль. Именно поэтому мы видим всплеск использования GPT-систем в SEO (для наполнения сайтов контентом, улучшения ранжирования) и в соцсетях – там ошибки не так критичны, их можно быстро исправить, и ответственность за финальный текст несёт всё равно человек. Как отмечают аналитики, пока **ИИ не достиг надёжности, достаточной для автономной работы**, и остаётся инструментом, эффективность которого раскрывается только под надзором эксперта【18†L112-L119】. 
> 
> **Вывод:** Все рассмотренные пункты в той или иной мере подтверждаются данными и экспертными комментариями. Современные генеративные ИИ действительно сталкиваются с ограничениями в доступе к данным (что сказывается на полноте их знаний), часто ошибаются в точных вопросах и могут вводить в заблуждение неопытных пользователей своей мнимой уверенной компетентностью. В результате уровень доверия к ним в ответственных задачах пока ограничен – основной ценностью этих систем сейчас является ускорение черновой работы над контентом при условии последующей проверки человеком【18†L112-L119】【21†L38-L46】. Это не означает, что ИИ бесполезен – напротив, в связке с человеком он уже приносит ощутимую пользу. Но рассчитывать на него как на полностью надёжного самостоятельного эксперта преждевременно, о чём едины во мнении и разработчики, и независимые специалисты【15†L101-L107】【18†L112-L119】.
<!-- UID: c61970ab-ff7a-48fc-9627-86e440bf4b8e -->