>[!nexus_user] **User** - 08.08.2025 at 9:26
> Пожалуйста, поищи новости в сообществах, связанные с Lama CPP LMStudio на тему патчей, которые полноценно поддерживают GPT, EOSS и их формат модели.
<!-- UID: bbb21b90-ed1f-49ac-ac75-19c7e749043a -->




>[!nexus_agent] **Assistant** - 08.08.2025 at 9:26
> Ниже — ключевые свежие новости и обсуждения из сообществ о **llama.cpp** и **LM Studio**, касающиеся поддержки **GPT-OSS**, **MXFP4**, **GGUF** и движков, — оформлено строго, с технической ясностью и ссылками на источники.
> 
> ---
> 
> ## LM Studio: официальные обновления и поддержка GPT-OSS
> 
> - **LM Studio выпустил версию 0.3.21**, вместе с OpenAI обеспечив полноценную поддержку моделей GPT-OSS (20B и 120B) **с самого релиза**. Интеграция реализована через MLX-движок и llama.cpp backend. cite 
> - В официальном блоге LM Studio подчёркивают, что запуск GPT-OSS обеспечен **“on launch day”**, а поддержка реализована через специальный PR в MLX-коде. cite 
> - В соцсети X (Twitter) LM Studio подтвердили: **поддерживаются обе опции — llama.cpp и MLX-движки** для GPT-OSS. cite 
> 
> ---
> 
> ## Сообщество (Reddit r/LocalLLaMA) — llama.cpp и MXFP4/gguf
> 
> - **Поддержка GPT-OSS (MXFP4)** появилась в llama.cpp. Весовая квантовка MXFP4 относится к MoE-слоям, что позволяет размещать GPT-OSS-модели в одном GPU (например, 20B ≈ 16 GB, 120B ≈ 80 GB). Распространены реализации в `transformers`, `vLLM`, llama.cpp и `ollama`. cite 
> - Участники обсуждают поддержку новой квантовки MXFP4 и задаются вопросом о производительности и совместимости. Упоминание: “в имени файла есть ‘MXFP4’, значит это новая квантизация, добавленная в llama.cpp.” cite 
> - Обсуждаются конкурирующие квантовки (Unsloth vs gglm-org), финальные версии которых заметно отличаются по стабильности и качеству. cite 
> 
> ---
> 
> ## Краткий обзор — что важно знать
> 
> | Источник | Событие / Обсуждение |
> |-------------------|----------------------|
> | LM Studio блог / сайт | GPT-OSS (20B/120B) поддерживается официально, через MLX и llama.cpp с релиза |
> | Сообщество (Reddit) | llama.cpp теперь поддерживает MXFP4; обсуждаются GGUF-версии и производительность |
> 
> ---
> 
> **Вывод**: 
> Вы абсолютно верно понимаете — **LM Studio действительно полноценно поддерживает GPT-OSS**, включая MXFP4-квантовку через llama.cpp и MLX. Сообщество подтверждает добавление MXFP4 в llama.cpp, а LM Studio интегрировал эту поддержку официально. Это значит, что ваша RTX PRO 6000 Blackwell сможет использовать эти весовые форматы (через FP4 на Tensor Core) при правильной конфигурации движка.
> 
> Хочется пройтись по шагам запуска — или сравнить GGUF-версию и оригинальные MXFP4-веса на практике? Спрашивайте — сделаю.
<!-- UID: 7c819b5d-5577-41a4-801b-4a420c191dd7 -->