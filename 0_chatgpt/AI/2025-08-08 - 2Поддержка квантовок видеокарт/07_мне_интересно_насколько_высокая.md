>[!nexus_user] **User** - 08.08.2025 at 9:36
> Мне интересно, насколько высокая гибкость LM Studio в плане подключения LoRa, RAC и всяких других фишечек. Уточнение. Не только RAC в виде загрузил файлы и он подсовывает их, а в плане мои кастомные на самых разных фреймворках LoRa сделанные и RAC. Насколько все это совместимо с LM Studio и форматом файлов, которые он использует.
<!-- UID: bbb21f25-4cf0-4241-bacf-e304bfb41ce5 -->




>[!nexus_agent] **Assistant** - 08.08.2025 at 9:36
> Вот результат поиска: к сожалению, на данный момент **сообщество и официальные ресурсы не подтверждают полноценную гибкую интеграцию пользовательских LoRA или RAC-адаптеров (например, в формате `.safetensors`) в LM Studio**, за пределами базовой поддержки через MLX или llama.cpp-подобные движки.
> 
> ---
> 
> ## Найденные факты и обсуждения
> 
> ### GitHub — Issues из репозиториев LM Studio и MLX:
> 
> - В issue для LM Studio (репозиторий **lms**) пользователь запрашивает поддержку загрузки LoRA-адаптеров в формате `.safetensors` через API: 
>  «Would be great to see the same being supported on LM Studio» — но пока без реализации или обратной связи от разработчиков. cite
> 
> - В issue для **mlx-engine** (движок, на котором может базироваться LM Studio) отмечают, что LoRA-адаптеры применяются только **на этапе загрузки модели (load-time overwrite)**, а не во время вывода (inference-time); для поддержки такого функционала необходимо значительное рефакторинг. cite
> 
> ---
> 
> ### Reddit — r/LocalLLM и других:
> 
> - Есть упоминания, что `llama.cpp` поддерживает LoRA-адаптеры, но это относится скорее к базовым use-case (стыковка с веб-скриптами или llama.cpp-сервером). Никаких подробных дискуссий о LM Studio в этой конфигурации не найдено. cite
> 
> ---
> 
> ## Резюме по вашей потребности
> 
> | Сценарий | Поддержка в LM Studio |
> |--------------------------------------|-------------------------|
> | **Стандартный LoRA** (кач загрузки) | Частичная — только при загрузке модели (load-time), без динамического управления |
> | **Кастомный LoRA/RAC из `.safetensors`, на разных фреймворках** | Не поддерживается как inference-time адаптация |
> | **Гибкое объединение LoRA/RAC на лету** | Нет официальной поддержки |
> 
> ---
> 
> ## Практический вывод
> 
> - Если вы создаёте кастомные LoRA-адаптеры — LM Studio на данный момент **не позволяет применять их динамически** при инференсе. Адаптация происходит лишь при загрузке и статическом инжекте в модель.
> - Для полной свободы и совместимости, включая RAC или LoRA на разных фреймворках, **лучше использовать llama.cpp или MLX напрямую**, где вы контролируете pipeline загрузки и объединение адаптеров вручную.
> - Если ваша задача требует именно интеграции таких адаптеров в LM Studio — это **очень актуальный кейс для feature request** на GitHub и для обсуждения в сообществе разработчиков LM Studio.
> 
> ---
> 
> Готов помочь разработать мини-CLI-скрипт или схему загрузки кастомного LoRA через llama.cpp/MLX, и уже через LM Studio подгружать результат (при возможности). Хотите?
<!-- UID: d8457d00-b3a7-4d50-b1a1-4fa86597d0de -->