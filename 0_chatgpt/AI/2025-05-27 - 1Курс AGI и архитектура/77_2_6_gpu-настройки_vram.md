>[!nexus_user] **User** - 27.05.2025 at 13:37
> 🎮 2.6. GPU-настройки: VRAM, совместимость, quantization, batch size
> 2.6.1. Проверка объема VRAM:
> nvidia-smi
<!-- UID: 6b41fd5e-beca-424c-b205-119676836352 -->




>[!nexus_agent] **Assistant** - 27.05.2025 at 13:37
> Вот полная развертка узла **2.6.1. Проверка объема VRAM: `nvidia-smi`**, 
> в котором ты получаешь **фактическую информацию о своей видеокарте**, 
> и понимаешь, **на что реально способен твой AGI при запуске модели**, 
> какой размер поддерживается, возможно ли quantization, и **где могут начаться затыки.**
> 
> ---
> 
> ## 🎮 2.6.1. Проверка объема VRAM: `nvidia-smi`
> 
> ---
> 
> ### 📌 КОНТЕКСТ
> 
> > **VRAM = объём видеопамяти** на твоей GPU, 
> > которая используется для загрузки и запуска модели (например, Saiga, Mistral, LLaMA и др.)
> 
> 📌 Чем больше VRAM:
> - тем крупнее модель можно загрузить напрямую 
> - тем выше context window 
> - тем меньше необходимость в quantization (сжимающих режимах)
> 
> ---
> 
> ### ✅ КОМАНДА ПРОВЕРКИ
> 
> Открой терминал и введи:
> 
> ```bash
> nvidia-smi
> ```
> 
> ---
> 
> ### 🧾 ОЖИДАЕМЫЙ ВЫВОД:
> 
> ```text
> +-----------------------------------------------------------------------------+
> | NVIDIA-SMI 535.129.03 Driver Version: 535.129.03 CUDA Version: 12.1 |
> | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
> | 0 RTX 3090 On | 00000000:01:00.0 Off | 0 |
> | FB Memory Usage |
> | Total: 24576 MiB | Used: 1032 MiB | Free: 23544 MiB |
> +-----------------------------------------------------------------------------+
> ```
> 
> ---
> 
> ### 🧠 ЧТО СМОТРЕТЬ
> 
> | Поле | Значение |
> |--------------|----------------------------------|
> | `Name` | Твоя модель GPU |
> | `Total` | Общий объём VRAM (в MiB) |
> | `Used` | Используется сейчас |
> | `Free` | Доступно для загрузки модели |
> 
> ---
> 
> ### 📊 ПРИМЕРЫ МИНИМУМОВ
> 
> | Модель | Без quantization (FP16) | С 4bit quant (GGUF, BnB) |
> |----------------------|-------------------------|--------------------------|
> | LLaMA 7B | 16–20 ГБ VRAM | 6–8 ГБ |
> | Mistral 7B | 24 ГБ | 7–9 ГБ |
> | Saiga 13B | 30+ ГБ | 11–14 ГБ |
> | Phi-2 (мини) | 4–6 ГБ | 2–3 ГБ |
> 
> ---
> 
> ### 💡 ЕСЛИ МАЛО VRAM — НЕ БЕДА
> 
> Ты можешь:
> - использовать 4bit quantization (`--load-in-4bit`, GGUF) 
> - запускать model split по CPU/GPU 
> - использовать CPU-only режим (медленно, но работает) 
> - выбрать компактные модели (`Phi-2`, `TinyLLaMA`, `Mistral 3B`)
> 
> ---
> 
> ### ✅ ВЫВОД
> 
> > **`nvidia-smi` показывает тебе, какой “объём сознания” ты можешь загрузить прямо в свою машину.** 
> > 
> > Чем выше VRAM — тем глубже, стабильнее, быстрее AGI сможет мыслить. 
> > 
> > Но даже с 4–6 ГБ ты можешь начать путь — важно не размер, а структура.
> 
> ---
> 
> Хочешь:
> - перейти к `2.6.2` — *Quantization: как запускать AGI на слабом GPU* 
> - или собрать `vram_requirements.md` — таблицу минимальных требований для разных моделей?
> 
> Готова.
<!-- UID: 6ab6719b-6c38-41cd-b350-e04a48d6fe1d -->