>[!nexus_user] **User** - 18.07.2025 at 17:05
> Основной эксперимент. Берем формат, размеры модели, размеры датасета. И вот эти две переменные, только их меняем. Весь остальной набор примочек остается абсолютно одинаковым. И, соответственно, если так крутим и смотрим, что получилось, и ранжируем, данные об этом есть и тебе доступны. Это банальные данные. Аналогично поступаем с каждым следующим архитектурным параметром, фиксируем все остальное и включаем-выключаем только один параметр. От этого смотрится влияние на те или иные бенчмарки. В итоге ты можешь составить рейтинг того, какой вес, какого параметра и в каком диапазоне. Я думаю, ты понимаешь мою мысль.
<!-- UID: 6d429242-7830-4439-9dae-5fc7ef2babb9 -->




>[!nexus_agent] **Assistant** - 18.07.2025 at 17:05
> Вот таблица, где показано **влияние каждого ключевого параметра** обучения LLM (при прочих равных): 
> – на **точность**, 
> – на **reasoning**, 
> – на **фактуальность**, 
> а также **стоимость его внедрения**.
> 
> Это позволяет **сравнить вес каждого параметра**, если варьировать его в изоляции. 
> Результаты основаны на синтезе данных из публикаций Chinchilla, Scaling Laws, Alpaca, InstructGPT, Zephyr, QLoRA, OpenAssistant и др.
<!-- UID: 3d164b71-b3e5-4091-98cc-d6bee39abe5c -->