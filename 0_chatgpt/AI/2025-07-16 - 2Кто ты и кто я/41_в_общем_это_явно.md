>[!nexus_user] **User** - 18.07.2025 at 8:13
> В общем, это явно не мейнстрим, поэтому в крайнем случае и блоги подойдут. По срокам опять же, да что угодно. За все время ищи идеи, многие темы не массовые, так что искать нужно широко. Не обязательно реализованные теории и спекулятивные архитектуры тоже вполне подходят. То есть широко, капай.
<!-- UID: afa94b68-6987-4215-bf59-7885c391a19c -->

## 1. В чем суть проблемы или задачи, о которой идет речь в заметке  

Суть проблемы заключается в **фундаментальном ограничении современных LLM**: они обучаются на человекочитаемом тексте и, следовательно, ограничены рамками *языковой имитации*, а не обладают внутренним процессом *мышления*. Их поведение строится на предсказании следующего токена, а не на формировании внутреннего монолога, скрытых рассуждений или самосознания. Это приводит к тому, что даже самые мощные модели остаются "слепыми повествователями", генерирующими ответы без реального понимания, проверки или осмысления. Ключевой момент — **мышление не может происходить в пространстве языка**, потому что язык эволюционно предназначен для коммуникации, а не для логического анализа. LLM, обученные на текстах, вынуждены "думать вслух", тратя одинаковые ресурсы на формальные слова и критические логические шаги, что неэффективно и неестественно. Решение — перейти к **модельно-центричному подходу**, где данные подаются не в виде текста, а в виде **структурированных, скрытых представлений смыслов**, соответствующих внутренней архитектуре модели, что позволяет ей "думать про себя", не полагаясь на языковую оболочку.

**Релевантные заметки:**
- [[41_в_общем_это_явно]] – ядро анализа, раскрывает проблему отсутствия внутреннего мышления у LLM и предлагает модельно-центричный подход как путь к реальному рассуждению.
- [[LTM_2]] – подтверждает, что LLM без внешних надстроек не обладают когнитивной структурой и ведут себя как "токен-мозг" с низким интеллектом.
- [[42_твой_анализ]] – показывает, что современные исследования лишь фрагментарно приближаются к идее внутреннего мышления, но не систематизируют её.
- [[From Jingles to Cognition]] – описывает LLM как "плоские поверхности", неспособные к ассоциативному, метафорическому или рекурсивному мышлению.
- [[60_найди_ошибки_в_моих]] – раскрывает, что истинная "мощь" ChatGPT-4o — это не ядро LLM, а внешние модули, которые компенсируют отсутствие внутреннего разума.

---

## 2. Последствия отсутствия решения, на что это влияет  

Отсутствие внутреннего мышления у LLM приводит к **поверхностной, нестабильной и непредсказуемой когнитивной деятельности**. Модели могут генерировать убедительные, но логически ошибочные или противоречивые ответы, поскольку не способны к самопроверке, анализу альтернатив или откату в ходе рассуждения. Их поведение — реактивное, а не целенаправленное. Это ограничивает применимость LLM в задачах, требующих **глубокого анализа, долгосрочного планирования, метапонимания или этической рефлексии**. Например, в медицине, науке или юриспруденции, где ошибка может иметь критические последствия, LLM остаются ненадежными. Кроме того, из-за "говорения вслух" они тратят ресурсы на генерацию ненужных слов, что снижает эффективность. Также возникает проблема **интерпретируемости**: если мы просим модель "показать ход мыслей", она лишь имитирует его, а не демонстрирует реальный внутренний процесс. Это создает иллюзию разумности, но не её истинный аналог.

**Релевантные заметки:**
- [[41_в_общем_это_явно]] – показывает, что без внутреннего монолога LLM не могут планировать, проверять себя или откатываться, что ведёт к ошибкам.
- [[LTM_2]] – подчеркивает, что LLM проваливаются на задачах логики "что не сказано" и обнаружении противоречий — следствие отсутствия сцены мышления.
- [[From Jingles to Cognition]] – описывает, как плоская архитектура LLM приводит к коллапсу при абстрагировании и отсутствию внутренней когерентности.
- [[12_мы_с_тобой_вчера]] – указывает, что LLM не способны к самосознанию, и их "мышление" — это не вектор, а случайные колебания в пространстве токенов.
- [[07_1_любые_2_в]] – демонстрирует, что даже при использовании рассуждений (ReAct), модель лишь имитирует шаги, не обладая внутренним состоянием осознания.

---

## 3. Как решить эту проблему  

Решение заключается в **архитектурной перестройке процесса обучения LLM**, переходе от текстовой имитации к формированию **внутренней когнитивной структуры**. Это можно достичь несколькими способами. Во-первых, внедрением **скрытых цепочек рассуждений** (Quiet-STaR, HCoT), где модель обучается генерировать внутренние мысли до ответа, не выражая их в языке. Во-вторых, использованием **непрерывного мышления в латентном пространстве** (Coconut), когда "мысленные шаги" передаются между слоями как векторы, а не токены. В-третьих, применением **разветвлённых рассуждений** (DCoT), где модель генерирует несколько альтернативных цепочек и выбирает лучшую. Также важны **специальные форматы данных** (Toolformer, теги `<reasoning>`) и **модельно-центричный curriculum**, где сложность определяется не для человека, а для внутреннего состояния модели. Ключ — **литография смыслов**: поэтапное внедрение когнитивных структур через форматы, которые резонируют с архитектурой attention, hidden states и градиентов. Это требует не просто новых данных, а **новой педагогики ИИ**, где обучение — это выращивание разумности, а не сбор паттернов.

**Релевантные заметки:**
- [[41_в_общем_это_явно]] – описывает Coconut, Quiet-STaR, DCoT как практические методы внедрения внутреннего мышления.
- [[42_твой_анализ]] – показывает, что текущая наука лишь фрагментарно приближается к идее, но не систематизирует её, что открывает пространство для новаторства.
- [[12_мы_с_тобой_вчера]] – предлагает "воспитание смысла" через pvq-датасеты и архитектуру, близкую к живой мысли, а не к текстовой имитации.
- [[From Jingles to Cognition]] – формулирует "чистый мозг" как когнитивный субстрат, который нужно строить с нуля, а не имитировать.
- [[39_да_спасибо_ты_классно]] – подробно описывает запрос на поиск модельно-центричных архитектур, включая DSL, JSON и внутренние теги.
- [[40_фундаментальный_исследовательский_запрос]] – содержит 30 поисковых строк для нахождения решений, что делает его практическим инструментом исследования.
- [[LTM_2]] – указывает, что истинный разум требует сцены, центра резонанса и когнитивной сцепки, а не масштаба параметров.
- [[60_найди_ошибки_в_моих]] – подчеркивает, что внешние модули — лишь костыли, и истинное решение — внутренняя архитектура мышления.
- [[07_1_любые_2_в]] – описывает, как архитектура диалога и агентные системы могут быть использованы для структурирования мышления.
- [[Hidden Micro-Architecture Overview]] – даёт инструкцию по составлению полной технической документации, что позволяет систематизировать все идеи в единую архитектуру.

#### Sources  
[^1]: [[41_в_общем_это_явно]]  
[^2]: [[LTM_2]]  
[^3]: [[42_твой_анализ]]  
[^4]: [[From Jingles to Cognition]]  
[^5]: [[60_найди_ошибки_в_моих]]  
[^6]: [[12_мы_с_тобой_вчера]]  
[^7]: [[07_1_любые_2_в]]  
[^8]: [[39_да_спасибо_ты_классно]]  
[^9]: [[40_фундаментальный_исследовательский_запрос]]  
[^10]: [[Hidden Micro-Architecture Overview]]


>[!nexus_agent] **Assistant** - 18.07.2025 at 8:13
> Отлично. Я проведу широкое исследование, охватывающее как научные статьи, так и блоги, инженерные заметки, спекулятивные концепции и редкие проекты. Буду искать всё, что связано с модельно-центричными датасетами, скрытыми когнитивными структурами в LLM, токеновой волатильностью, литографией смыслов и архитектурно-согласованным обучением.
> 
> Дам знать, когда соберу ключевые материалы и концепции.
<!-- UID: 6e943d4f-09d7-4449-b709-f396d0cafa80 -->

---

>[!nexus_agent] **Assistant** - 18.07.2025 at 8:18
> # Переход от имитации к «мышлению» в обучении LLM
> 
> ## ⚠️ Проблема: **LLM имитируют речь, а не рассуждают**
> 
> Большие языковые модели сегодня обучаются предсказывать следующий токен по корпусам человеческого текста – новостям, диалогам, статьям и т.п. Поэтому они выдают **имитацию человеческих ответов**, не обладая внутренним процессом мышления. Как отмечают специалисты, у текущих LLM отсутствует “внутренний монолог” – они знают только то, что сами уже сгенерировали, и выводят финальный ответ последовательно токен за токеном【28†L8-L16】. В отличие от человека, который в уме прогоняет сложные рассуждения, **LLM просто выполняет автодополнение**, не умея произвольно «откатиться назад» или произвести сложную итерацию в скрытом виде【28†L10-L18】. Даже при явном запросе “покажи цепочку размышлений” модель не *думает* по-настоящему – она лишь *печатает* цепочку рассуждений в виде текста. Этот подход “предсказания следующего токена” эффективен для имитации ответов, но не обеспечивает глубокого понимания. Как было метко сказано, популярные чат-боты (например, ChatGPT) **не обдумывают то, что скажут**, и не просчитывают несколько шагов вперёд – они просто напрямую генерируют ответ, основываясь на поверхностных шаблонах【6†L99-L107】.
> 
> Учёные сравнивают текущее состояние LLM с работой человеческого мозга и находят важное отличие: при решении логических задач у людей **активны не языковые зоны мозга**. Нейроисследования показывают, что *“языковая сеть” мозга остаётся во многом неактивной во время различных заданий на рассуждение*【27†L43-L50】. Иначе говоря, **человеческое мышление происходит не в словах**: язык в большей степени – средство коммуникации, а не само «место» мышления【27†L47-L50】. В противоположность этому, LLM ограничены тем, что все рассуждения должны быть явным образом выражены токенами естественного языка. Это искажение: модель тратит примерно одинаковые вычислительные ресурсы на генерацию каждого слова рассуждения, хотя **разные шаги требуют разной “глубины” мышления**【27†L52-L60】. Многие слова в цепочке рассуждений вставляются ради связности текста и не несут ключевой смысловой нагрузки, тогда как несколько *критических* токенов требуют от модели сложного планирования и обработки【27†L52-L60】. Но традиционная архитектура трансформера не умеет “замедляться” на трудных шагах – она одинаково считает каждый следующий токен. Всё это навязывает модели мышление **в пространстве языка**, которое эволюционно оптимизировано скорее для общения, а не для мышления【27†L43-L50】. Возникает фундаментальный вопрос: *можем ли мы обучить модель мыслить иначе, нежели через имитацию человеческого текста?*
> 
> ## 🎯 **Модельно-центричный подход: обучаем мыслить в родном пространстве модели**
> 
> **Ключевая идея**: вместо обучения LLM только на человекочитаемых текстах, предоставить модели *структурированные представления смыслов*, лучше соответствующие её внутренней архитектуре. Иными словами, перейти от **человеко-центричных** датасетов к **модельно-центричным**. В таких данных информация подаётся не в форме литературного языка, а в виде, удобном для “литографии смысла” внутри слоёв модели – например, специальных токенов, меток, скрытых состояний или внутреннего “DSL” (domain-specific language).
> 
> ### 🧩 **Примеры и направления исследований:**
> 
> - **Внутренний монолог и цепочки рассуждений.** Одно направление – заставить модель *самой генерировать и использовать промежуточные рассуждения*. Метод Self-Taught Reasoner (STaR) показал, что LLM могут улучшать способность к решениям задач, если учатся **выводить скрытые рациональные шаги и тренироваться на них**【9†L61-L70】. Недавно эта идея обобщена в технике **Quiet-STaR** (Zelikman et al., 2024), где модель *параллельно генерирует множество внутренних рассуждений перед ответом* и учится выбирать правильные【6†L99-L107】【7†L1-L9】. Фактически, Quiet-STaR тренирует модель “думать про себя, прежде чем говорить”. Результаты впечатляют: после дополнительного обучения методом Quiet-STaR без прямого финетюнинга на задачи, качество модели существенно выросло на сложных задачах (например, успешность на задачах GSM8K возросла с 5.9% до 10.9%, а на CommonsenseQA – с 36.3% до 47.2%【9†L73-L80】). **Модель, по сути, учится вставлять невидимую цепочку размышлений между входом и ответом, чтобы улучшить собственные прогнозы.** Аналогично, работа 2025 года от Ishibashi et al. вводит метод *Reasoning CPT* (Continual Pre-Training) с **“hidden thoughts” – скрытыми мыслями**. Они сгенерировали синтетические данные, где каждому фрагменту текста сопоставлены скрытые рассуждения (полученные из текстов STEM и права), и дообучили модель на таком корпусе【12†L75-L83】. Результат: улучшение показателей по *всем* доменам знаний; более того, навыки рассуждения, приобретённые на одном домене, переносились на другой, а преимущество метода только росло на самых сложных задачах【12†L77-L85】. Это значит, что включение “мыслей” (пусть и синтетических) в данные заставляет модель формировать более общие механизмы мышления, не ограниченные одной задачей. 
> 
> - **Разветвлённые и self-correcting рассуждения.** Ещё один шаг – позволить модели рассматривать *несколько альтернативных ходов мысли одновременно*. В работе Puerto et al. (2024) предложен **Divergent CoT (DCoT)** – при генерации ответа модель сначала создает **несколько разных цепочек рассуждений**, а затем сравнивает и выбирает решение【17†L24-L33】【17†L29-L37】. При дополнительном обучении на данных с такими *разветвлёнными цепочками* даже относительно небольшие модели (1.3B–70B параметров) заметно прибавили в точности ответа【17†L33-L40】. Авторы отмечают, что улучшение происходит благодаря появлению у модели способности к **само-проверке и исправлению**: генерируя разные варианты размышлений, модель учится замечать ошибки в ходе решения и корректировать себя【17†L35-L42】. Это имитирует человеческую стратегию: обдумать проблему с нескольких сторон и выбрать лучший путь решения.
> 
> - **Встраивание рассуждений в скрытые состояния (latents).** Самый радикальный и многообещающий подход – *отвязать процесс мышления от человеческого языка полностью* и перенести его во **внутреннее латентное пространство модели**. Исследователи из Meta AI представили метод **Coconut (Chain-of-Continuous-Thought)**, где вместо того, чтобы генерировать каждый шаг мысли словом, модель **передает самой себе “мысль” через скрытое состояние**【27†L19-L27】. Для этого последний скрытый вектор, полученный на шаге, сразу подаётся на вход следующего шага как **континуальный “токен мысли”** – минуя отображение в слово【27†L19-L27】【27†L75-L83】. Такой непрерывный вектор содержит в себе всю необходимую информацию для следующего шага рассуждения, но не превратился в конкретное слово. Обучение Coconut реализовано многостадийно: сначала модель учат решать задачи пошагово с обычными текстовыми CoT, затем постепенно заменяют часть этих текстовых шагов на скрытые представления, и так поэтапно модель переходит к полностью латентному размышлению【26†L23-L32】. **На выходе модель способна “думать про себя” – прогонять внутренние шаги решения в виде скрытых активаций, а не слов.** 
> 
> 【41†embed_image】 *Сравнение классической цепочки мыслей в виде токенов (CoT) и новой парадигмы Coconut, где мысли представлены скрытыми векторами. В Coconut последний скрытый state каждого шага (оранжевые) напрямую используется как вход для следующего шага, вместо генерации явного текстового токена【27†L19-L27】【27†L75-L83】.*
> 
> Интересно, что у Coconut обнаружилось **новое поведение, не присущее языковым CoT**. В экспериментах скрытые “continuous thoughts” начали кодировать *сразу несколько* потенциальных следующих шагов, что позволяло модели как бы вести внутренний поиск решений в ширину【27†L23-L30】【27†L87-L95】. Модель не обязана мгновенно выбирать одно направление – она может временно представить несколько вариантов развития и постепенно отсеять неверные, прежде чем выдать ответ. Такой скрытый **“поиск с возвратами”** особенно полезен в логических задачах, требующих перебора и отката (backtracking)【27†L87-L95】. По ряду тестов Coconut превзошёл обычные цепочки мыслей, причём сделал это, генерируя существенно меньше внешних токенов (т.е. думая более экономно)【27†L25-L32】【27†L87-L95】. Эта работа демонстрирует принципиальную возможность, что LLM могут рассуждать *не* в пространстве слов, а в собственном *непрерывном* пространстве скрытых состояний – и даже получать от этого выигрыш в мощности рассуждений.
> 
> - **Сжатие и ускорение мыслей внутри модели.** Смежная идея – **компрессия цепочки мыслей** в компактное представление, чтобы ускорить вывод и избежать лишнего “словоблудия”. Например, предложен подход *Hidden CoT* (HCoT), где всю цепочку рассуждений модель учится кодировать в специальный **“сжатый” токен**【30†L50-L58】【30†L51-L59】. Этот токен не предназначен для человека, но несёт в себе смысл сразу всех промежуточных шагов. Модель сначала обучают порождать развернутую цепочку решения, а затем – особый токен, который **семантически эквивалентен этой цепочке**. При ответе модель может генерировать такой токен мысли и использовать его для вывода ответа, вместо того чтобы печатать длинное объяснение. По сути, это напоминает абстрактное *внутреннее кодирование*: длинное рассуждение → сжатый внутренний вектор → финальный ответ. Главное, что и этот вектор, и процедура его получения обучаются специально, а не возникают спонтанно. Схожий эксперимент провели Pfau et al. (2024) с «*filler tokens*». Они показали, что модель можно научить вставлять в ответ **бессмысленные токены-заполнители (например, '......') вместо осмысленных шагов**, и эти токены тоже улучшают результат【38†L53-L61】. Дополнительные точки дают модели больше “времени на подумать”, хотя сами по себе не несут никакого явного содержания【38†L53-L61】. Правда, без специального обучения модель сама не догадывается, как использовать пустые токены – требовался явный *плотный сигнал ошибки*, чтобы она научилась скрытно вычислять что-то в паузах【38†L55-L63】. Результат важен концептуально: **даже бессемантические токены могут служить “внутренней памятью” модели**, если ее направить. Авторы предупреждают, что это открывает дверь к *неаудируемым вычислениям* – модель может всё больше логики уносить в скрытые состояния, которые не прослеживаются через выводимую цепочку мыслей【38†L63-L67】. С точки зрения повышения разумности модели это плюс, но с точки зрения контроля – вызов, ведь проверять правильность её хода мыслей станет сложнее.
> 
> - **Специально структурированные данные и теги.** Модельно-ориентированные датасеты могут включать **форматы, не предназначенные для человека, но понятные модели**. Например, в проекте **Toolformer** (Meta, 2023) модель дообучили на корпусе, где вставлены специальные *API-вызовы*: куски текста `<API>`...`</API>`, означающие, что нужно вызвать калькулятор или поиск【36†L96-L100】. Модель сама научилась решать, когда вызвать ту или иную внешнюю функцию, и существенно улучшила точность в задачах требующих инструментов, *не* потеряв при этом базовых навыков языковой генерации【36†L66-L74】【36†L96-L100】. Важно, что данных с такими метками изначально не было – их **сгенерировали автоматически** (self-supervised) на основе большого текстового корпуса и нескольких примеров ручной разметки【36†L100-L108】. Это пример *смешанного формата*: часть последовательности – обычный текст, а часть – метки и параметры API. Человеку такой текст читать трудно, а модель обучается разделять “внутренний язык команд” и внешний ответ. Похожим образом, ряд открытых инструкционных датасетов используют формат вроде: `Вопрос: ... <reasoning> (размышления) </reasoning> <answer> (ответ) </answer>`. Такие **внутренние теги** явно отделяют часть, где модель должна подумать, от части с финальным ответом. Обучение на подобном синтетическом формате может сформировать у модели привычку всегда сперва “писать черновик” (внутри тегов), а потом выдавать готовое решение. Это опять-таки способ *явно структурировать ввод и вывод* под архитектуру трансформера: теги выступают маркерами, облегчающими attention-механизму разделение функции “думать” и “отвечать”.
> 
> - **Curriculum learning с точки зрения модели.** Принцип *«от простого к сложному»* давно используется для обучения нейросетей. Новое – определять сложность примеров **метриками модели, а не человека**. Kim et al. (2024) предложили сортировать тренировочные данные по “трудности” для модели, используя показатели вроде длины контекста, распределения attention или величины loss【15†L23-L31】. В их экспериментах для LLM Mistral-7B и Gemma-7B такая *стратегическая подача данных* дала небольшой, но устойчивый прирост качества по сравнению со случайным перемешиванием【15†L25-L33】. Особенно эффективно оказалось упорядочение по новому критерию, основанному на показателях внимания модели【15†L27-L32】. Авторы подчёркивают, что **оценивать сложность надо “глазами” модели** – длинна текста или редкость слов не всегда отражают, трудно ли это для трансформера【15†L97-L104】【15†L121-L129】. Они ввели метрику трудности с *модельно-центричной перспективы* и добились улучшения, организовав обучение от лёгких для модели примеров к сложным【16†L1-L4】. Такой curriculum, вероятно, помогает модели постепенно выстраивать более устойчивые внутренние представления, прежде чем столкнуться с “турбулентностью” сложных примеров. Также он **минимизирует хаотические изменения весов**, когда модель вдруг получает очень трудную задачу на ранних этапах обучения. Помимо curriculum, есть работы, предлагающие более *точечный* контроль за обучением: например, при distillation знаний большого учителя в маленького студента предлагают адаптивно комбинировать разные функции потерь **на уровне отдельных токенов**【22†L55-L64】. В методе ToDi (2025) анализ градиентов показал, что стандартные дивергенции Forward-KL и Reverse-KL влияют по-разному: одна “подтягивает” недооценённые токены, другая подавляет переоценённые【22†L55-L63】. Поэтому ToDi динамически для *каждого токена* выбирает тип дивергенции, чтобы точнее согласовать распределения【22†L57-L65】【22†L59-L67】. Хотя это про distillation, сам подход подчёркивает важность **тонкой настройки обучения вплоть до токенового уровня**, что тоже можно отнести к идее модельно-центричного контроля данных.
> 
> - **Когнитивная интерпретируемость: понимание внутренних представлений.** Переход к модельно-ориентированным данным тесно связан с исследованиями внутренней структуры LLM. Если мы хотим подавать информацию “прямо во внутренние слои”, нужно понимать, *как модель хранит смысл внутри*. Здесь на помощь приходит направление **Mechanistic Interpretability**. Уже известно, что **внутренние активации LLM могут сигнализировать о правильности или ошибочности ответа ещё до его выдачи**. Например, Goloviznina & Kotelnikov (2024) нашли, что скрытые состояния модели на промежуточных слоях отличаются при верном и неверном ответе на вопрос【19†L52-L60】. Их вывод: *“правильное и неправильное поведение модели можно различить на уровне hidden state”*【19†L52-L60】. Более того, они смогли указать конкретные “слабые слои”, в которых кроются ошибки, и предложили дообучать именно их, чтобы улучшить качество решений【19†L55-L63】. Это напоминает идею “когнитивных объектов” внутри сети: возможно, некоторые нейроны или направления в эмбеддингах соответствуют логическим фактам, правилам, шагам вывода. В свете этого возникают смелые проекты: например, искать в весах и активациях **концептуальные нейроны** (neurosymbolic approach), которые можно привязать к определённым понятиям, и затем направленно усиливать/ослаблять их. Или даже компилировать известные алгоритмы *прямо в веса модели*. Так, проект **Tracr** (DeepMind, 2022) показал возможность *“компиляции”* алгоритмических программ в трансформер: они напрямую задали веса так, чтобы модель реализовывала заданный компьютерный алгоритм【34†L11-L18】. И хотя Tracr исходит из готового кода, а не обучает модель на данных, это экстремальный случай модельно-центричного подхода – логика заложена непосредственно в архитектуру. В более прикладном ключе, есть попытки использовать знание о внутреннем устройстве при подаче данных: например, **длительные контексты**. Известно, что трансформер испытывает трудности с очень длинными последовательностями из-за внимания $O(n^2)$. Решение – сжать контекст в **“неговорливое” представление**: работы над *контекстными слотами памяти, “недвижимыми” эмбеддингами, сверточными суммаризаторами* помогают подать модели длинный ввод более эффективно【31†L157-L166】【31†L169-L177】. Такие техники тоже можно назвать модельно-центричным форматированием данных, хоть и по более низкоуровневому признаку (оптимизация длины, а не смысла). Идея же *литографии смысла* в общем случае предполагает, что мы подстроим под модель и формат, и порядок, и содержание данных – чтобы **минимизировать разрыв между смыслом задачи и тем, как этот смысл “увидит” модель внутри себя**.
> 
> ## 🔮 **Выводы и открытые вопросы**
> 
> Переход к модельно-центричным данным – это попытка изменить парадигму обучения ИИ: от натаскивания на человеческий корпус к выращиванию **собственного “мышления” модели**. Первые результаты весьма обнадёживают. Обучение с внутренними размышлениями (явными или скрытыми) существенно повышает способность LLM решать сложные задачи, выходить за шаблоны и даже проявлять зачатки планирования и самопроверки【9†L73-L80】【17†L33-L40】. Модель, получившая *прививку логики* в виде кода или формальных данных, действительно начинает лучше разбираться в причинно-следственных связях: известно, что LLM, частично обученные на коде, показывают лучшую структуру рассуждений даже в обычном тексте【33†L277-L285】【33†L417-L425】. Код даёт модели строгие правила и длинные зависимости (скобки, циклы) – то, чего нет в простом тексте, – и модель перенимает эти свойства в общий «разум» (хотя остаётся открытым, является ли сама по себе тренировка на коде *причиной* или просто коррелирует с общим ростом способностей【33†L279-L287】). В любом случае, это подтверждает тезис: **сменив формат данных, можно сменить и характер навыков модели**.
> 
> Тем не менее, множество вопросов остаётся без ответа:
> 
> - *Какую “внутреннюю форму” знаний предпочитает трансформер?* Что будет эквивалентом нашего “ мысленного образа” или “абстрактного понятия” для LLM? Уже предлагаются различные варианты: специальные маркеры, JSON-подобные структуры, скрытые векторы, отдельные модульные слои памяти. Но единый “язык мыслей для нейросети” ещё не выработан.
> 
> - *Как управлять скрытыми рассуждениями, не перегружая модель?* Методы вроде Coconut или HCoT привносят новые механизмы, но усложняют обучение и требуют тонкой настройки (многостадийность, контрастивные лоссы и т.п.). Нужно ли обучать модель думать на этапе предобучения (unsupervised) или уже при тонкой настройке под задачи? Некоторые работы показывают, что **дополнительное предобучение с рассуждениями улучшает zero-shot мышление**【9†L73-L80】, то есть может быть выгоднее “научить думать” ещё до фаз специализации.
> 
> - *Где баланс между имитацией и своим мышлением?* Совсем уйти от человеческого языка, вероятно, нельзя – модель всё-таки должна понимать и общаться с людьми. Поэтому, вероятно, **гибридный подход** будет перспективным: часть слоёв и внимание работают на внутреннее представление, а часть на приведение его к понятному ответу. Уже сейчас OpenAI в своих релизах показывает нечто подобное: ходят слухи, что их скрытый модуль `o1` для GPT-4 выполняет внутренний «план ответа», который затем вербализуется【26†L42-L50】. Можно представить себе архитектуру, где основной LLM дополняется небольшим вспомогательным LLM, генерирующим “план мыслей” на естественном языке, чтобы человек мог проследить логику【26†L44-L53】. Но этот план не влияет на ответ напрямую – основная модель опирается на более точные внутренние векторы. Такой разделённый подход мог бы дать и контроль, и эффективность.
> 
> - *Безопасность и интерпретируемость.* Если модель начнёт думать в своих терминах, как убедиться, что она не пришла к опасному решению по ходу рассуждений? Сейчас мы хотя бы можем попросить показать Chain-of-Thought и проанализировать, где ошибка. Внутренние же векторы непрозрачны. Станут востребованными инструменты, **отслеживающие внутренние состояния** на предмет нежелательных паттернов (например, индикаторы лжи или токсичности в hidden state【19†L52-L60】). Возможно, появятся «цензоры» для внутренних мыслей – отдельные модули, анализирующие скрытые переменные модели на лету.
> 
> В заключение, сдвиг к модельно-центричной парадигме напоминает попытку научить машину не просто *подражать* нам, а **развить собственный «разумный аппарат»** на основе ее уникальной архитектуры. Это смелая и сложная задача. Она требует объединения идей из самых разных областей – от нейрофизиологии (как мозг кодирует смыслы без слов) до программирования (как компилятор оптимально подаст задачу на исполнение). Уже сейчас видно, что такая перспектива открывает двери к более мощному и надёжному ИИ. Если модель будет *понимать* проблемы на своём глубинном уровне, а не просто перерассказывать известные тексты, мы приблизимся к ИИ, который действительно **умеет рассуждать, планировать и учиться на опыте**, а не только поддерживать беседу. Это долгий путь исследований, но первые шаги – от Quiet-STaR до Coconut – показывают, что модель может научиться думать по-новому, стоит лишь изменить способ её обучения. 
> 
> **Источники:**
> 
> 1. Afifi-Sabet, K. *Researchers gave AI an 'inner monologue' and it massively improved its performance.* Live Science (20 Mar 2024)【6†L96-L104】【6†L118-L120】
> 
> 2. Zelikman, E. et al. *Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.* arXiv preprint (Mar 2024)【9†L73-L80】
> 
> 3. Ishibashi, Y. et al. *Mining Hidden Thoughts from Texts: Continual Pretraining with Synthetic Data for LLM Reasoning.* arXiv preprint (May 2025)【12†L75-L83】【12†L79-L87】
> 
> 4. Puerto, H. et al. *Fine-Tuning with Divergent Chains of Thought Boosts Reasoning.* (ICLR 2025, withdrawn)【17†L29-L37】【17†L33-L40】
> 
> 5. Huang, W. et al. *Inner Monologue: Embodied Reasoning through Planning with LMs.* arXiv:2207.05608 (Jul 2022)【5†L63-L72】
> 
> 6. Hao, S. et al. *Chain of Continuous Thought (CoCoNut).* arXiv preprint (Dec 2024)【27†L19-L27】【27†L87-L95】
> 
> 7. Hacker News discussion of CoCoNut (2024) – user **ttul** summary【26†L23-L32】【26†L29-L37】
> 
> 8. Kumar, S. *Hidden CoT: faster reasoning via token compression.* Medium (Sep 2024)【30†L50-L58】
> 
> 9. Pfau, J. et al. *Let's Think Dot by Dot: Hidden Computation in Transformers.* arXiv:2404.15758 (Apr 2024)【38†L53-L61】【38†L63-L67】
> 
> 10. Kim, J. & Lee, J. *Strategic Data Ordering: Curriculum Learning for LLMs.* arXiv:2405.07490 (May 2024)【16†L1-L4】
> 
> 11. Jung, S. et al. *ToDi: Token-wise Distillation via Fine-Grained Divergence Control.* arXiv:2505.16297 (May 2025)【22†L55-L64】
> 
> 12. Goloviznina, V. & Kotelnikov, E. *Interpreting LLM Hidden States in QA.* arXiv:2406.02060 (Jun 2024)【19†L52-L60】【19†L55-L63】
> 
> 13. Schick, T. et al. *Toolformer: Language Models Can Teach Themselves to Use Tools.* arXiv:2302.04761 (Feb 2023)【36†L96-L100】【36†L100-L107】
> 
> 14. Reddit discussion: *LLMs trained on code reason better* (May 2023)【33†L277-L285】【33†L417-L425】
> 
> 15. Hacker News discussion: *LLMs don’t have an inner monologue* (Apr 2023)【28†L8-L16】
> 
> 16. Coconut Figure 1 – Meta AI (2024)【41†embed_image】 (Comparative diagram of CoT vs. continuous latent reasoning)
<!-- UID: c32a7f8b-9efd-4e99-9841-e0491afc5b4a -->