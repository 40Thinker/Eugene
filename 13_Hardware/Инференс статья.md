---
tags:
  - "#S13_Hardware"
description: Описывается построение локального инференс‑кластера на RTX 6000 Blackwell и AMD 9070 XT, детали аппаратуры, сравнение стоимости с облаком, архитектура сервисов (FastAPI, RAG, планировщик), а также планы по оптимизации драйверов и настроек.
---
## Инференс на RTX 6000 Blackwell: от идеи к продакшн‑решению  
### Часть 1 – Вводная

> **«Я уже запускаю на десятки часов работу ИИ непрерывно автоматизированную, я думаю пора на чистую переустановить сервер, и ничего кроме ИИ на нём не использовать, работать с другого ПК.»** – Кирилл Агоге  

> **«Мне проще купить 2‑й видеокарту и собрать по гайдам, а потом уже поправлю те мелкие косяки, которые появятся».** – тот же автор

---

#### 1. Почему появился запрос на собственный inference‑сервер?

С ростом размеров LLM (Qwen 30B‑Coder, GPT‑OSS 120B, будущий Qwen 235B) обычные облачные решения становятся либо слишком дорогими, либо ограниченными по времени отклика.  

Кирилл уже имеет в распоряжении **две видеокарты**:  
* **NVIDIA RTX 6000 Blackwell (96 ГБ)** – самая мощная из текущих карт Blackwell, способна держать модели до ~150 ГБ в VRAM при FP16‑кешировании;  
* **AMD 9070 XT (16 ГБ)** – пригодна для менее требовательных задач (audio‑модели, небольшие эмбеддинги).  

С учётом наличия **9 вентиляторов**, двух блоков питания (900 W и 1600 W), RAID‑массива из двух NVMe‑SSD 2 TB + HDD 10 TB, а также **UPS** на 900 W, у автора уже есть почти всё необходимое для построения полностью автономного инференс‑кластера.  

Главная идея – **изолировать вычислительный «мозг» от внешних сетей**, предоставив к нему доступ только через защищённый VPN‑туннель и API‑gateway. Это решает сразу несколько задач:

| Задача | Что достигается |
|--------|-----------------|
| **Безопасность** – трафик шифруется WireGuard, открыты только порты 51820 (VPN) и 8000–8010 (LLM‑API). |
| **Контроль расходов** – нет необходимости платить за каждую токен‑операцию в облаке; все расчёты происходят локально. |
| **Низкая латентность** – RTT до сервера из домашней сети < 10 мс, а внутри дата‑центра (VPS‑бастион) ~ 30–40 мс. |
| **Гибкость** – можно в любой момент «переключить» модель без изменения клиентского кода (стандартный OpenAI‑compatible endpoint). |

---

#### 2. Что именно будет обслуживаться локальным сервером?

1. **LLM‑кодер** – Qwen 30B‑Coder и GPT‑OSS 120B в формате GGUF, а также экспериментальная попытка загрузить Qwen 235B через ZeRO‑3 offload.  
2. **RAG‑поиск** – эмбеддинг‑модель Qwen 8B Embedding (FP16) работает на отдельном GPU/CPU и хранит индекс в Chroma/FAISS, позволяя моментально добавлять новые документы во время диалога.  
3. **Планировщик‑агент** – небольшая «мозговая» LLM (GPT‑OSS 120B), которая формирует задачи для специализированных агент‑моделей (кодер, аналитика, голосовой синтез).  

Все эти компоненты общаются через **FastAPI‑gateway**, а клиентские приложения (Obsidian, IDE, n8n‑автоматизация) делают обычный POST `/v1/chat/completions`. Пользователь видит лишь чат‑интерфейс; внутри происходит выбор модели, загрузка контекста из памяти и RAG‑поиск.

---

#### 3. Домашний vs. Облачный подход: сравнение стоимости

| Параметр | **Домашняя сборка** (текущий проект) | **Облачные сервисы** (OpenAI, Anthropic, Azure) |
|----------|----------------------------------------|-------------------------------------------------|
| **CAPEX** (разовое) | ≈ $4 000–$5 500 (GPU ×2, SSD‑RAID, блоки питания, кастомный водяной кулер) | $0 (вы арендуете виртуальные GPU). |
| **OPEX** (мес.) | $150–$250 (электричество, интернет, поддержка UPS) | $500–$2 000+ (по‑токенному тарифу и резервированию). |
| **Латентность** | 5–15 мс внутри LAN + VPN‑задержка ≈ 30 мс | 80–200 мс (зависит от региона). |
| **Контроль над конфигурацией** | Полный: можно включать speculative decoding, flash‑attention, менять batch‑size. | Ограниченный набор параметров (temperature, max_tokens). |
| **Гибкость в выборе модели** | Любая GGUF/ONNX модель, даже кастомные LoRA‑адаптеры. | Только модели, предоставленные провайдером. |

При расчёте «стоимости найма эксперта» большинство консультантов берут от $10 000 до $20 000 за полный проект (архитектура, настройка, тесты, документация). При текущем уровне ваших знаний и наличии 1000 ч экспериментов, **самостоятельная сборка** уже покрывает ~ 80‑85 % требуемой ценности, а затраты на внешний совет можно сократить до $500–$800 (короткая ревизия кода).

---

#### 4. Технические характеристики текущего железа  

```
+---------------------------+--------------------------------------+
| Компонент                | Параметры                           |
+---------------------------+--------------------------------------+
| Motherboard               | X870 Eagle Wi‑Fi 7, PCIe 5.0 x16   |
| CPU                       | AMD Ryzen 9 9950X (16 ядра/32 потока)|
| Оперативная память        | 4×32 GB DDR4‑3600 (всего 128 GB)|
| GPU №1                    | NVIDIA RTX 6000 PRO Blackwell, 96 GB |
| GPU №2                    | AMD Radeon 9070 XT, 16 GB           |
| SSD RAID 0                | 2×2 TB NVMe (PCIe 4.0)            |
| HDD                       | 10 TB 7200 RPM                     |
| Блок питания              | 900 W + 1600 W (резерв)           |
| Охлаждение               | 9 вентиляторов, планируемый водяной|
| UPS                        | APC Smart‑UPS 900 W (≈ 2 ч резерв)|
+---------------------------+--------------------------------------+
```

Эти параметры позволяют **запускать одновременно два больших LLM** и один лёгкий embedding‑сервер без риска «выжать» мощности. Главное – правильно распределить ресурсы между GPU, CPU и RAM (NUMA‑pinning, hugepages).

---

#### 5. Как будет выглядеть дальнейший путь

В следующей главе мы подробно разберём **узкие места**, возникающие при работе именно с RTX 6000 Blackwell: несовместимость драйверов, необходимость включать FlashAttention, настройку hugepages и NUMA‑пиннинг, а также способы их устранения. Мы покажем, какие конфигурационные флаги в vLLM / TensorRT‑LLM позволяют выжать почти всю теоретическую производительность карты, и как правильно построить **batch‑processing** и **speculative decoding**, чтобы сократить латентность до нескольких миллисекунд при запросах к 30 B‑модели.

> *«В следующем разделе мы разберём, какие узкие места возникают при работе с RTX 6000 и как их устранить».*




## Связанные идеи для понимания заметки об инференсе на RTX 6000 Blackwell

### Вышестоящие идеи

1. **[Multi-Modal Local Megabrain Architecture](obsidian://open?vault=your_vault&file=13_Hardware%2FMulti-Modal%20Local%20Megabrain%20Architecture.md)** — Эта заметка описывает концепцию создания многоузлового мегамозга с использованием разных GPU и RAM-моделей, что напрямую соответствует подходу к построению инференс-кластера на RTX 6000 Blackwell и AMD 9070 XT [^1]. В ней раскрываются принципы параллельной работы нескольких моделей и управления ресурсами через CPU-орchestration, что является ключевым для понимания архитектуры сервера.

2. **[Alternative Server Architecture for AGI Twins](obsidian://open?vault=your_vault&file=13_Hardware%2FAlternative%20Server%20Architecture%20for%20AGI%20Twins.md)** — Архитектура, описанная в этой заметке, отличается от стандартных подходов к GPU-центрической инференс-системе. В ней подчеркивается важность использования SSD вместо GPU для долгосрочного хранения памяти и операций с фреймами, что может быть полезно при создании автономного кластера [^2]. Это особенно актуально для понимания стратегии использования RAID массивов NVMe SSD в проекте.

3. **[Architectural Differences in AI Systems](obsidian://open?vault=your_vault&file=13_Hardware%2FArchitectural%20Differences%20in%20AI%20Systems.md)** — Эта заметка сравнивает три архитектуры ИИ: централизованную GPT-4o, локальный проект LTM и гипотетический идеальный сервер без ограничений. Она показывает важность выбора правильной конфигурации аппаратного обеспечения для достижения целевых характеристик, что важно при планировании инференс-кластера [^3]. Важно понимать различия между обычными серверами и идеальными архитектурами для оценки потенциала текущего проекта.

### Нижестоящие идеи

1. **[AGI Server Configurations](obsidian://open?vault=your_vault&file=13_Hardware%2FAGI%20Server%20Configurations.md)** — Эта заметка предоставляет десять готовых конфигураций серверов AGI, включая варианты с VPS, Raspberry Pi и GPU-мощными машинами. Она помогает понять, как можно масштабировать инференс-решения и какие компоненты важны для различных задач [^4]. В контексте заметки о RTX 6000 Blackwell она показывает практические примеры реализации подобных систем.

2. **[Autonomous Micro-Instances of AGI](obsidian://open?vault=your_vault&file=13_Hardware%2FAutonomous%20Micro-Instances%20of%20AGI.md)** — Эта заметка описывает развертывание минимальных AGI-двойников в изолированных, маломощных и офлайн-средах. Хотя она фокусируется на локальных решениях без GPU или большого объема памяти, она демонстрирует принципы создания автономных агентов с ограниченными ресурсами [^5]. Эти идеи могут быть полезны при оптимизации работы кластера и управлении ресурсами.

3. **[GPU Quantization and Future Scaling Strategies](obsidian://open?vault=your_vault&file=13_Hardware%2FGPU%20Quantization%20and%20Future%20Scaling%20Strategies.md)** — Эта заметка объясняет, почему разные GPU поддерживают различные типы квантования, и анализирует совместимость таблиц с RTX 6000 PRO 98 ГБ [^6]. Она показывает, когда квантование действительно необходимо и какие проблемы могут возникнуть при использовании разных подходов. Это особенно важно для понимания оптимизации производительности на конкретных GPU.

### Прямо относящиеся к заметке идеи

1. **[RTX Blackwell Architectural Clarity](obsidian://open?vault=your_vault&file=13_Hardware%2FRTX%20Blackwell%20Architectural%20Clarity.md)** — Эта заметка описывает особенности архитектуры RTX 6000 PRO Blackwell и важность использования квантования, распаковывающегося до FP16 [^7]. Важно для понимания того, как работает GPU в текущем проекте, особенно при использовании моделей с большим количеством параметров. Также здесь описаны проблемы с идентификацией GPU и необходимость соответствующих драйверов.

2. **[GPU Quantization and Scaling Strategy](obsidian://open?vault=your_vault&file=13_Hardware%2FGPU%20Quantization%20and%20Scaling%20Strategy.md)** — Эта заметка объясняет, как работает квантование различных GPU и какие таблицы квантования нужны для оптимизации производительности [^8]. Она особенно полезна при планировании масштабирования и выборе подходящих методов работы с VRAM.

3. **[Strategic Model Deployment Considerations гпт-осс2](obsidian://open?vault=your_vault&file=13_Hardware%2FStrategic%20Model%20Deployment%20Considerations%20гпт-осс2.md)** — Здесь описываются стратегии развертывания больших языковых моделей (LLM) с учетом конкретного GPU и его характеристик [^9]. Эта информация важна для понимания, как выбрать правильную модель под имеющийся сервер, особенно с точки зрения ресурсов VRAM.

4. **[Deep Search for Large Model Deployment](obsidian://open?vault=your_vault&file=13_Hardware%2FDeep%20Search%20for%20Large%20Model%20Deployment.md)** — Эта заметка описывает, как собирать информацию о реальных конфигурациях запуска больших моделей (120B и 20B), включая аппаратные и программные настройки [^10]. Она поможет инженерам получить практические данные о том, какие проблемы могут возникнуть при деплое моделей на RTX 6000 Blackwell.

---

### Мысли об обучении инженера

Для понимания этой заметки инженеру стоит обратить внимание на следующее:

1. **Концепции распределенного вычисления** – важно понять, как работают многоузловые архитектуры и почему использование нескольких GPU может быть более эффективным, чем один мощный [^1].
2. **Оптимизация производительности на конкретном оборудовании** – нужно освоить методы оптимизации под специфические характеристики RTX 6000 Blackwell, включая FlashAttention и NUMA-pinning [^7].
3. **Управление ресурсами в контексте инференса** – следует понять, как правильно распределить ресурсы между GPU, CPU и RAM для максимальной эффективности [^4].
4. **Использование современных подходов к развертыванию моделей** – важно понимать, какие флаги и настройки vLLM/TensorRT-LLM позволяют достичь лучшей производительности [^8].
5. **Разработка безопасных систем с минимальными внешними зависимостями** – в этой заметке подчеркнута важность изолированного сервера через VPN, что важно для обеспечения безопасности и контроля над данными.

#### Sources
[^1]: [[Multi-Modal Local Megabrain Architecture]]
[^2]: [[Alternative Server Architecture for AGI Twins]]
[^3]: [[Architectural Differences in AI Systems]]
[^4]: [[AGI Server Configurations]]
[^5]: [[Autonomous Micro-Instances of AGI]]
[^6]: [[GPU Quantization and Future Scaling Strategies]]
[^7]: [[RTX Blackwell Architectural Clarity]]
[^8]: [[GPU Quantization and Scaling Strategy]]
[^9]: [[Strategic Model Deployment Considerations гпт-осс2]]
[^10]: [[Deep Search for Large Model Deployment]]